{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Avocado_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyQTRzMcUCB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare tensorflow-model-optimization for pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIe8xTeVtIWd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNPotwubYRu",
        "colab_type": "code",
        "outputId": "3fc7bef8-f3ba-4aeb-adf0-b727fe3200d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tensorflow-gpu==1.14.0\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 61.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (45.2.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c4/4c3d011e432bd9c19f0323f7da7d3f783402615e4c3b5a98416c7da9cb05/tensorflow_model_optimization-0.2.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.17.5)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Collecting enum34~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/df/6e/46a0304561a07bda942c4283b67228d5b87d1ebd1189c866ede4b4cd6a0a/enum34-1.1.9-py3-none-any.whl\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.9 tensorflow-model-optimization-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfBLXz1mCNV",
        "colab_type": "code",
        "outputId": "52eb2555-c08c-4959-813f-d4573f05c139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk1dYHpbicZ",
        "colab_type": "text"
      },
      "source": [
        "#Download dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu64LC4Wm1oc",
        "colab_type": "code",
        "outputId": "55bf8dc7-0f46-43c2-d11b-34bead3c019b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWT-iIqSb_46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://drive.google.com/open?id=1Wz2xqSZSjLDzXXTLdkf6zyTUUcBopn3i\n",
        "download = drive.CreateFile({'id': '1Wz2xqSZSjLDzXXTLdkf6zyTUUcBopn3i'})\n",
        "download.GetContentFile('avocado_dataset_w64.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDlBcilciuY",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRqt2u-MVfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from operator import truediv\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Data augmentation functions\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "def add_rotation_flip(x, y):\n",
        "\n",
        "    x = np.reshape(x, (x.shape[0], x.shape[1], x.shape[2], x.shape[3], 1))\n",
        "\n",
        "    # Flip horizontally\n",
        "    x_h = np.flip(x[:, :, :, :, :], 1)\n",
        "    # Flip vertically\n",
        "    x_v = np.flip(x[:, :, :, :, :], 2)\n",
        "    # Flip horizontally and vertically\n",
        "    x_hv = np.flip(x_h[:, :, :, :, :], 2)\n",
        "\n",
        "    # Concatenate\n",
        "    x = np.concatenate((x, x_h, x_v, x_hv))\n",
        "    y = np.concatenate((y, y, y, y))\n",
        "\n",
        "    return x, y\n",
        "    \n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "LOAD HDF5 FILE\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "hdf5_file = h5py.File('avocado_dataset_w64.hdf5', \"r\")\n",
        "train_x = np.array(hdf5_file[\"train_img\"][...])\n",
        "train_y = np.array(hdf5_file[\"train_labels\"][...])\n",
        "\n",
        "# Average consecutive bands\n",
        "img2 = np.zeros((train_x.shape[0], int(train_x.shape[1]/2), int(train_x.shape[2]/2), int(train_x.shape[3] / 2)))\n",
        "for n in range(0, train_x.shape[0]):\n",
        "    xt = cv2.resize(np.float32(train_x[n, :, :, :]), (32, 32), interpolation=cv2.INTER_CUBIC)\n",
        "    for i in range(0, train_x.shape[3], 2):\n",
        "        img2[n, :, :, int(i / 2)] = (xt[:, :, i] + xt[:, :, i + 1]) / 2.\n",
        "\n",
        "train_x = img2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3wrK8dNe-T7",
        "colab_type": "code",
        "outputId": "58852846-c704-49ef-daed-e39de0c36268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], 5))\n",
        "\n",
        "temp[:, :, :, 0] = train_x[:, :, :,138]\n",
        "temp[:, :, :, 1] = train_x[:, :, :,140]\n",
        "temp[:, :, :, 2] = train_x[:, :, :,143]\n",
        "temp[:, :, :, 3] = train_x[:, :, :,145]\n",
        "temp[:, :, :, 4] = train_x[:, :, :,149]\n",
        "\n",
        "\n",
        "train_x = temp\n",
        "\n",
        "train_x, train_y = add_rotation_flip(train_x, train_y)\n",
        "print(train_x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(964, 32, 32, 5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvx55MBgcrZJ",
        "colab_type": "text"
      },
      "source": [
        "# Trained pruned network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2xU6uWWAuz",
        "colab_type": "code",
        "outputId": "b487db66-0bff-4a5d-b1ce-14411600f905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data = 'AVOCADO'\n",
        "loaded_model = tf.keras.models.load_model(\"weights5-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE2WVq7DItg",
        "colab_type": "code",
        "outputId": "9e940b01-30e3-4458-8c2d-51585a39ad1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 16;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.5,\n",
        "                                                   final_sparsity=0.9,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adadelta',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "915\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.debugging.assert_greater_equal is deprecated. Please use tf.compat.v1.debugging.assert_greater_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py:59: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 32, 32, 5, 1)]    0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv3d_3 (None, 32, 32, 5, 16)     882       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_reshape_ (None, 32, 32, 80)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 32, 32, 80)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 32, 32, 320)       53522     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 32, 32, 320)       1281      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 32, 32, 320)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 16, 16, 256)       166978    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 16, 16, 256)       1025      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 16, 16, 256)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 8, 8, 256)         133634    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 8, 8, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_global_a (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_fc1 (Pru (None, 1)                 515       \n",
            "=================================================================\n",
            "Total params: 357,843\n",
            "Trainable params: 182,929\n",
            "Non-trainable params: 174,914\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYBXiSTtaDt",
        "colab_type": "code",
        "outputId": "4e300025-9d31-446c-d00c-d3a3809903da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 2\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "cva1 = []\n",
        "cva2 = []\n",
        "cva3 = []\n",
        "\n",
        "\n",
        "data = 'AVOCADO'\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "\n",
        "    ytrain = train_y[train]\n",
        "    ytest = train_y[test]\n",
        "\n",
        "    xtrain = train_x[train]\n",
        "    xtest = train_x[test]\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.05,\n",
        "                                                      final_sparsity=0.15,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    loaded_model.load_weights(\"weights5-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.binary_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # checkpoint\n",
        "    filepath = \"pruned5-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint, sparsity.UpdatePruningStep()]\n",
        "\n",
        "    # Train model on dataset\n",
        "    print(data + \": Training\" + str(ntrain) + \"begins...\")\n",
        "    history = new_pruned_model.fit(x=xtrain, y=ytrain, validation_data=(xtest, ytest),\n",
        "                        batch_size=8, epochs=epochs, callbacks=callbacks_list)\n",
        "    \n",
        "    new_pruned_model.load_weights(\"pruned5-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    # new_pruned_model.compile(\n",
        "    #     loss=tf.keras.losses.binary_crossentropy,\n",
        "    #     optimizer='adadelta',\n",
        "    #     metrics=['accuracy'])\n",
        "    ypred = new_pruned_model.predict(xtest)\n",
        "    ypred = ypred.round()\n",
        "    print(np.sum(ypred.transpose()==ytest)/len(ytest))\n",
        "    \n",
        "    ntrain += 1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AVOCADO: Training1begins...\n",
            "Train on 867 samples, validate on 97 samples\n",
            "Epoch 1/15\n",
            "864/867 [============================>.] - ETA: 0s - loss: 2.0978e-04 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to pruned5-weights-hyper3dnetAVOCADO1-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 5s 6ms/sample - loss: 2.0906e-04 - acc: 1.0000 - val_loss: 4.7784e-04 - val_acc: 1.0000\n",
            "Epoch 2/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 3.5938e-04 - acc: 1.0000\n",
            "Epoch 00002: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 3.5177e-04 - acc: 1.0000 - val_loss: 5.3623e-04 - val_acc: 1.0000\n",
            "Epoch 3/15\n",
            "856/867 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9988\n",
            "Epoch 00003: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0023 - acc: 0.9988 - val_loss: 4.9538e-04 - val_acc: 1.0000\n",
            "Epoch 4/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 1.3537e-04 - acc: 1.0000\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 1.3242e-04 - acc: 1.0000 - val_loss: 6.3441e-04 - val_acc: 1.0000\n",
            "Epoch 5/15\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9988\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0010 - acc: 0.9988 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 6/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 7/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 3.4122e-04 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 3.4026e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 8/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 1.2676e-04 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 4.1480e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
            "Epoch 9/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 1.4445e-04 - acc: 1.0000\n",
            "Epoch 00009: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 1.4593e-04 - acc: 1.0000 - val_loss: 9.2859e-04 - val_acc: 1.0000\n",
            "Epoch 10/15\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 00010: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 8.9931e-04 - val_acc: 1.0000\n",
            "Epoch 11/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 4.3295e-04 - acc: 1.0000\n",
            "Epoch 00011: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 4.1950e-04 - acc: 1.0000 - val_loss: 8.5124e-04 - val_acc: 1.0000\n",
            "Epoch 12/15\n",
            "856/867 [============================>.] - ETA: 0s - loss: 3.0869e-04 - acc: 1.0000\n",
            "Epoch 00012: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 3.0478e-04 - acc: 1.0000 - val_loss: 8.1733e-04 - val_acc: 1.0000\n",
            "Epoch 13/15\n",
            "832/867 [===========================>..] - ETA: 0s - loss: 2.9394e-04 - acc: 1.0000\n",
            "Epoch 00013: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 2.8216e-04 - acc: 1.0000 - val_loss: 8.5239e-04 - val_acc: 1.0000\n",
            "Epoch 14/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9988\n",
            "Epoch 00014: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0010 - acc: 0.9988 - val_loss: 7.9178e-04 - val_acc: 1.0000\n",
            "Epoch 15/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 7.7715e-04 - acc: 1.0000\n",
            "Epoch 00015: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 7.6012e-04 - acc: 1.0000 - val_loss: 8.6977e-04 - val_acc: 1.0000\n",
            "1.0\n",
            "AVOCADO: Training2begins...\n",
            "Train on 867 samples, validate on 97 samples\n",
            "Epoch 1/15\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9942\n",
            "Epoch 00001: val_acc improved from -inf to 0.92784, saving model to pruned5-weights-hyper3dnetAVOCADO2-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0173 - acc: 0.9942 - val_loss: 0.2150 - val_acc: 0.9278\n",
            "Epoch 2/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9988\n",
            "Epoch 00002: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0057 - acc: 0.9988 - val_loss: 0.2703 - val_acc: 0.9072\n",
            "Epoch 3/15\n",
            "856/867 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9977\n",
            "Epoch 00003: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0059 - acc: 0.9977 - val_loss: 0.2138 - val_acc: 0.9175\n",
            "Epoch 4/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9952\n",
            "Epoch 00004: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0104 - acc: 0.9954 - val_loss: 0.2141 - val_acc: 0.9175\n",
            "Epoch 5/15\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9977\n",
            "Epoch 00005: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0085 - acc: 0.9977 - val_loss: 0.1991 - val_acc: 0.9278\n",
            "Epoch 6/15\n",
            "856/867 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9977\n",
            "Epoch 00006: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0069 - acc: 0.9977 - val_loss: 0.2590 - val_acc: 0.8969\n",
            "Epoch 7/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9952\n",
            "Epoch 00007: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0156 - acc: 0.9954 - val_loss: 0.2608 - val_acc: 0.8969\n",
            "Epoch 8/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9965\n",
            "Epoch 00008: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0092 - acc: 0.9965 - val_loss: 0.2470 - val_acc: 0.8969\n",
            "Epoch 9/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9988\n",
            "Epoch 00009: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0076 - acc: 0.9988 - val_loss: 0.2408 - val_acc: 0.8969\n",
            "Epoch 10/15\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9965\n",
            "Epoch 00010: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0093 - acc: 0.9965 - val_loss: 0.2467 - val_acc: 0.8969\n",
            "Epoch 11/15\n",
            "856/867 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 00011: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 0.2520 - val_acc: 0.8969\n",
            "Epoch 12/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9976\n",
            "Epoch 00012: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0063 - acc: 0.9977 - val_loss: 0.2403 - val_acc: 0.8969\n",
            "Epoch 13/15\n",
            "856/867 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9988\n",
            "Epoch 00013: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0073 - acc: 0.9988 - val_loss: 0.2428 - val_acc: 0.8969\n",
            "Epoch 14/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9976\n",
            "Epoch 00014: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0097 - acc: 0.9977 - val_loss: 0.2437 - val_acc: 0.8969\n",
            "Epoch 15/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9976\n",
            "Epoch 00015: val_acc did not improve from 0.92784\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0066 - acc: 0.9977 - val_loss: 0.2442 - val_acc: 0.8969\n",
            "0.9278350515463918\n",
            "AVOCADO: Training3begins...\n",
            "Train on 867 samples, validate on 97 samples\n",
            "Epoch 1/15\n",
            "864/867 [============================>.] - ETA: 0s - loss: 3.9691e-04 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 0.97938, saving model to pruned5-weights-hyper3dnetAVOCADO3-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 3.9553e-04 - acc: 1.0000 - val_loss: 0.0638 - val_acc: 0.9794\n",
            "Epoch 2/15\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9988\n",
            "Epoch 00002: val_acc improved from 0.97938 to 0.98969, saving model to pruned5-weights-hyper3dnetAVOCADO3-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0012 - acc: 0.9988 - val_loss: 0.0180 - val_acc: 0.9897\n",
            "Epoch 3/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 1.4782e-04 - acc: 1.0000\n",
            "Epoch 00003: val_acc improved from 0.98969 to 1.00000, saving model to pruned5-weights-hyper3dnetAVOCADO3-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 1.4363e-04 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 1.0000\n",
            "Epoch 4/15\n",
            "856/867 [============================>.] - ETA: 0s - loss: 6.2432e-04 - acc: 1.0000\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 6.1643e-04 - acc: 1.0000 - val_loss: 0.0202 - val_acc: 0.9897\n",
            "Epoch 5/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 1.5240e-04 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 1.4987e-04 - acc: 1.0000 - val_loss: 0.0169 - val_acc: 1.0000\n",
            "Epoch 6/15\n",
            "856/867 [============================>.] - ETA: 0s - loss: 7.2058e-05 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 7.1437e-05 - acc: 1.0000 - val_loss: 0.0150 - val_acc: 1.0000\n",
            "Epoch 7/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 1.4516e-04 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 1.5871e-04 - acc: 1.0000 - val_loss: 0.0154 - val_acc: 1.0000\n",
            "Epoch 8/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 8.1823e-05 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 8.0066e-05 - acc: 1.0000 - val_loss: 0.0164 - val_acc: 1.0000\n",
            "Epoch 9/15\n",
            "856/867 [============================>.] - ETA: 0s - loss: 7.1445e-05 - acc: 1.0000\n",
            "Epoch 00009: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 7.0644e-05 - acc: 1.0000 - val_loss: 0.0169 - val_acc: 0.9897\n",
            "Epoch 10/15\n",
            "864/867 [============================>.] - ETA: 0s - loss: 2.3988e-04 - acc: 1.0000\n",
            "Epoch 00010: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 4.0107e-04 - acc: 1.0000 - val_loss: 0.0169 - val_acc: 0.9897\n",
            "Epoch 11/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 3.7130e-04 - acc: 1.0000\n",
            "Epoch 00011: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 3.6318e-04 - acc: 1.0000 - val_loss: 0.0171 - val_acc: 0.9897\n",
            "Epoch 12/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 2.5617e-04 - acc: 1.0000\n",
            "Epoch 00012: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 2.5132e-04 - acc: 1.0000 - val_loss: 0.0172 - val_acc: 0.9897\n",
            "Epoch 13/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 2.5476e-05 - acc: 1.0000\n",
            "Epoch 00013: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 2.4921e-05 - acc: 1.0000 - val_loss: 0.0178 - val_acc: 0.9897\n",
            "Epoch 14/15\n",
            "848/867 [============================>.] - ETA: 0s - loss: 6.9682e-05 - acc: 1.0000\n",
            "Epoch 00014: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 6.8214e-05 - acc: 1.0000 - val_loss: 0.0178 - val_acc: 0.9897\n",
            "Epoch 15/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 3.1654e-05 - acc: 1.0000\n",
            "Epoch 00015: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 8.5615e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9897\n",
            "1.0\n",
            "AVOCADO: Training4begins...\n",
            "Train on 867 samples, validate on 97 samples\n",
            "Epoch 1/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 1.1300e-04 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 0.97938, saving model to pruned5-weights-hyper3dnetAVOCADO4-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 1.0951e-04 - acc: 1.0000 - val_loss: 0.0746 - val_acc: 0.9794\n",
            "Epoch 2/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 9.8109e-06 - acc: 1.0000\n",
            "Epoch 00002: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 1.0079e-05 - acc: 1.0000 - val_loss: 0.1023 - val_acc: 0.9794\n",
            "Epoch 3/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 7.6146e-06 - acc: 1.0000\n",
            "Epoch 00003: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 0.0020 - acc: 0.9988 - val_loss: 0.0860 - val_acc: 0.9691\n",
            "Epoch 4/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 3.6450e-05 - acc: 1.0000\n",
            "Epoch 00004: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 3.5569e-05 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 0.9794\n",
            "Epoch 5/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 1.1229e-05 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 1.0880e-05 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9794\n",
            "Epoch 6/15\n",
            "856/867 [============================>.] - ETA: 0s - loss: 1.9672e-05 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 2.6358e-04 - acc: 1.0000 - val_loss: 0.0955 - val_acc: 0.9691\n",
            "Epoch 7/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 3.1109e-05 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 3.0145e-05 - acc: 1.0000 - val_loss: 0.0850 - val_acc: 0.9794\n",
            "Epoch 8/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 1.1149e-05 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 1.0840e-05 - acc: 1.0000 - val_loss: 0.0824 - val_acc: 0.9794\n",
            "Epoch 9/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 3.3763e-05 - acc: 1.0000\n",
            "Epoch 00009: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 4.2812e-05 - acc: 1.0000 - val_loss: 0.0772 - val_acc: 0.9794\n",
            "Epoch 10/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 7.5653e-06 - acc: 1.0000\n",
            "Epoch 00010: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 7.3821e-06 - acc: 1.0000 - val_loss: 0.0768 - val_acc: 0.9794\n",
            "Epoch 11/15\n",
            "864/867 [============================>.] - ETA: 0s - loss: 3.4932e-05 - acc: 1.0000\n",
            "Epoch 00011: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 3.4811e-05 - acc: 1.0000 - val_loss: 0.0767 - val_acc: 0.9794\n",
            "Epoch 12/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 3.1209e-05 - acc: 1.0000\n",
            "Epoch 00012: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 3.0346e-05 - acc: 1.0000 - val_loss: 0.0760 - val_acc: 0.9794\n",
            "Epoch 13/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 3.0082e-05 - acc: 1.0000\n",
            "Epoch 00013: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 2.9147e-05 - acc: 1.0000 - val_loss: 0.0765 - val_acc: 0.9794\n",
            "Epoch 14/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 6.7525e-05 - acc: 1.0000\n",
            "Epoch 00014: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 6.5423e-05 - acc: 1.0000 - val_loss: 0.0765 - val_acc: 0.9794\n",
            "Epoch 15/15\n",
            "840/867 [============================>.] - ETA: 0s - loss: 1.5273e-05 - acc: 1.0000\n",
            "Epoch 00015: val_acc did not improve from 0.97938\n",
            "867/867 [==============================] - 2s 2ms/sample - loss: 1.4806e-05 - acc: 1.0000 - val_loss: 0.0781 - val_acc: 0.9794\n",
            "0.979381443298969\n",
            "AVOCADO: Training5begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9988    \n",
            "Epoch 00001: val_acc improved from -inf to 0.94792, saving model to pruned5-weights-hyper3dnetAVOCADO5-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 4ms/sample - loss: 0.0029 - acc: 0.9988 - val_loss: 0.1148 - val_acc: 0.9479\n",
            "Epoch 2/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 2.1701e-04 - acc: 1.0000\n",
            "Epoch 00002: val_acc improved from 0.94792 to 0.96875, saving model to pruned5-weights-hyper3dnetAVOCADO5-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.3613e-04 - acc: 1.0000 - val_loss: 0.0757 - val_acc: 0.9688\n",
            "Epoch 3/15\n",
            "848/868 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9988\n",
            "Epoch 00003: val_acc improved from 0.96875 to 0.97917, saving model to pruned5-weights-hyper3dnetAVOCADO5-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0020 - acc: 0.9988 - val_loss: 0.0596 - val_acc: 0.9792\n",
            "Epoch 4/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9988\n",
            "Epoch 00004: val_acc did not improve from 0.97917\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0016 - acc: 0.9988 - val_loss: 0.0605 - val_acc: 0.9688\n",
            "Epoch 5/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9988\n",
            "Epoch 00005: val_acc did not improve from 0.97917\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0015 - acc: 0.9988 - val_loss: 0.0565 - val_acc: 0.9792\n",
            "Epoch 6/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9988\n",
            "Epoch 00006: val_acc did not improve from 0.97917\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0922 - val_acc: 0.9688\n",
            "Epoch 7/15\n",
            "848/868 [============================>.] - ETA: 0s - loss: 4.4977e-04 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 0.97917\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 4.3945e-04 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9688\n",
            "Epoch 8/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 0.97917\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9688\n",
            "Epoch 9/15\n",
            "848/868 [============================>.] - ETA: 0s - loss: 9.2052e-04 - acc: 1.0000\n",
            "Epoch 00009: val_acc did not improve from 0.97917\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 8.9943e-04 - acc: 1.0000 - val_loss: 0.0533 - val_acc: 0.9792\n",
            "Epoch 10/15\n",
            "848/868 [============================>.] - ETA: 0s - loss: 2.1015e-04 - acc: 1.0000\n",
            "Epoch 00010: val_acc improved from 0.97917 to 0.98958, saving model to pruned5-weights-hyper3dnetAVOCADO5-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.0548e-04 - acc: 1.0000 - val_loss: 0.0502 - val_acc: 0.9896\n",
            "Epoch 11/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 2.3703e-04 - acc: 1.0000\n",
            "Epoch 00011: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.4669e-04 - acc: 1.0000 - val_loss: 0.0486 - val_acc: 0.9896\n",
            "Epoch 12/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
            "Epoch 00012: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0021 - acc: 0.9988 - val_loss: 0.0496 - val_acc: 0.9896\n",
            "Epoch 13/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
            "Epoch 00013: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0020 - acc: 0.9988 - val_loss: 0.0493 - val_acc: 0.9896\n",
            "Epoch 14/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9988\n",
            "Epoch 00014: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0017 - acc: 0.9988 - val_loss: 0.0476 - val_acc: 0.9896\n",
            "Epoch 15/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 5.6868e-04 - acc: 1.0000\n",
            "Epoch 00015: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 5.9010e-04 - acc: 1.0000 - val_loss: 0.0494 - val_acc: 0.9896\n",
            "0.9895833333333334\n",
            "AVOCADO: Training6begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 9.9466e-05 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to pruned5-weights-hyper3dnetAVOCADO6-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 4ms/sample - loss: 1.1121e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
            "Epoch 2/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 3.2879e-05 - acc: 1.0000\n",
            "Epoch 00002: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.2912e-05 - acc: 1.0000 - val_loss: 6.2938e-04 - val_acc: 1.0000\n",
            "Epoch 3/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 1.9809e-04 - acc: 1.0000\n",
            "Epoch 00003: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.5680e-04 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
            "Epoch 4/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 4.8513e-04 - acc: 1.0000\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 4.6955e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
            "Epoch 5/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9988\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0024 - acc: 0.9988 - val_loss: 0.0032 - val_acc: 1.0000\n",
            "Epoch 6/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 2.4004e-04 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.3239e-04 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
            "Epoch 7/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 5.5321e-04 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 5.4556e-04 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 1.0000\n",
            "Epoch 8/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 2.4649e-04 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.4313e-04 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
            "Epoch 9/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 4.5275e-04 - acc: 1.0000\n",
            "Epoch 00009: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 4.3824e-04 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
            "Epoch 10/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 3.2844e-04 - acc: 1.0000\n",
            "Epoch 00010: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.2693e-04 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
            "Epoch 11/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9988\n",
            "Epoch 00011: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0026 - acc: 0.9988 - val_loss: 0.0044 - val_acc: 1.0000\n",
            "Epoch 12/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 6.8752e-04 - acc: 1.0000\n",
            "Epoch 00012: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 6.6541e-04 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
            "Epoch 13/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 3.9510e-04 - acc: 1.0000\n",
            "Epoch 00013: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.8383e-04 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
            "Epoch 14/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 6.7538e-04 - acc: 1.0000\n",
            "Epoch 00014: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 6.7531e-04 - acc: 1.0000 - val_loss: 0.0039 - val_acc: 1.0000\n",
            "Epoch 15/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9988    \n",
            "Epoch 00015: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0019 - acc: 0.9988 - val_loss: 0.0041 - val_acc: 1.0000\n",
            "1.0\n",
            "AVOCADO: Training7begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 1.9521e-05 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 0.97917, saving model to pruned5-weights-hyper3dnetAVOCADO7-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 4ms/sample - loss: 3.9140e-05 - acc: 1.0000 - val_loss: 0.0347 - val_acc: 0.9792\n",
            "Epoch 2/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 5.4963e-04 - acc: 1.0000\n",
            "Epoch 00002: val_acc improved from 0.97917 to 0.98958, saving model to pruned5-weights-hyper3dnetAVOCADO7-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 5.3219e-04 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9896\n",
            "Epoch 3/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 4.0009e-05 - acc: 1.0000\n",
            "Epoch 00003: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.9859e-05 - acc: 1.0000 - val_loss: 0.0247 - val_acc: 0.9896\n",
            "Epoch 4/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9988\n",
            "Epoch 00004: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0024 - acc: 0.9988 - val_loss: 0.0222 - val_acc: 0.9896\n",
            "Epoch 5/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 3.3346e-05 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.2898e-05 - acc: 1.0000 - val_loss: 0.0342 - val_acc: 0.9896\n",
            "Epoch 6/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 1.4858e-05 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 1.4656e-05 - acc: 1.0000 - val_loss: 0.0241 - val_acc: 0.9896\n",
            "Epoch 7/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 9.4844e-05 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 9.1808e-05 - acc: 1.0000 - val_loss: 0.0231 - val_acc: 0.9896\n",
            "Epoch 8/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 8.4280e-04 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 8.2673e-04 - acc: 1.0000 - val_loss: 0.0224 - val_acc: 0.9896\n",
            "Epoch 9/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 1.1608e-04 - acc: 1.0000\n",
            "Epoch 00009: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 1.1554e-04 - acc: 1.0000 - val_loss: 0.0212 - val_acc: 0.9896\n",
            "Epoch 10/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 7.5980e-04 - acc: 1.0000\n",
            "Epoch 00010: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 7.3529e-04 - acc: 1.0000 - val_loss: 0.0225 - val_acc: 0.9896\n",
            "Epoch 11/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 4.5632e-05 - acc: 1.0000\n",
            "Epoch 00011: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 4.4318e-05 - acc: 1.0000 - val_loss: 0.0217 - val_acc: 0.9896\n",
            "Epoch 12/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 6.8909e-05 - acc: 1.0000\n",
            "Epoch 00012: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 6.7578e-05 - acc: 1.0000 - val_loss: 0.0227 - val_acc: 0.9896\n",
            "Epoch 13/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 3.0280e-05 - acc: 1.0000\n",
            "Epoch 00013: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.9866e-05 - acc: 1.0000 - val_loss: 0.0225 - val_acc: 0.9896\n",
            "Epoch 14/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 1.2656e-04 - acc: 1.0000\n",
            "Epoch 00014: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 1.2604e-04 - acc: 1.0000 - val_loss: 0.0236 - val_acc: 0.9896\n",
            "Epoch 15/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 3.3573e-05 - acc: 1.0000\n",
            "Epoch 00015: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.4096e-05 - acc: 1.0000 - val_loss: 0.0227 - val_acc: 0.9896\n",
            "0.9895833333333334\n",
            "AVOCADO: Training8begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 6.1574e-05 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 0.97917, saving model to pruned5-weights-hyper3dnetAVOCADO8-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 4ms/sample - loss: 5.9750e-05 - acc: 1.0000 - val_loss: 0.0445 - val_acc: 0.9792\n",
            "Epoch 2/15\n",
            "848/868 [============================>.] - ETA: 0s - loss: 5.3322e-04 - acc: 1.0000\n",
            "Epoch 00002: val_acc improved from 0.97917 to 0.98958, saving model to pruned5-weights-hyper3dnetAVOCADO8-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 5.2094e-04 - acc: 1.0000 - val_loss: 0.0366 - val_acc: 0.9896\n",
            "Epoch 3/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 4.8125e-04 - acc: 1.0000\n",
            "Epoch 00003: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 4.6579e-04 - acc: 1.0000 - val_loss: 0.0403 - val_acc: 0.9792\n",
            "Epoch 4/15\n",
            "848/868 [============================>.] - ETA: 0s - loss: 5.2220e-05 - acc: 1.0000\n",
            "Epoch 00004: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 5.1044e-05 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9792\n",
            "Epoch 5/15\n",
            "848/868 [============================>.] - ETA: 0s - loss: 1.4441e-04 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 1.4240e-04 - acc: 1.0000 - val_loss: 0.0380 - val_acc: 0.9792\n",
            "Epoch 6/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 1.3636e-04 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 1.3573e-04 - acc: 1.0000 - val_loss: 0.0433 - val_acc: 0.9792\n",
            "Epoch 7/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 3.4097e-05 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.3209e-05 - acc: 1.0000 - val_loss: 0.0338 - val_acc: 0.9896\n",
            "Epoch 8/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 4.1424e-05 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 4.2216e-05 - acc: 1.0000 - val_loss: 0.0327 - val_acc: 0.9896\n",
            "Epoch 9/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 1.0959e-04 - acc: 1.0000\n",
            "Epoch 00009: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 1.1076e-04 - acc: 1.0000 - val_loss: 0.0320 - val_acc: 0.9896\n",
            "Epoch 10/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9976\n",
            "Epoch 00010: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0040 - acc: 0.9977 - val_loss: 0.0321 - val_acc: 0.9896\n",
            "Epoch 11/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 4.1696e-05 - acc: 1.0000\n",
            "Epoch 00011: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 4.0956e-05 - acc: 1.0000 - val_loss: 0.0312 - val_acc: 0.9896\n",
            "Epoch 12/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 3.7789e-05 - acc: 1.0000\n",
            "Epoch 00012: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.6575e-05 - acc: 1.0000 - val_loss: 0.0305 - val_acc: 0.9896\n",
            "Epoch 13/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9988    \n",
            "Epoch 00013: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0304 - val_acc: 0.9896\n",
            "Epoch 14/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 00014: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0314 - val_acc: 0.9896\n",
            "Epoch 15/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 8.1817e-05 - acc: 1.0000\n",
            "Epoch 00015: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 8.1450e-05 - acc: 1.0000 - val_loss: 0.0319 - val_acc: 0.9896\n",
            "0.9895833333333334\n",
            "AVOCADO: Training9begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 9.8400e-06 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to pruned5-weights-hyper3dnetAVOCADO9-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 4ms/sample - loss: 1.0025e-05 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
            "Epoch 2/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 1.2375e-05 - acc: 1.0000\n",
            "Epoch 00002: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 7.1905e-04 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 0.9896\n",
            "Epoch 3/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 1.3888e-05 - acc: 1.0000\n",
            "Epoch 00003: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 1.3591e-05 - acc: 1.0000 - val_loss: 0.0214 - val_acc: 0.9896\n",
            "Epoch 4/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 5.6311e-04 - acc: 1.0000\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 5.6051e-04 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 0.9896\n",
            "Epoch 5/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 1.4588e-04 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 1.4152e-04 - acc: 1.0000 - val_loss: 0.0107 - val_acc: 0.9896\n",
            "Epoch 6/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 3.7854e-05 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.7681e-05 - acc: 1.0000 - val_loss: 0.0107 - val_acc: 0.9896\n",
            "Epoch 7/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 4.0961e-05 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.9980e-05 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9896\n",
            "Epoch 8/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 2.0402e-05 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.0594e-05 - acc: 1.0000 - val_loss: 0.0144 - val_acc: 0.9896\n",
            "Epoch 9/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 4.7608e-05 - acc: 1.0000\n",
            "Epoch 00009: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 4.6301e-05 - acc: 1.0000 - val_loss: 0.0135 - val_acc: 0.9896\n",
            "Epoch 10/15\n",
            "848/868 [============================>.] - ETA: 0s - loss: 2.3013e-04 - acc: 1.0000\n",
            "Epoch 00010: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.2484e-04 - acc: 1.0000 - val_loss: 0.0135 - val_acc: 0.9896\n",
            "Epoch 11/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 5.7130e-05 - acc: 1.0000\n",
            "Epoch 00011: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 5.6981e-05 - acc: 1.0000 - val_loss: 0.0129 - val_acc: 0.9896\n",
            "Epoch 12/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 4.5097e-05 - acc: 1.0000\n",
            "Epoch 00012: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 4.3646e-05 - acc: 1.0000 - val_loss: 0.0130 - val_acc: 0.9896\n",
            "Epoch 13/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 2.7957e-05 - acc: 1.0000\n",
            "Epoch 00013: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.7828e-05 - acc: 1.0000 - val_loss: 0.0137 - val_acc: 0.9896\n",
            "Epoch 14/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 3.8293e-04 - acc: 1.0000\n",
            "Epoch 00014: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 3.7060e-04 - acc: 1.0000 - val_loss: 0.0132 - val_acc: 0.9896\n",
            "Epoch 15/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 4.4839e-05 - acc: 1.0000\n",
            "Epoch 00015: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 4.4632e-05 - acc: 1.0000 - val_loss: 0.0124 - val_acc: 0.9896\n",
            "1.0\n",
            "AVOCADO: Training10begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 2.6994e-04 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 0.97917, saving model to pruned5-weights-hyper3dnetAVOCADO10-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 4ms/sample - loss: 2.6632e-04 - acc: 1.0000 - val_loss: 0.0728 - val_acc: 0.9792\n",
            "Epoch 2/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 7.6854e-04 - acc: 1.0000\n",
            "Epoch 00002: val_acc improved from 0.97917 to 0.98958, saving model to pruned5-weights-hyper3dnetAVOCADO10-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 7.5792e-04 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9896\n",
            "Epoch 3/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 1.2512e-04 - acc: 1.0000\n",
            "Epoch 00003: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 1.3005e-04 - acc: 1.0000 - val_loss: 0.0479 - val_acc: 0.9688\n",
            "Epoch 4/15\n",
            "848/868 [============================>.] - ETA: 0s - loss: 6.2715e-04 - acc: 1.0000\n",
            "Epoch 00004: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 6.5837e-04 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 0.9896\n",
            "Epoch 5/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 2.9393e-04 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.9258e-04 - acc: 1.0000 - val_loss: 0.0505 - val_acc: 0.9896\n",
            "Epoch 6/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 2.2759e-04 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.3016e-04 - acc: 1.0000 - val_loss: 0.0556 - val_acc: 0.9896\n",
            "Epoch 7/15\n",
            "848/868 [============================>.] - ETA: 0s - loss: 8.5369e-05 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 8.3537e-05 - acc: 1.0000 - val_loss: 0.0604 - val_acc: 0.9896\n",
            "Epoch 8/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9988\n",
            "Epoch 00008: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0021 - acc: 0.9988 - val_loss: 0.0666 - val_acc: 0.9688\n",
            "Epoch 9/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9988\n",
            "Epoch 00009: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0033 - acc: 0.9988 - val_loss: 0.0715 - val_acc: 0.9688\n",
            "Epoch 10/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9988\n",
            "Epoch 00010: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0067 - acc: 0.9988 - val_loss: 0.0669 - val_acc: 0.9688\n",
            "Epoch 11/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 2.7863e-04 - acc: 1.0000\n",
            "Epoch 00011: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 2.7737e-04 - acc: 1.0000 - val_loss: 0.0639 - val_acc: 0.9688\n",
            "Epoch 12/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9988\n",
            "Epoch 00012: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0019 - acc: 0.9988 - val_loss: 0.0651 - val_acc: 0.9688\n",
            "Epoch 13/15\n",
            "840/868 [============================>.] - ETA: 0s - loss: 4.6701e-04 - acc: 1.0000\n",
            "Epoch 00013: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 8.7455e-04 - acc: 1.0000 - val_loss: 0.0661 - val_acc: 0.9688\n",
            "Epoch 14/15\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9988\n",
            "Epoch 00014: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0673 - val_acc: 0.9688\n",
            "Epoch 15/15\n",
            "856/868 [============================>.] - ETA: 0s - loss: 8.8630e-04 - acc: 1.0000\n",
            "Epoch 00015: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 2s 2ms/sample - loss: 8.7413e-04 - acc: 1.0000 - val_loss: 0.0642 - val_acc: 0.9792\n",
            "0.9895833333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmz9PSRTvPP",
        "colab_type": "code",
        "outputId": "679c37a3-988b-4a2d-a8d7-83061671d6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "new_pruned_model.load_weights(\"pruned5-weights-hyper3dnet\" + data + str(3) + \"-best_3layers_4filters.h5\")\n",
        "final_model = sparsity.strip_pruning(new_pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 32, 32, 5, 1)]    0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 32, 32, 5, 16)     448       \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 32, 32, 80)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32, 32, 80)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_7 (Separabl (None, 32, 32, 320)       27920     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 32, 32, 320)       1280      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 32, 32, 320)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_8 (Separabl (None, 16, 16, 256)       85056     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_9 (Separabl (None, 8, 8, 256)         68096     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 184,081\n",
            "Trainable params: 182,929\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8drkv1mK1CUb",
        "colab_type": "code",
        "outputId": "61ebcb21-1d46-4057-88eb-a02cca3b3f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "import numpy as np\n",
        "for i, w in enumerate(final_model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.4f}%\".format(\n",
        "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv3d_3/kernel:0 -- Total:432, Zeros: 12.0370%\n",
            "conv3d_3/bias:0 -- Total:16, Zeros: 0.0000%\n",
            "separable_conv2d_7/depthwise_kernel:0 -- Total:2000, Zeros: 0.0000%\n",
            "separable_conv2d_7/pointwise_kernel:0 -- Total:25600, Zeros: 11.9648%\n",
            "separable_conv2d_7/bias:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_5/gamma:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_5/beta:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_5/moving_mean:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_5/moving_variance:0 -- Total:320, Zeros: 0.0000%\n",
            "separable_conv2d_8/depthwise_kernel:0 -- Total:2880, Zeros: 0.0000%\n",
            "separable_conv2d_8/pointwise_kernel:0 -- Total:81920, Zeros: 11.9641%\n",
            "separable_conv2d_8/bias:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_6/gamma:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_6/beta:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_6/moving_mean:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_6/moving_variance:0 -- Total:256, Zeros: 0.0000%\n",
            "separable_conv2d_9/depthwise_kernel:0 -- Total:2304, Zeros: 0.0000%\n",
            "separable_conv2d_9/pointwise_kernel:0 -- Total:65536, Zeros: 11.9629%\n",
            "separable_conv2d_9/bias:0 -- Total:256, Zeros: 0.0000%\n",
            "fc1/kernel:0 -- Total:256, Zeros: 12.1094%\n",
            "fc1/bias:0 -- Total:1, Zeros: 0.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIRfZDn02rV",
        "colab_type": "code",
        "outputId": "860aa1b2-c321-4da7-df9f-395610030a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Saving pruned model to: ', \"Avocado_hyper3DNet_pruned.h5\")\n",
        "tf.keras.models.save_model(final_model, \"Avocado_hyper3DNet_pruned_5bands.h5\", \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  Avocado_hyper3DNet_pruned.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tkgm44OZDt",
        "colab_type": "text"
      },
      "source": [
        "# Predict and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVT1KBWvEODg",
        "colab_type": "code",
        "outputId": "89ad01b6-34ed-420a-c7f3-b7b44d5ca6ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import truediv\n",
        "import h5py\n",
        "import pickle\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Prepare lists to save metrics\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 2\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_m):\n",
        "    list_diag = np.diag(confusion_m)\n",
        "    list_raw_sum = np.sum(confusion_m, axis=1)\n",
        "    each_ac = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_ac)\n",
        "    return each_ac, average_acc\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "PREDICT AND CALCULATE METRICS\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "loaded_model = tf.keras.models.load_model(\"weights5-hyper3dnet\" + 'AVOCADO' + str(1) + \"-best_3layers_4filters.h5\")\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "dataset = 'AVOCADO'\n",
        "# Initialize\n",
        "confmatrices = np.zeros((10, int(classes), int(classes)))\n",
        "\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "    ytest = train_y[test]\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.30,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.load_weights(\"pruned5-weights-hyper3dnet\" + dataset + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "    ypred = ypred.round()\n",
        "\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        con_mat = tf.math.confusion_matrix(labels=ytest,\n",
        "                                           predictions=ypred).numpy()\n",
        "\n",
        "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "    classes_list = list(range(0, int(classes)))\n",
        "    con_mat_df = pd.DataFrame(con_mat_norm, index=classes_list, columns=classes_list)\n",
        "\n",
        "    confmatrices[ntrain - 1, :, :] = con_mat_df.values\n",
        "\n",
        "    # Calculate metrics\n",
        "    oa = accuracy_score(ytest, ypred)\n",
        "    confusion = confusion_matrix(ytest, ypred)\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(ytest, ypred)\n",
        "    prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
        "\n",
        "    # Add metrics to the list\n",
        "    cvoa.append(oa * 100)\n",
        "    cvaa.append(aa * 100)\n",
        "    cvka.append(kappa * 100)\n",
        "    cvpre.append(prec * 100)\n",
        "    cvrec.append(rec * 100)\n",
        "    cvf1.append(f1 * 100)\n",
        "\n",
        "    ntrain += 1\n",
        "\n",
        "file_name = \"classification_report_hyper3dnet_pruned_5bands_\" + dataset + \".txt\"\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write(\"Overall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvoa)), float(np.std(cvoa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Average accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvaa)), float(np.std(cvaa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Kappa accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvka)), float(np.std(cvka))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Precision accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvpre)), float(np.std(cvpre))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Recall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvrec)), float(np.std(cvrec))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"F1 accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvf1)), float(np.std(cvf1))))\n",
        "\n",
        "# Calculate mean and std\n",
        "means = np.mean(confmatrices * 100, axis=0)\n",
        "stds = np.std(confmatrices * 100, axis=0)\n",
        "\n",
        "def plot_confusion_matrix(cm, cms, classescf,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classescf))\n",
        "    plt.xticks(tick_marks, classescf, rotation=45)\n",
        "    plt.yticks(tick_marks, classescf)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "\n",
        "            if (cm[i, j] == 100 or cm[i, j] == 0) and cms[i, j] == 0:\n",
        "                plt.text(j, i, '{0:.0f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.0f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "            else:\n",
        "                plt.text(j, i, '{0:.2f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.2f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "with open('meanshyper3dnet5p', 'wb') as f:\n",
        "    pickle.dump(means, f)\n",
        "with open('stdshyper3dnet5p', 'wb') as f:\n",
        "    pickle.dump(stds, f)\n",
        "with open('cvf1hyper3dnet5p', 'wb') as f:\n",
        "    pickle.dump(cvf1, f)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "classes_list = list(range(0, int(classes)))\n",
        "plt.figure()\n",
        "plot_confusion_matrix(means, stds, classescf=classes_list)\n",
        "\n",
        "plt.savefig('MatrixConfusion_AVOCADO_hyper3dnet_pruned_5bands.png', dpi=1200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAEmCAYAAAA5jbhCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/2BqM600Kh0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}