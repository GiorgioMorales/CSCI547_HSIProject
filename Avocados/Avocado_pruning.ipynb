{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Avocado_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyQTRzMcUCB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare tensorflow-model-optimization for pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNPotwubYRu",
        "colab_type": "code",
        "outputId": "f8594ed4-71ce-457a-f4ae-1a147cffcfb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tensorflow-gpu==1.14.0\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 54.9MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 57.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c4/4c3d011e432bd9c19f0323f7da7d3f783402615e4c3b5a98416c7da9cb05/tensorflow_model_optimization-0.2.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.17.5)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Collecting enum34~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/df/6e/46a0304561a07bda942c4283b67228d5b87d1ebd1189c866ede4b4cd6a0a/enum34-1.1.9-py3-none-any.whl\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.9 tensorflow-model-optimization-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfBLXz1mCNV",
        "colab_type": "code",
        "outputId": "01ada922-f908-4dc7-b39e-a979a023c87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk1dYHpbicZ",
        "colab_type": "text"
      },
      "source": [
        "#Download dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu64LC4Wm1oc",
        "colab_type": "code",
        "outputId": "42fa879e-4a2b-4479-a2eb-e6fcc2acab5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWT-iIqSb_46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://drive.google.com/open?id=1Wz2xqSZSjLDzXXTLdkf6zyTUUcBopn3i\n",
        "download = drive.CreateFile({'id': '1Wz2xqSZSjLDzXXTLdkf6zyTUUcBopn3i'})\n",
        "download.GetContentFile('avocado_dataset_w64.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDlBcilciuY",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRqt2u-MVfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from operator import truediv\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Data augmentation functions\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "def add_rotation_flip(x, y):\n",
        "\n",
        "    x = np.reshape(x, (x.shape[0], x.shape[1], x.shape[2], x.shape[3], 1))\n",
        "\n",
        "    # Flip horizontally\n",
        "    x_h = np.flip(x[:, :, :, :, :], 1)\n",
        "    # Flip vertically\n",
        "    x_v = np.flip(x[:, :, :, :, :], 2)\n",
        "    # Flip horizontally and vertically\n",
        "    x_hv = np.flip(x_h[:, :, :, :, :], 2)\n",
        "\n",
        "    # Concatenate\n",
        "    x = np.concatenate((x, x_h, x_v, x_hv))\n",
        "    y = np.concatenate((y, y, y, y))\n",
        "\n",
        "    return x, y\n",
        "    \n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "LOAD HDF5 FILE\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "hdf5_file = h5py.File('avocado_dataset_w64.hdf5', \"r\")\n",
        "train_x = np.array(hdf5_file[\"train_img\"][...])\n",
        "train_y = np.array(hdf5_file[\"train_labels\"][...])\n",
        "\n",
        "# Average consecutive bands\n",
        "img2 = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], int(train_x.shape[3] / 2)))\n",
        "for n in range(0, train_x.shape[0]):\n",
        "    for i in range(0, train_x.shape[3], 2):\n",
        "        img2[n, :, :, int(i / 2)] = (train_x[n, :, :, i] + train_x[n, :, :, i + 1]) / 2.\n",
        "\n",
        "train_x = img2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3wrK8dNe-T7",
        "colab_type": "code",
        "outputId": "4fda6bda-6cc4-4c32-dd98-7c7fbd0bb28e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x, train_y = add_rotation_flip(train_x, train_y)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3]))\n",
        "\n",
        "# temp = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], 20))\n",
        "\n",
        "# temp[:, :, :, 0] = train_x[:, :, :, 10]\n",
        "# temp[:, :, :, 1] = train_x[:, :, :, 15] \n",
        "# temp[:, :, :, 2] = train_x[:, :, :,20] \n",
        "# temp[:, :, :, 3] = train_x[:, :, :,22] \n",
        "# temp[:, :, :, 4] = train_x[:, :, :,24] \n",
        "# temp[:, :, :, 5] = train_x[:, :, :, 27] \n",
        "# temp[:, :, :, 6] = train_x[:, :, :, 30]\n",
        "# temp[:, :, :, 7] = train_x[:, :, :, 40]\n",
        "# temp[:, :, :, 8] = train_x[:, :, :,55]\n",
        "# temp[:, :, :, 9] = train_x[:, :, :, 36]  \n",
        "# temp[:, :, :, 10] = train_x[:, :, :, 59] \n",
        "# temp[:, :, :, 11] = train_x[:, :, :, 61]    \n",
        "# temp[:, :, :, 12] = train_x[:, :, :, 66] \n",
        "# temp[:, :, :, 13] = train_x[:, :, :, 73]\n",
        "# temp[:, :, :, 14] = train_x[:, :, :, 114]\n",
        "# temp[:, :, :, 15] = train_x[:, :, :, 128]\n",
        "# temp[:, :, :, 16] = train_x[:, :, :, 131]\n",
        "# temp[:, :, :, 17] = train_x[:, :, :, 136] \n",
        "# temp[:, :, :, 18] = train_x[:, :, :, 141]\n",
        "# temp[:, :, :, 19] = train_x[:, :, :, 144]\n",
        "# train_x = temp\n",
        "\n",
        "print(train_x.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(964, 64, 64, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvx55MBgcrZJ",
        "colab_type": "text"
      },
      "source": [
        "# Trained pruned network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2xU6uWWAuz",
        "colab_type": "code",
        "outputId": "9e1aee39-5b67-4a4e-a3f1-1efc1a994d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data = 'AVOCADO'\n",
        "loaded_model = tf.keras.models.load_model(\"weights-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE2WVq7DItg",
        "colab_type": "code",
        "outputId": "79607de6-3ff5-4b5c-e526-6eb06428a59b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.5,\n",
        "                                                   final_sparsity=0.9,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adadelta',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "248\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 64, 64, 150)]     0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 64, 64, 96)        32648     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 64, 64, 96)        385       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 64, 64, 96)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 32, 32, 96)        19394     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 32, 32, 96)        385       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 32, 32, 96)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 16, 16, 96)        19394     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 16, 16, 96)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_global_a (None, 96)                1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_fc1 (Pru (None, 1)                 195       \n",
            "=================================================================\n",
            "Total params: 72,405\n",
            "Trainable params: 39,079\n",
            "Non-trainable params: 33,326\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYBXiSTtaDt",
        "colab_type": "code",
        "outputId": "6e3d8685-2cd8-46e2-83af-e36da64e3b7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 2\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "cva1 = []\n",
        "cva2 = []\n",
        "cva3 = []\n",
        "\n",
        "\n",
        "data = 'AVOCADO'\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "\n",
        "    ytrain = train_y[train]\n",
        "    ytest = train_y[test]\n",
        "\n",
        "    xtrain = train_x[train]\n",
        "    xtest = train_x[test]\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.01,\n",
        "                                                      final_sparsity=0.05,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    loaded_model.load_weights(\"weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.binary_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # checkpoint\n",
        "    filepath = \"pruned-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint, sparsity.UpdatePruningStep()]\n",
        "\n",
        "    # Train model on dataset\n",
        "    print(data + \": Training\" + str(ntrain) + \"begins...\")\n",
        "    history = new_pruned_model.fit(x=xtrain, y=ytrain, validation_data=(xtest, ytest),\n",
        "                        batch_size=32, epochs=epochs, callbacks=callbacks_list)\n",
        "    \n",
        "    new_pruned_model.load_weights(\"pruned-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.binary_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    ypred = new_pruned_model.predict(xtest)\n",
        "    print(np.sum(ypred.transpose==ytest)/len(ytest))\n",
        "    \n",
        "    ntrain += 1\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AVOCADO: Training1begins...\n",
            "Train on 867 samples, validate on 97 samples\n",
            "Epoch 1/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9954\n",
            "Epoch 00001: val_acc improved from -inf to 0.97938, saving model to pruned-weights-hyper3dnetAVOCADO1-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 5s 6ms/sample - loss: 0.0270 - acc: 0.9954 - val_loss: 0.0558 - val_acc: 0.9794\n",
            "Epoch 2/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9965\n",
            "Epoch 00002: val_acc improved from 0.97938 to 0.98969, saving model to pruned-weights-hyper3dnetAVOCADO1-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0266 - acc: 0.9965 - val_loss: 0.0355 - val_acc: 0.9897\n",
            "Epoch 3/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9977\n",
            "Epoch 00003: val_acc improved from 0.98969 to 1.00000, saving model to pruned-weights-hyper3dnetAVOCADO1-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0263 - acc: 0.9977 - val_loss: 0.0281 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9954\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0223 - acc: 0.9954 - val_loss: 0.0279 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9942\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0245 - acc: 0.9942 - val_loss: 0.0267 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9977\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0200 - acc: 0.9965 - val_loss: 0.0258 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9931\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0252 - acc: 0.9931 - val_loss: 0.0248 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9988\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0199 - acc: 0.9988 - val_loss: 0.0286 - val_acc: 0.9897\n",
            "2.845360824742268\n",
            "AVOCADO: Training2begins...\n",
            "Train on 867 samples, validate on 97 samples\n",
            "Epoch 1/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9977\n",
            "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to pruned-weights-hyper3dnetAVOCADO2-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 6s 7ms/sample - loss: 0.0102 - acc: 0.9977 - val_loss: 0.0120 - val_acc: 1.0000\n",
            "Epoch 2/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 1.0000\n",
            "Epoch 00002: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0083 - acc: 1.0000 - val_loss: 0.0102 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 00003: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 1.0000\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0093 - acc: 1.0000 - val_loss: 0.0093 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0092 - acc: 1.0000 - val_loss: 0.0092 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0090 - acc: 0.9988 - val_loss: 0.0092 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 4ms/sample - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0096 - val_acc: 1.0000\n",
            "2.8969072164948453\n",
            "AVOCADO: Training3begins...\n",
            "Train on 867 samples, validate on 97 samples\n",
            "Epoch 1/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9977\n",
            "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to pruned-weights-hyper3dnetAVOCADO3-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 6s 7ms/sample - loss: 0.0179 - acc: 0.9977 - val_loss: 0.0292 - val_acc: 1.0000\n",
            "Epoch 2/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9988\n",
            "Epoch 00002: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0149 - acc: 0.9988 - val_loss: 0.0229 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 1.0000\n",
            "Epoch 00003: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0177 - acc: 0.9988 - val_loss: 0.0198 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9988\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0179 - acc: 0.9988 - val_loss: 0.0197 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9988\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0172 - acc: 0.9977 - val_loss: 0.0196 - val_acc: 0.9897\n",
            "Epoch 6/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0134 - acc: 1.0000 - val_loss: 0.0194 - val_acc: 0.9897\n",
            "Epoch 7/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0145 - acc: 1.0000 - val_loss: 0.0194 - val_acc: 0.9897\n",
            "Epoch 8/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0200 - acc: 0.9988 - val_loss: 0.0214 - val_acc: 0.9897\n",
            "1.8969072164948453\n",
            "AVOCADO: Training4begins...\n",
            "Train on 867 samples, validate on 97 samples\n",
            "Epoch 1/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9942\n",
            "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to pruned-weights-hyper3dnetAVOCADO4-best_3layers_4filters.h5\n",
            "867/867 [==============================] - 6s 7ms/sample - loss: 0.0372 - acc: 0.9942 - val_loss: 0.0421 - val_acc: 1.0000\n",
            "Epoch 2/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9907\n",
            "Epoch 00002: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0439 - acc: 0.9908 - val_loss: 0.0367 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9965\n",
            "Epoch 00003: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0365 - acc: 0.9954 - val_loss: 0.0345 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9907\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0419 - acc: 0.9908 - val_loss: 0.0354 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9907\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0430 - acc: 0.9885 - val_loss: 0.0336 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9931\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0358 - acc: 0.9931 - val_loss: 0.0334 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9931\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0438 - acc: 0.9919 - val_loss: 0.0335 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "864/867 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9954\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "867/867 [==============================] - 4s 5ms/sample - loss: 0.0374 - acc: 0.9954 - val_loss: 0.0336 - val_acc: 1.0000\n",
            "1.422680412371134\n",
            "AVOCADO: Training5begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9988\n",
            "Epoch 00001: val_acc improved from -inf to 0.94792, saving model to pruned-weights-hyper3dnetAVOCADO5-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 6s 7ms/sample - loss: 0.0213 - acc: 0.9988 - val_loss: 0.1003 - val_acc: 0.9479\n",
            "Epoch 2/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9965\n",
            "Epoch 00002: val_acc improved from 0.94792 to 0.96875, saving model to pruned-weights-hyper3dnetAVOCADO5-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0267 - acc: 0.9965 - val_loss: 0.0662 - val_acc: 0.9688\n",
            "Epoch 3/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9988\n",
            "Epoch 00003: val_acc did not improve from 0.96875\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0250 - acc: 0.9988 - val_loss: 0.0497 - val_acc: 0.9688\n",
            "Epoch 4/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9965\n",
            "Epoch 00004: val_acc improved from 0.96875 to 0.97917, saving model to pruned-weights-hyper3dnetAVOCADO5-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0269 - acc: 0.9965 - val_loss: 0.0416 - val_acc: 0.9792\n",
            "Epoch 5/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 1.0000\n",
            "Epoch 00005: val_acc improved from 0.97917 to 0.98958, saving model to pruned-weights-hyper3dnetAVOCADO5-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0232 - acc: 0.9988 - val_loss: 0.0361 - val_acc: 0.9896\n",
            "Epoch 6/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9988\n",
            "Epoch 00006: val_acc improved from 0.98958 to 1.00000, saving model to pruned-weights-hyper3dnetAVOCADO5-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0243 - acc: 0.9988 - val_loss: 0.0320 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9988\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0246 - acc: 0.9988 - val_loss: 0.0301 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9954\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0297 - acc: 0.9954 - val_loss: 0.0347 - val_acc: 0.9896\n",
            "1.4791666666666667\n",
            "AVOCADO: Training6begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9931\n",
            "Epoch 00001: val_acc improved from -inf to 0.98958, saving model to pruned-weights-hyper3dnetAVOCADO6-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 6s 7ms/sample - loss: 0.0476 - acc: 0.9931 - val_loss: 0.0477 - val_acc: 0.9896\n",
            "Epoch 2/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9977\n",
            "Epoch 00002: val_acc improved from 0.98958 to 1.00000, saving model to pruned-weights-hyper3dnetAVOCADO6-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0425 - acc: 0.9977 - val_loss: 0.0433 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9873\n",
            "Epoch 00003: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0504 - acc: 0.9873 - val_loss: 0.0399 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9954\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0499 - acc: 0.9954 - val_loss: 0.0406 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9826\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0580 - acc: 0.9827 - val_loss: 0.0399 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9861\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0483 - acc: 0.9862 - val_loss: 0.0400 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9907\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0536 - acc: 0.9896 - val_loss: 0.0395 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9942\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0499 - acc: 0.9942 - val_loss: 0.0417 - val_acc: 1.0000\n",
            "0.9583333333333334\n",
            "AVOCADO: Training7begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9734\n",
            "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to pruned-weights-hyper3dnetAVOCADO7-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 6s 7ms/sample - loss: 0.0847 - acc: 0.9735 - val_loss: 0.0575 - val_acc: 1.0000\n",
            "Epoch 2/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9745\n",
            "Epoch 00002: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0786 - acc: 0.9747 - val_loss: 0.0545 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9792\n",
            "Epoch 00003: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0745 - acc: 0.9793 - val_loss: 0.0514 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9734\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0893 - acc: 0.9724 - val_loss: 0.0516 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9780\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0792 - acc: 0.9781 - val_loss: 0.0517 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9745\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0815 - acc: 0.9747 - val_loss: 0.0506 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9757\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0817 - acc: 0.9735 - val_loss: 0.0512 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9722\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0827 - acc: 0.9724 - val_loss: 0.0491 - val_acc: 1.0000\n",
            "0.0\n",
            "AVOCADO: Training8begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9977\n",
            "Epoch 00001: val_acc improved from -inf to 0.96875, saving model to pruned-weights-hyper3dnetAVOCADO8-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 6s 6ms/sample - loss: 0.0159 - acc: 0.9977 - val_loss: 0.0717 - val_acc: 0.9688\n",
            "Epoch 2/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9977\n",
            "Epoch 00002: val_acc improved from 0.96875 to 0.97917, saving model to pruned-weights-hyper3dnetAVOCADO8-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0151 - acc: 0.9977 - val_loss: 0.0429 - val_acc: 0.9792\n",
            "Epoch 3/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9988\n",
            "Epoch 00003: val_acc improved from 0.97917 to 1.00000, saving model to pruned-weights-hyper3dnetAVOCADO8-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0150 - acc: 0.9988 - val_loss: 0.0284 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9988\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0171 - acc: 0.9977 - val_loss: 0.0202 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9988\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0115 - acc: 0.9988 - val_loss: 0.0196 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0115 - acc: 1.0000 - val_loss: 0.0194 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9965\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0171 - acc: 0.9965 - val_loss: 0.0191 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0197 - val_acc: 1.0000\n",
            "3.3958333333333335\n",
            "AVOCADO: Training9begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9988\n",
            "Epoch 00001: val_acc improved from -inf to 0.98958, saving model to pruned-weights-hyper3dnetAVOCADO9-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 6s 7ms/sample - loss: 0.0217 - acc: 0.9977 - val_loss: 0.0348 - val_acc: 0.9896\n",
            "Epoch 2/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9977\n",
            "Epoch 00002: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0155 - acc: 0.9977 - val_loss: 0.0289 - val_acc: 0.9896\n",
            "Epoch 3/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9965\n",
            "Epoch 00003: val_acc did not improve from 0.98958\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0199 - acc: 0.9965 - val_loss: 0.0253 - val_acc: 0.9896\n",
            "Epoch 4/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9988\n",
            "Epoch 00004: val_acc improved from 0.98958 to 1.00000, saving model to pruned-weights-hyper3dnetAVOCADO9-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0160 - acc: 0.9988 - val_loss: 0.0242 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9988\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0175 - acc: 0.9988 - val_loss: 0.0227 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9988\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0171 - acc: 0.9988 - val_loss: 0.0217 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9988\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0149 - acc: 0.9988 - val_loss: 0.0213 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9988\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0267 - acc: 0.9965 - val_loss: 0.0213 - val_acc: 1.0000\n",
            "1.4375\n",
            "AVOCADO: Training10begins...\n",
            "Train on 868 samples, validate on 96 samples\n",
            "Epoch 1/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9988\n",
            "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to pruned-weights-hyper3dnetAVOCADO10-best_3layers_4filters.h5\n",
            "868/868 [==============================] - 6s 7ms/sample - loss: 0.0151 - acc: 0.9988 - val_loss: 0.0181 - val_acc: 1.0000\n",
            "Epoch 2/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 1.0000\n",
            "Epoch 00002: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0150 - val_acc: 1.0000\n",
            "Epoch 3/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 1.0000\n",
            "Epoch 00003: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0129 - acc: 0.9988 - val_loss: 0.0147 - val_acc: 1.0000\n",
            "Epoch 4/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9977\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0155 - acc: 0.9977 - val_loss: 0.0143 - val_acc: 1.0000\n",
            "Epoch 5/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0150 - acc: 1.0000 - val_loss: 0.0131 - val_acc: 1.0000\n",
            "Epoch 6/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0113 - acc: 1.0000 - val_loss: 0.0127 - val_acc: 1.0000\n",
            "Epoch 7/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0118 - acc: 1.0000 - val_loss: 0.0126 - val_acc: 1.0000\n",
            "Epoch 8/8\n",
            "864/868 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9988\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "868/868 [==============================] - 4s 5ms/sample - loss: 0.0134 - acc: 0.9988 - val_loss: 0.0127 - val_acc: 1.0000\n",
            "2.9166666666666665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmz9PSRTvPP",
        "colab_type": "code",
        "outputId": "0e3da181-0231-4b90-b93b-217e3d684625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "new_pruned_model.load_weights(\"pruned-weights-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")\n",
        "final_model = sparsity.strip_pruning(new_pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 64, 64, 150)]     0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_6 (Separabl (None, 64, 64, 96)        18246     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 64, 64, 96)        384       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 64, 64, 96)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_7 (Separabl (None, 32, 32, 96)        10176     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32, 32, 96)        384       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 32, 32, 96)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_8 (Separabl (None, 16, 16, 96)        10176     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 96)                0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 1)                 97        \n",
            "=================================================================\n",
            "Total params: 39,463\n",
            "Trainable params: 39,079\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8drkv1mK1CUb",
        "colab_type": "code",
        "outputId": "b4abd07d-5a70-4001-cb27-f32eef4e1c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "import numpy as np\n",
        "for i, w in enumerate(final_model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.4f}%\".format(\n",
        "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "separable_conv2d_6/depthwise_kernel:0 -- Total:3750, Zeros: 0.0000%\n",
            "separable_conv2d_6/pointwise_kernel:0 -- Total:14400, Zeros: 1.0000%\n",
            "separable_conv2d_6/bias:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_7/gamma:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_7/beta:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_7/moving_mean:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_7/moving_variance:0 -- Total:96, Zeros: 0.0000%\n",
            "separable_conv2d_7/depthwise_kernel:0 -- Total:864, Zeros: 0.0000%\n",
            "separable_conv2d_7/pointwise_kernel:0 -- Total:9216, Zeros: 0.9983%\n",
            "separable_conv2d_7/bias:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_8/gamma:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_8/beta:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_8/moving_mean:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_8/moving_variance:0 -- Total:96, Zeros: 0.0000%\n",
            "separable_conv2d_8/depthwise_kernel:0 -- Total:864, Zeros: 0.0000%\n",
            "separable_conv2d_8/pointwise_kernel:0 -- Total:9216, Zeros: 0.9983%\n",
            "separable_conv2d_8/bias:0 -- Total:96, Zeros: 0.0000%\n",
            "fc1/kernel:0 -- Total:96, Zeros: 1.0417%\n",
            "fc1/bias:0 -- Total:1, Zeros: 0.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIRfZDn02rV",
        "colab_type": "code",
        "outputId": "bfe8a654-ceed-4a1e-d6b7-153cac84b009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Saving pruned model to: ', \"Avocado_hyper3DNet_pruned.h5\")\n",
        "tf.keras.models.save_model(final_model, \"Avocado_hyper3DNet_pruned.h5\", \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  Avocado_hyper3DNet_pruned.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tkgm44OZDt",
        "colab_type": "text"
      },
      "source": [
        "# Predict and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVT1KBWvEODg",
        "colab_type": "code",
        "outputId": "88daef25-07c7-4646-bac9-8c6991f79345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import truediv\n",
        "import h5py\n",
        "import pickle\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Prepare lists to save metrics\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 2\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_m):\n",
        "    list_diag = np.diag(confusion_m)\n",
        "    list_raw_sum = np.sum(confusion_m, axis=1)\n",
        "    each_ac = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_ac)\n",
        "    return each_ac, average_acc\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "PREDICT AND CALCULATE METRICS\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "loaded_model = tf.keras.models.load_model(\"weights-hyper3dnet\" + 'AVOCADO' + str(1) + \"-best_3layers_4filters.h5\")\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "dataset = 'AVOCADO'\n",
        "# Initialize\n",
        "confmatrices = np.zeros((10, int(classes), int(classes)))\n",
        "\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "    ytest = train_y[test]\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.30,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.load_weights(\"pruned-weights-hyper3dnet\" + dataset + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "    ypred = ypred.round()\n",
        "\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        con_mat = tf.math.confusion_matrix(labels=ytest,\n",
        "                                           predictions=ypred).numpy()\n",
        "\n",
        "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "    classes_list = list(range(0, int(classes)))\n",
        "    con_mat_df = pd.DataFrame(con_mat_norm, index=classes_list, columns=classes_list)\n",
        "\n",
        "    confmatrices[ntrain - 1, :, :] = con_mat_df.values\n",
        "\n",
        "    # Calculate metrics\n",
        "    oa = accuracy_score(ytest, ypred)\n",
        "    confusion = confusion_matrix(ytest, ypred)\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(ytest, ypred)\n",
        "    prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
        "\n",
        "    # Add metrics to the list\n",
        "    cvoa.append(oa * 100)\n",
        "    cvaa.append(aa * 100)\n",
        "    cvka.append(kappa * 100)\n",
        "    cvpre.append(prec * 100)\n",
        "    cvrec.append(rec * 100)\n",
        "    cvf1.append(f1 * 100)\n",
        "\n",
        "    ntrain += 1\n",
        "\n",
        "file_name = \"classification_report_hyper3dnet_pruned\" + dataset + \".txt\"\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write(\"Overall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvoa)), float(np.std(cvoa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Average accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvaa)), float(np.std(cvaa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Kappa accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvka)), float(np.std(cvka))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Precision accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvpre)), float(np.std(cvpre))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Recall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvrec)), float(np.std(cvrec))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"F1 accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvf1)), float(np.std(cvf1))))\n",
        "\n",
        "# Calculate mean and std\n",
        "means = np.mean(confmatrices * 100, axis=0)\n",
        "stds = np.std(confmatrices * 100, axis=0)\n",
        "\n",
        "def plot_confusion_matrix(cm, cms, classescf,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classescf))\n",
        "    plt.xticks(tick_marks, classescf, rotation=45)\n",
        "    plt.yticks(tick_marks, classescf)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "\n",
        "            if (cm[i, j] == 100 or cm[i, j] == 0) and cms[i, j] == 0:\n",
        "                plt.text(j, i, '{0:.0f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.0f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "            else:\n",
        "                plt.text(j, i, '{0:.2f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.2f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "with open('meanshyper3dnetp', 'wb') as f:\n",
        "    pickle.dump(means, f)\n",
        "with open('stdshyper3dnetp', 'wb') as f:\n",
        "    pickle.dump(stds, f)\n",
        "with open('cvf1hyper3dnetp', 'wb') as f:\n",
        "    pickle.dump(cvf1, f)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "classes_list = list(range(0, int(classes)))\n",
        "plt.figure()\n",
        "plot_confusion_matrix(means, stds, classescf=classes_list)\n",
        "\n",
        "plt.savefig('MatrixConfusion_AVOCADO_hyper3dnet_pruned.png', dpi=1200)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEmCAYAAAAnRIjxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nioiERAlWRCQk/x8UmQOpu7+nkgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}