{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HSI_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyQTRzMcUCB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare tensorflow-model-optimization for pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNPotwubYRu",
        "colab_type": "code",
        "outputId": "a7ba1758-223d-4f3b-975b-e41159858cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tensorflow-gpu==1.14.0\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 23.7MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (45.2.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c4/4c3d011e432bd9c19f0323f7da7d3f783402615e4c3b5a98416c7da9cb05/tensorflow_model_optimization-0.2.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.17.5)\n",
            "Collecting enum34~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/63/f6/ccb1c83687756aeabbf3ca0f213508fcfb03883ff200d201b3a4c60cedcc/enum34-1.1.10-py3-none-any.whl\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.10 tensorflow-model-optimization-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfBLXz1mCNV",
        "colab_type": "code",
        "outputId": "ff359716-3121-4018-9c38-a18a81490ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk1dYHpbicZ",
        "colab_type": "text"
      },
      "source": [
        "#Download dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu64LC4Wm1oc",
        "colab_type": "code",
        "outputId": "f935a787-2706-481a-df11-962526c33bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWT-iIqSb_46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8/view?usp=sharing\n",
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8\n",
        "download = drive.CreateFile({'id': '1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8'})\n",
        "download.GetContentFile('weed_dataset_w25.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDlBcilciuY",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRqt2u-MVfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from operator import truediv\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syCzlwNcr-di",
        "colab_type": "code",
        "outputId": "5ec37902-de12-435e-a743-51e2bb64b1cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "LOAD HDF5 FILE\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "hdf5_file = h5py.File('weed_dataset_w25.hdf5', \"r\")\n",
        "train_x = np.array(hdf5_file[\"train_img\"][...])\n",
        "# train_x = train_x / np.max(train_x)\n",
        "# train_x = np.clip(train_x, 0, 1)\n",
        "#train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))\n",
        "train_y = np.array(hdf5_file[\"train_labels\"][...])\n",
        "\n",
        "# Average consecutive bands\n",
        "img2 = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], int(train_x.shape[3]/2)))\n",
        "for n in range(0, train_x.shape[0]):\n",
        "    # Average consecutive bands\n",
        "    for i in range(0, train_x.shape[3], 2):\n",
        "        img2[n, :, :, int(i/2)] = (train_x[n, :, :, i] + train_x[n, :, :, i + 1]) / 2.\n",
        "\n",
        "train_x = img2\n",
        "\n",
        "print(train_x.shape)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3wrK8dNe-T7",
        "colab_type": "code",
        "outputId": "941a27c5-ec32-4543-d65c-bc8ad12c45b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3]))\n",
        "\n",
        "temp = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], 6))\n",
        "\n",
        "temp[:, :, :, 0] = train_x[:, :, :, 10] \n",
        "temp[:, :, :, 1] = train_x[:, :, :,20] \n",
        "temp[:, :, :, 2] = train_x[:, :, :, 36]  \n",
        "temp[:, :, :, 3] = train_x[:, :, :, 66] \n",
        "temp[:, :, :, 4] = train_x[:, :, :, 136] \n",
        "temp[:, :, :, 5] = train_x[:, :, :, 144]\n",
        "\n",
        "train_x = temp\n",
        "\n",
        "print(train_x.shape)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvx55MBgcrZJ",
        "colab_type": "text"
      },
      "source": [
        "# Trained pruned network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2xU6uWWAuz",
        "colab_type": "code",
        "outputId": "ad45c7bf-1d74-4ac9-8528-5f0a958a56b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data = 'WEED'\n",
        "loaded_model = tf.keras.models.load_model(\"selected5-weights-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE2WVq7DItg",
        "colab_type": "code",
        "outputId": "951f1857-7d38-4ef7-87d7-e63ffcabd7e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.5,\n",
        "                                                   final_sparsity=0.9,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adadelta',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2970\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.debugging.assert_greater_equal is deprecated. Please use tf.compat.v1.debugging.assert_greater_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py:59: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 6, 1)]    0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv3d_6 (None, 25, 25, 6, 16)     882       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 6, 16)     65        \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 6, 16)     1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv3d_7 (None, 25, 25, 6, 16)     13842     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 6, 16)     65        \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 6, 16)     1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_reshape_ (None, 25, 25, 96)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 25, 25, 96)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 25, 25, 320)       64162     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 320)       1281      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 320)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 13, 13, 256)       166978    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 13, 13, 256)       1025      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 13, 13, 256)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 7, 7, 256)         133634    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 7, 7, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_global_a (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_fc3 (Pru (None, 3)                 1541      \n",
            "=================================================================\n",
            "Total params: 383,483\n",
            "Trainable params: 195,955\n",
            "Non-trainable params: 187,528\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYBXiSTtaDt",
        "colab_type": "code",
        "outputId": "c586cf72-1294-4e0d-b26c-49fb26f85c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "cva1 = []\n",
        "cva2 = []\n",
        "cva3 = []\n",
        "\n",
        "\n",
        "data = 'WEED'\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "\n",
        "    ytrain = tf.keras.utils.to_categorical(train_y[train]).astype(np.int32)\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.15,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    loaded_model.load_weights(\"selected5-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "    #new_pruned_model.summary()\n",
        "\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.categorical_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # checkpoint\n",
        "    filepath = \"pruned5-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint, sparsity.UpdatePruningStep()]\n",
        "\n",
        "    # Train model on dataset\n",
        "    print(data + \": Training\" + str(ntrain) + \"begins...\")\n",
        "    history = new_pruned_model.fit(x=train_x[train], y=ytrain, validation_data=(train_x[test], ytest),\n",
        "                        batch_size=32, epochs=epochs, callbacks=callbacks_list)\n",
        "    \n",
        "    new_pruned_model.load_weights(\"pruned5-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.categorical_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "    print(np.sum(ypred*ytest)/len(ytest))\n",
        "    \n",
        "    ntrain += 1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WEED: Training1begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9956\n",
            "Epoch 00001: val_acc improved from -inf to 0.94462, saving model to pruned5-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 28s 5ms/sample - loss: 0.0117 - acc: 0.9956 - val_loss: 0.3038 - val_acc: 0.9446\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9975\n",
            "Epoch 00002: val_acc improved from 0.94462 to 0.95411, saving model to pruned5-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0090 - acc: 0.9975 - val_loss: 0.1575 - val_acc: 0.9541\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9959\n",
            "Epoch 00003: val_acc improved from 0.95411 to 0.96044, saving model to pruned5-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0113 - acc: 0.9960 - val_loss: 0.1417 - val_acc: 0.9604\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9944\n",
            "Epoch 00004: val_acc did not improve from 0.96044\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0171 - acc: 0.9942 - val_loss: 0.2856 - val_acc: 0.9367\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9944\n",
            "Epoch 00005: val_acc improved from 0.96044 to 0.96361, saving model to pruned5-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0174 - acc: 0.9944 - val_loss: 0.1391 - val_acc: 0.9636\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9936\n",
            "Epoch 00006: val_acc did not improve from 0.96361\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0219 - acc: 0.9937 - val_loss: 0.1592 - val_acc: 0.9573\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9945\n",
            "Epoch 00007: val_acc improved from 0.96361 to 0.96519, saving model to pruned5-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0181 - acc: 0.9944 - val_loss: 0.1451 - val_acc: 0.9652\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9951\n",
            "Epoch 00008: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0167 - acc: 0.9949 - val_loss: 0.1536 - val_acc: 0.9573\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9958\n",
            "Epoch 00009: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0123 - acc: 0.9958 - val_loss: 0.1636 - val_acc: 0.9557\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9945\n",
            "Epoch 00010: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0180 - acc: 0.9945 - val_loss: 0.1578 - val_acc: 0.9573\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9949\n",
            "Epoch 00011: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0147 - acc: 0.9949 - val_loss: 0.1482 - val_acc: 0.9573\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9956\n",
            "Epoch 00012: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0100 - acc: 0.9956 - val_loss: 0.1507 - val_acc: 0.9573\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9959\n",
            "Epoch 00013: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0128 - acc: 0.9960 - val_loss: 0.2045 - val_acc: 0.9462\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9970\n",
            "Epoch 00014: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0077 - acc: 0.9970 - val_loss: 0.1473 - val_acc: 0.9573\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9958\n",
            "Epoch 00015: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0115 - acc: 0.9958 - val_loss: 0.1480 - val_acc: 0.9541\n",
            "0.9601954603008322\n",
            "WEED: Training2begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9988\n",
            "Epoch 00001: val_acc improved from -inf to 0.96519, saving model to pruned5-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 23s 4ms/sample - loss: 0.0051 - acc: 0.9988 - val_loss: 0.1824 - val_acc: 0.9652\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9986\n",
            "Epoch 00002: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0043 - acc: 0.9986 - val_loss: 0.1684 - val_acc: 0.9636\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993\n",
            "Epoch 00003: val_acc improved from 0.96519 to 0.97310, saving model to pruned5-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0022 - acc: 0.9993 - val_loss: 0.1622 - val_acc: 0.9731\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9986\n",
            "Epoch 00004: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0032 - acc: 0.9986 - val_loss: 0.1708 - val_acc: 0.9604\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9989\n",
            "Epoch 00005: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0039 - acc: 0.9989 - val_loss: 0.1703 - val_acc: 0.9636\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993\n",
            "Epoch 00006: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0022 - acc: 0.9993 - val_loss: 0.1683 - val_acc: 0.9668\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9991\n",
            "Epoch 00007: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0027 - acc: 0.9991 - val_loss: 0.1712 - val_acc: 0.9668\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
            "Epoch 00008: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.2090 - val_acc: 0.9589\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9986\n",
            "Epoch 00009: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.1654 - val_acc: 0.9668\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9991\n",
            "Epoch 00010: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0028 - acc: 0.9991 - val_loss: 0.1637 - val_acc: 0.9668\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
            "Epoch 00011: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1681 - val_acc: 0.9636\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9979\n",
            "Epoch 00012: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0048 - acc: 0.9979 - val_loss: 0.1668 - val_acc: 0.9668\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9996\n",
            "Epoch 00013: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 0.1683 - val_acc: 0.9668\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986\n",
            "Epoch 00014: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.1670 - val_acc: 0.9652\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9996\n",
            "Epoch 00015: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0015 - acc: 0.9996 - val_loss: 0.1669 - val_acc: 0.9668\n",
            "0.9666137975752903\n",
            "WEED: Training3begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9989\n",
            "Epoch 00001: val_acc improved from -inf to 0.93987, saving model to pruned5-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 18s 3ms/sample - loss: 0.0050 - acc: 0.9989 - val_loss: 0.2881 - val_acc: 0.9399\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9982\n",
            "Epoch 00002: val_acc improved from 0.93987 to 0.96677, saving model to pruned5-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0055 - acc: 0.9982 - val_loss: 0.1795 - val_acc: 0.9668\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9988\n",
            "Epoch 00003: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0047 - acc: 0.9986 - val_loss: 0.1911 - val_acc: 0.9652\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9979\n",
            "Epoch 00004: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0070 - acc: 0.9979 - val_loss: 0.1938 - val_acc: 0.9589\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9979\n",
            "Epoch 00005: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0047 - acc: 0.9979 - val_loss: 0.1943 - val_acc: 0.9620\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9966\n",
            "Epoch 00006: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0094 - acc: 0.9967 - val_loss: 0.2089 - val_acc: 0.9557\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9952\n",
            "Epoch 00007: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0122 - acc: 0.9952 - val_loss: 0.2664 - val_acc: 0.9478\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9928\n",
            "Epoch 00008: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0215 - acc: 0.9928 - val_loss: 0.2321 - val_acc: 0.9541\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9944\n",
            "Epoch 00009: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0140 - acc: 0.9944 - val_loss: 0.2186 - val_acc: 0.9557\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9944\n",
            "Epoch 00010: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0200 - acc: 0.9944 - val_loss: 0.2163 - val_acc: 0.9557\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9936\n",
            "Epoch 00011: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0200 - acc: 0.9935 - val_loss: 0.2172 - val_acc: 0.9557\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9945\n",
            "Epoch 00012: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0140 - acc: 0.9945 - val_loss: 0.2164 - val_acc: 0.9589\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9970\n",
            "Epoch 00013: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0085 - acc: 0.9970 - val_loss: 0.2173 - val_acc: 0.9589\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9947\n",
            "Epoch 00014: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0176 - acc: 0.9947 - val_loss: 0.2159 - val_acc: 0.9604\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9954\n",
            "Epoch 00015: val_acc did not improve from 0.96677\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0156 - acc: 0.9954 - val_loss: 0.2177 - val_acc: 0.9573\n",
            "0.959045464227748\n",
            "WEED: Training4begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9952\n",
            "Epoch 00001: val_acc improved from -inf to 0.85601, saving model to pruned5-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 19s 3ms/sample - loss: 0.0133 - acc: 0.9951 - val_loss: 0.7142 - val_acc: 0.8560\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9940\n",
            "Epoch 00002: val_acc improved from 0.85601 to 0.94937, saving model to pruned5-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0171 - acc: 0.9940 - val_loss: 0.2048 - val_acc: 0.9494\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9954\n",
            "Epoch 00003: val_acc improved from 0.94937 to 0.95570, saving model to pruned5-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0187 - acc: 0.9954 - val_loss: 0.1901 - val_acc: 0.9557\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9945\n",
            "Epoch 00004: val_acc did not improve from 0.95570\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0158 - acc: 0.9945 - val_loss: 0.1784 - val_acc: 0.9557\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9945\n",
            "Epoch 00005: val_acc did not improve from 0.95570\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0146 - acc: 0.9945 - val_loss: 0.1847 - val_acc: 0.9557\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9949\n",
            "Epoch 00006: val_acc did not improve from 0.95570\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0181 - acc: 0.9947 - val_loss: 0.8368 - val_acc: 0.8291\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9926\n",
            "Epoch 00007: val_acc improved from 0.95570 to 0.96044, saving model to pruned5-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0207 - acc: 0.9926 - val_loss: 0.1793 - val_acc: 0.9604\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9908\n",
            "Epoch 00008: val_acc did not improve from 0.96044\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0254 - acc: 0.9909 - val_loss: 0.2100 - val_acc: 0.9509\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9873\n",
            "Epoch 00009: val_acc did not improve from 0.96044\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0345 - acc: 0.9868 - val_loss: 0.4034 - val_acc: 0.8924\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9922\n",
            "Epoch 00010: val_acc did not improve from 0.96044\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0256 - acc: 0.9923 - val_loss: 0.1850 - val_acc: 0.9557\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9891\n",
            "Epoch 00011: val_acc did not improve from 0.96044\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0305 - acc: 0.9886 - val_loss: 0.2191 - val_acc: 0.9494\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9912\n",
            "Epoch 00012: val_acc did not improve from 0.96044\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0246 - acc: 0.9912 - val_loss: 0.1907 - val_acc: 0.9573\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9935\n",
            "Epoch 00013: val_acc did not improve from 0.96044\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0214 - acc: 0.9935 - val_loss: 0.1840 - val_acc: 0.9604\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9926\n",
            "Epoch 00014: val_acc did not improve from 0.96044\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0212 - acc: 0.9926 - val_loss: 0.1855 - val_acc: 0.9604\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9945\n",
            "Epoch 00015: val_acc improved from 0.96044 to 0.96203, saving model to pruned5-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0166 - acc: 0.9945 - val_loss: 0.1855 - val_acc: 0.9620\n",
            "0.9488401774093213\n",
            "WEED: Training5begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9970\n",
            "Epoch 00001: val_acc improved from -inf to 0.95570, saving model to pruned5-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 20s 4ms/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.2285 - val_acc: 0.9557\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
            "Epoch 00002: val_acc improved from 0.95570 to 0.96203, saving model to pruned5-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.1938 - val_acc: 0.9620\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9984\n",
            "Epoch 00003: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0073 - acc: 0.9984 - val_loss: 0.2015 - val_acc: 0.9573\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9959\n",
            "Epoch 00004: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0114 - acc: 0.9960 - val_loss: 0.4387 - val_acc: 0.9035\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9956\n",
            "Epoch 00005: val_acc improved from 0.96203 to 0.96361, saving model to pruned5-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0117 - acc: 0.9956 - val_loss: 0.1919 - val_acc: 0.9636\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9974\n",
            "Epoch 00006: val_acc did not improve from 0.96361\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0072 - acc: 0.9974 - val_loss: 0.1995 - val_acc: 0.9636\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9975\n",
            "Epoch 00007: val_acc improved from 0.96361 to 0.96519, saving model to pruned5-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0087 - acc: 0.9974 - val_loss: 0.2062 - val_acc: 0.9652\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9968\n",
            "Epoch 00008: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0106 - acc: 0.9968 - val_loss: 0.2172 - val_acc: 0.9589\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9959\n",
            "Epoch 00009: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0117 - acc: 0.9960 - val_loss: 0.2069 - val_acc: 0.9604\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9966\n",
            "Epoch 00010: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0098 - acc: 0.9967 - val_loss: 0.2043 - val_acc: 0.9652\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9977\n",
            "Epoch 00011: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0069 - acc: 0.9977 - val_loss: 0.2081 - val_acc: 0.9620\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9959\n",
            "Epoch 00012: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0107 - acc: 0.9960 - val_loss: 0.2060 - val_acc: 0.9636\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9963\n",
            "Epoch 00013: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0091 - acc: 0.9963 - val_loss: 4.3367 - val_acc: 0.6234\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9942\n",
            "Epoch 00014: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0163 - acc: 0.9942 - val_loss: 0.3031 - val_acc: 0.9478\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9931\n",
            "Epoch 00015: val_acc did not improve from 0.96519\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0191 - acc: 0.9930 - val_loss: 0.2585 - val_acc: 0.9525\n",
            "0.9605007998044951\n",
            "WEED: Training6begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 4.4069e-04 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 0.95570, saving model to pruned5-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 18s 3ms/sample - loss: 4.3932e-04 - acc: 1.0000 - val_loss: 0.3251 - val_acc: 0.9557\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9993\n",
            "Epoch 00002: val_acc did not improve from 0.95570\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0014 - acc: 0.9993 - val_loss: 0.3146 - val_acc: 0.9541\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9995\n",
            "Epoch 00003: val_acc did not improve from 0.95570\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0014 - acc: 0.9995 - val_loss: 0.3365 - val_acc: 0.9509\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9996\n",
            "Epoch 00004: val_acc did not improve from 0.95570\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0011 - acc: 0.9996 - val_loss: 0.3114 - val_acc: 0.9541\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 5.5319e-04 - acc: 0.9996\n",
            "Epoch 00005: val_acc did not improve from 0.95570\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 5.5163e-04 - acc: 0.9996 - val_loss: 0.3093 - val_acc: 0.9525\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9996\n",
            "Epoch 00006: val_acc improved from 0.95570 to 0.95728, saving model to pruned5-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 0.0015 - acc: 0.9996 - val_loss: 0.3122 - val_acc: 0.9573\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 8.7756e-04 - acc: 0.9998\n",
            "Epoch 00007: val_acc did not improve from 0.95728\n",
            "5684/5684 [==============================] - 13s 2ms/sample - loss: 8.7461e-04 - acc: 0.9998 - val_loss: 0.3112 - val_acc: 0.9557\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9995\n",
            "Epoch 00008: val_acc did not improve from 0.95728\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 0.2945 - val_acc: 0.9557\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9996\n",
            "Epoch 00009: val_acc did not improve from 0.95728\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 0.3027 - val_acc: 0.9573\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9993\n",
            "Epoch 00010: val_acc did not improve from 0.95728\n",
            "5684/5684 [==============================] - 14s 2ms/sample - loss: 0.0011 - acc: 0.9993 - val_loss: 0.3071 - val_acc: 0.9541\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
            "Epoch 00011: val_acc did not improve from 0.95728\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0025 - acc: 0.9996 - val_loss: 0.4871 - val_acc: 0.9272\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9984\n",
            "Epoch 00012: val_acc did not improve from 0.95728\n",
            "5684/5684 [==============================] - 16s 3ms/sample - loss: 0.0030 - acc: 0.9984 - val_loss: 0.3422 - val_acc: 0.9557\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 6.8697e-04 - acc: 0.9998\n",
            "Epoch 00013: val_acc did not improve from 0.95728\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 6.8457e-04 - acc: 0.9998 - val_loss: 0.5581 - val_acc: 0.9225\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 9.0175e-04 - acc: 0.9996\n",
            "Epoch 00014: val_acc did not improve from 0.95728\n",
            "5684/5684 [==============================] - 15s 3ms/sample - loss: 0.0015 - acc: 0.9995 - val_loss: 0.3345 - val_acc: 0.9557\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
            "Epoch 00015: val_acc improved from 0.95728 to 0.95886, saving model to pruned5-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 16s 3ms/sample - loss: 0.0044 - acc: 0.9988 - val_loss: 0.3332 - val_acc: 0.9589\n",
            "0.9540807152525156\n",
            "WEED: Training7begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
            "Epoch 00001: val_acc improved from -inf to 0.96672, saving model to pruned5-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 24s 4ms/sample - loss: 0.0047 - acc: 0.9988 - val_loss: 0.1772 - val_acc: 0.9667\n",
            "Epoch 2/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
            "Epoch 00002: val_acc did not improve from 0.96672\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0025 - acc: 0.9989 - val_loss: 0.1632 - val_acc: 0.9667\n",
            "Epoch 3/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
            "Epoch 00003: val_acc did not improve from 0.96672\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.1563 - val_acc: 0.9667\n",
            "Epoch 4/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9986\n",
            "Epoch 00004: val_acc did not improve from 0.96672\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.1589 - val_acc: 0.9635\n",
            "Epoch 5/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9986\n",
            "Epoch 00005: val_acc did not improve from 0.96672\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0034 - acc: 0.9986 - val_loss: 0.1609 - val_acc: 0.9651\n",
            "Epoch 6/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9988\n",
            "Epoch 00006: val_acc did not improve from 0.96672\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0028 - acc: 0.9988 - val_loss: 0.1562 - val_acc: 0.9620\n",
            "Epoch 7/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9991\n",
            "Epoch 00007: val_acc did not improve from 0.96672\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0020 - acc: 0.9991 - val_loss: 0.1473 - val_acc: 0.9651\n",
            "Epoch 8/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9982\n",
            "Epoch 00008: val_acc improved from 0.96672 to 0.96989, saving model to pruned5-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0053 - acc: 0.9982 - val_loss: 0.1671 - val_acc: 0.9699\n",
            "Epoch 9/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9988\n",
            "Epoch 00009: val_acc did not improve from 0.96989\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0040 - acc: 0.9988 - val_loss: 0.2266 - val_acc: 0.9540\n",
            "Epoch 10/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9988\n",
            "Epoch 00010: val_acc did not improve from 0.96989\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0040 - acc: 0.9988 - val_loss: 0.1675 - val_acc: 0.9620\n",
            "Epoch 11/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9991\n",
            "Epoch 00011: val_acc did not improve from 0.96989\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0020 - acc: 0.9991 - val_loss: 0.1613 - val_acc: 0.9620\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9988\n",
            "Epoch 00012: val_acc did not improve from 0.96989\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0034 - acc: 0.9988 - val_loss: 0.1638 - val_acc: 0.9635\n",
            "Epoch 13/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9989\n",
            "Epoch 00013: val_acc did not improve from 0.96989\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0027 - acc: 0.9989 - val_loss: 0.1630 - val_acc: 0.9635\n",
            "Epoch 14/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9993\n",
            "Epoch 00014: val_acc did not improve from 0.96989\n",
            "5685/5685 [==============================] - 15s 3ms/sample - loss: 0.0019 - acc: 0.9993 - val_loss: 0.1617 - val_acc: 0.9620\n",
            "Epoch 15/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9993\n",
            "Epoch 00015: val_acc did not improve from 0.96989\n",
            "5685/5685 [==============================] - 14s 3ms/sample - loss: 0.0019 - acc: 0.9993 - val_loss: 0.1616 - val_acc: 0.9620\n",
            "0.9637063740289522\n",
            "WEED: Training8begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9986\n",
            "Epoch 00001: val_acc improved from -inf to 0.95563, saving model to pruned5-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 19s 3ms/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.2711 - val_acc: 0.9556\n",
            "Epoch 2/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9991\n",
            "Epoch 00002: val_acc improved from 0.95563 to 0.95880, saving model to pruned5-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0027 - acc: 0.9991 - val_loss: 0.2369 - val_acc: 0.9588\n",
            "Epoch 3/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 8.8930e-04 - acc: 0.9998\n",
            "Epoch 00003: val_acc improved from 0.95880 to 0.96355, saving model to pruned5-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 8.8611e-04 - acc: 0.9998 - val_loss: 0.2355 - val_acc: 0.9635\n",
            "Epoch 4/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
            "Epoch 00004: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.2386 - val_acc: 0.9588\n",
            "Epoch 5/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 9.9775e-04 - acc: 0.9998\n",
            "Epoch 00005: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 9.9409e-04 - acc: 0.9998 - val_loss: 0.2310 - val_acc: 0.9620\n",
            "Epoch 6/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9996\n",
            "Epoch 00006: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0030 - acc: 0.9996 - val_loss: 0.2275 - val_acc: 0.9588\n",
            "Epoch 7/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9982\n",
            "Epoch 00007: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0070 - acc: 0.9982 - val_loss: 0.2780 - val_acc: 0.9604\n",
            "Epoch 8/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9982\n",
            "Epoch 00008: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0033 - acc: 0.9982 - val_loss: 0.2709 - val_acc: 0.9477\n",
            "Epoch 9/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9986\n",
            "Epoch 00009: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0034 - acc: 0.9986 - val_loss: 0.2380 - val_acc: 0.9540\n",
            "Epoch 10/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
            "Epoch 00010: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.2369 - val_acc: 0.9572\n",
            "Epoch 11/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9982\n",
            "Epoch 00011: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0049 - acc: 0.9982 - val_loss: 0.2379 - val_acc: 0.9604\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9984\n",
            "Epoch 00012: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0094 - acc: 0.9977 - val_loss: 0.2343 - val_acc: 0.9572\n",
            "Epoch 13/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9986\n",
            "Epoch 00013: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.2650 - val_acc: 0.9477\n",
            "Epoch 14/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9979\n",
            "Epoch 00014: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0094 - acc: 0.9979 - val_loss: 0.2401 - val_acc: 0.9556\n",
            "Epoch 15/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9986\n",
            "Epoch 00015: val_acc did not improve from 0.96355\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.2362 - val_acc: 0.9556\n",
            "0.9579135141125222\n",
            "WEED: Training9begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9995\n",
            "Epoch 00001: val_acc improved from -inf to 0.93661, saving model to pruned5-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 19s 3ms/sample - loss: 0.0013 - acc: 0.9995 - val_loss: 0.3872 - val_acc: 0.9366\n",
            "Epoch 2/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
            "Epoch 00002: val_acc improved from 0.93661 to 0.95246, saving model to pruned5-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.3144 - val_acc: 0.9525\n",
            "Epoch 3/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9995\n",
            "Epoch 00003: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0011 - acc: 0.9995 - val_loss: 0.3103 - val_acc: 0.9493\n",
            "Epoch 4/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9995\n",
            "Epoch 00004: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0015 - acc: 0.9995 - val_loss: 0.4305 - val_acc: 0.9287\n",
            "Epoch 5/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9991\n",
            "Epoch 00005: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0027 - acc: 0.9989 - val_loss: 0.3222 - val_acc: 0.9493\n",
            "Epoch 6/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9981\n",
            "Epoch 00006: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0049 - acc: 0.9981 - val_loss: 0.3184 - val_acc: 0.9461\n",
            "Epoch 7/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9989\n",
            "Epoch 00007: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0033 - acc: 0.9989 - val_loss: 0.3203 - val_acc: 0.9477\n",
            "Epoch 8/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9989\n",
            "Epoch 00008: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0041 - acc: 0.9989 - val_loss: 0.3270 - val_acc: 0.9493\n",
            "Epoch 9/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9993\n",
            "Epoch 00009: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0016 - acc: 0.9993 - val_loss: 0.3190 - val_acc: 0.9525\n",
            "Epoch 10/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9991\n",
            "Epoch 00010: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0024 - acc: 0.9991 - val_loss: 0.3210 - val_acc: 0.9525\n",
            "Epoch 11/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9998\n",
            "Epoch 00011: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.3225 - val_acc: 0.9509\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9996\n",
            "Epoch 00012: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 0.3203 - val_acc: 0.9525\n",
            "Epoch 13/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9989\n",
            "Epoch 00013: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0027 - acc: 0.9989 - val_loss: 0.3182 - val_acc: 0.9525\n",
            "Epoch 14/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9988\n",
            "Epoch 00014: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0025 - acc: 0.9988 - val_loss: 0.3205 - val_acc: 0.9525\n",
            "Epoch 15/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
            "Epoch 00015: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.3192 - val_acc: 0.9525\n",
            "0.950402710085463\n",
            "WEED: Training10begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
            "Epoch 00001: val_acc improved from -inf to 0.92235, saving model to pruned5-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 19s 3ms/sample - loss: 0.0054 - acc: 0.9984 - val_loss: 0.5314 - val_acc: 0.9223\n",
            "Epoch 2/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9965\n",
            "Epoch 00002: val_acc improved from 0.92235 to 0.94929, saving model to pruned5-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0113 - acc: 0.9963 - val_loss: 0.3512 - val_acc: 0.9493\n",
            "Epoch 3/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9949\n",
            "Epoch 00003: val_acc did not improve from 0.94929\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0173 - acc: 0.9947 - val_loss: 0.6753 - val_acc: 0.9049\n",
            "Epoch 4/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9951\n",
            "Epoch 00004: val_acc improved from 0.94929 to 0.95087, saving model to pruned5-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0171 - acc: 0.9951 - val_loss: 0.3584 - val_acc: 0.9509\n",
            "Epoch 5/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9929\n",
            "Epoch 00005: val_acc did not improve from 0.95087\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0235 - acc: 0.9930 - val_loss: 0.3515 - val_acc: 0.9509\n",
            "Epoch 6/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9961\n",
            "Epoch 00006: val_acc did not improve from 0.95087\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0124 - acc: 0.9961 - val_loss: 0.3768 - val_acc: 0.9509\n",
            "Epoch 7/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9982\n",
            "Epoch 00007: val_acc did not improve from 0.95087\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0044 - acc: 0.9982 - val_loss: 0.3717 - val_acc: 0.9493\n",
            "Epoch 8/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9958\n",
            "Epoch 00008: val_acc did not improve from 0.95087\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0144 - acc: 0.9958 - val_loss: 0.3723 - val_acc: 0.9493\n",
            "Epoch 9/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9966\n",
            "Epoch 00009: val_acc did not improve from 0.95087\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0089 - acc: 0.9967 - val_loss: 0.3807 - val_acc: 0.9509\n",
            "Epoch 10/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9966\n",
            "Epoch 00010: val_acc did not improve from 0.95087\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0110 - acc: 0.9967 - val_loss: 0.3746 - val_acc: 0.9509\n",
            "Epoch 11/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9979\n",
            "Epoch 00011: val_acc did not improve from 0.95087\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0102 - acc: 0.9979 - val_loss: 0.3689 - val_acc: 0.9509\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9966\n",
            "Epoch 00012: val_acc did not improve from 0.95087\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0119 - acc: 0.9967 - val_loss: 0.3689 - val_acc: 0.9509\n",
            "Epoch 13/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9977\n",
            "Epoch 00013: val_acc improved from 0.95087 to 0.95246, saving model to pruned5-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 14s 2ms/sample - loss: 0.0065 - acc: 0.9977 - val_loss: 0.3662 - val_acc: 0.9525\n",
            "Epoch 14/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9968\n",
            "Epoch 00014: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 13s 2ms/sample - loss: 0.0080 - acc: 0.9968 - val_loss: 0.3662 - val_acc: 0.9509\n",
            "Epoch 15/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9965\n",
            "Epoch 00015: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 12s 2ms/sample - loss: 0.0106 - acc: 0.9965 - val_loss: 0.3625 - val_acc: 0.9509\n",
            "0.9530390303974238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmz9PSRTvPP",
        "colab_type": "code",
        "outputId": "178089d9-cc22-4c25-f788-8d6d7b845067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "new_pruned_model.load_weights(\"pruned5-weights-hyper3dnet\" + data + str(2) + \"-best_3layers_4filters.h5\")\n",
        "final_model = sparsity.strip_pruning(new_pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 6, 1)]    0         \n",
            "_________________________________________________________________\n",
            "conv3d_6 (Conv3D)            (None, 25, 25, 6, 16)     448       \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 25, 25, 6, 16)     64        \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 25, 25, 6, 16)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_7 (Conv3D)            (None, 25, 25, 6, 16)     6928      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 25, 25, 6, 16)     64        \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 25, 25, 6, 16)     0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 25, 25, 96)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 25, 25, 96)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_8 (Separabl (None, 25, 25, 320)       33440     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 25, 25, 320)       1280      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 25, 25, 320)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_9 (Separabl (None, 13, 13, 256)       85056     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 13, 13, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_10 (Separab (None, 7, 7, 256)         68096     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 197,171\n",
            "Trainable params: 195,955\n",
            "Non-trainable params: 1,216\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8drkv1mK1CUb",
        "colab_type": "code",
        "outputId": "0fce0f8f-3b4a-481d-8348-315999ef3969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "import numpy as np\n",
        "for i, w in enumerate(final_model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.4f}%\".format(\n",
        "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv3d_6/kernel:0 -- Total:432, Zeros: 12.0370%\n",
            "conv3d_6/bias:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_12/gamma:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_12/beta:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_12/moving_mean:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_12/moving_variance:0 -- Total:16, Zeros: 0.0000%\n",
            "conv3d_7/kernel:0 -- Total:6912, Zeros: 12.1238%\n",
            "conv3d_7/bias:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_13/gamma:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_13/beta:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_13/moving_mean:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_13/moving_variance:0 -- Total:16, Zeros: 0.0000%\n",
            "separable_conv2d_8/depthwise_kernel:0 -- Total:2400, Zeros: 0.0000%\n",
            "separable_conv2d_8/pointwise_kernel:0 -- Total:30720, Zeros: 12.1224%\n",
            "separable_conv2d_8/bias:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_14/gamma:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_14/beta:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_14/moving_mean:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_14/moving_variance:0 -- Total:320, Zeros: 0.0000%\n",
            "separable_conv2d_9/depthwise_kernel:0 -- Total:2880, Zeros: 0.0000%\n",
            "separable_conv2d_9/pointwise_kernel:0 -- Total:81920, Zeros: 12.1240%\n",
            "separable_conv2d_9/bias:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_15/gamma:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_15/beta:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_15/moving_mean:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_15/moving_variance:0 -- Total:256, Zeros: 0.0000%\n",
            "separable_conv2d_10/depthwise_kernel:0 -- Total:2304, Zeros: 0.0000%\n",
            "separable_conv2d_10/pointwise_kernel:0 -- Total:65536, Zeros: 12.1246%\n",
            "separable_conv2d_10/bias:0 -- Total:256, Zeros: 0.0000%\n",
            "fc3/kernel:0 -- Total:768, Zeros: 12.1094%\n",
            "fc3/bias:0 -- Total:3, Zeros: 0.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIRfZDn02rV",
        "colab_type": "code",
        "outputId": "2b9cf166-bd2b-4d11-a0c4-5135e14e93ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Saving pruned model to: ', \"Kochia_hyper3DNet_pruned.h5\")\n",
        "tf.keras.models.save_model(final_model, \"Kochia_hyper3DNet_5bands_pruned.h5\", \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  Kochia_hyper3DNet_pruned.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tkgm44OZDt",
        "colab_type": "text"
      },
      "source": [
        "# Predict and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVT1KBWvEODg",
        "colab_type": "code",
        "outputId": "2090a833-3681-44b3-e6f2-003929cef119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import truediv\n",
        "import h5py\n",
        "import pickle\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Prepare lists to save metrics\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_m):\n",
        "    list_diag = np.diag(confusion_m)\n",
        "    list_raw_sum = np.sum(confusion_m, axis=1)\n",
        "    each_ac = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_ac)\n",
        "    return each_ac, average_acc\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "PREDICT AND CALCULATE METRICS\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "loaded_model = tf.keras.models.load_model(\"selected5-weights-hyper3dnet\" + 'WEED' + str(1) + \"-best_3layers_4filters.h5\")\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "dataset = 'WEED'\n",
        "# Initialize\n",
        "confmatrices = np.zeros((10, int(classes), int(classes)))\n",
        "\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.30,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.load_weights(\"pruned5-weights-hyper3dnet\" + dataset + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        con_mat = tf.math.confusion_matrix(labels=np.argmax(ytest, axis=-1),\n",
        "                                           predictions=np.argmax(ypred, axis=-1)).numpy()\n",
        "\n",
        "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "    classes_list = list(range(0, int(classes)))\n",
        "    con_mat_df = pd.DataFrame(con_mat_norm, index=classes_list, columns=classes_list)\n",
        "\n",
        "    confmatrices[ntrain - 1, :, :] = con_mat_df.values\n",
        "\n",
        "    # Calculate metrics\n",
        "    oa = accuracy_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    confusion = confusion_matrix(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    prec, rec, f1, support = precision_recall_fscore_support(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1),\n",
        "                                                             average='macro')\n",
        "\n",
        "    # Add metrics to the list\n",
        "    cvoa.append(oa * 100)\n",
        "    cvaa.append(aa * 100)\n",
        "    cvka.append(kappa * 100)\n",
        "    cvpre.append(prec * 100)\n",
        "    cvrec.append(rec * 100)\n",
        "    cvf1.append(f1 * 100)\n",
        "\n",
        "    ntrain += 1\n",
        "\n",
        "file_name = \"classification_report_hyper3dnet_pruned5\" + dataset + \".txt\"\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write(\"Overall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvoa)), float(np.std(cvoa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Average accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvaa)), float(np.std(cvaa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Kappa accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvka)), float(np.std(cvka))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Precision accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvpre)), float(np.std(cvpre))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Recall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvrec)), float(np.std(cvrec))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"F1 accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvf1)), float(np.std(cvf1))))\n",
        "\n",
        "# Calculate mean and std\n",
        "means = np.mean(confmatrices * 100, axis=0)\n",
        "stds = np.std(confmatrices * 100, axis=0)\n",
        "\n",
        "def plot_confusion_matrix(cm, cms, classescf,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classescf))\n",
        "    plt.xticks(tick_marks, classescf, rotation=45)\n",
        "    plt.yticks(tick_marks, classescf)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "\n",
        "            if (cm[i, j] == 100 or cm[i, j] == 0) and cms[i, j] == 0:\n",
        "                plt.text(j, i, '{0:.0f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.0f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "            else:\n",
        "                plt.text(j, i, '{0:.2f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.2f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "with open('meanshyper3dnet5p', 'wb') as f:\n",
        "    pickle.dump(means, f)\n",
        "with open('stdshyper3dnet5p', 'wb') as f:\n",
        "    pickle.dump(stds, f)\n",
        "with open('cvf1hyper3dnet5p', 'wb') as f:\n",
        "    pickle.dump(cvf1, f)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "classes_list = list(range(0, int(3)))\n",
        "plt.figure()\n",
        "plot_confusion_matrix(means, stds, classescf=classes_list)\n",
        "\n",
        "plt.savefig('MatrixConfusion_WEEDhyper3dnet_pruned_5p.png', dpi=1200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAEmCAYAAAA5jbhCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naKTIrigAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}