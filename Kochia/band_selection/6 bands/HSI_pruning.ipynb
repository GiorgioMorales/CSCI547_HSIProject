{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HSI_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyQTRzMcUCB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare tensorflow-model-optimization for pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNPotwubYRu",
        "colab_type": "code",
        "outputId": "57d99928-c28d-4e74-d8c6-245e680050be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tensorflow-gpu==1.14.0\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 57.1MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 68.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c4/4c3d011e432bd9c19f0323f7da7d3f783402615e4c3b5a98416c7da9cb05/tensorflow_model_optimization-0.2.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Collecting enum34~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/df/6e/46a0304561a07bda942c4283b67228d5b87d1ebd1189c866ede4b4cd6a0a/enum34-1.1.9-py3-none-any.whl\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.17.5)\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.9 tensorflow-model-optimization-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfBLXz1mCNV",
        "colab_type": "code",
        "outputId": "68742902-1886-404d-d4ca-5fd76403ae6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk1dYHpbicZ",
        "colab_type": "text"
      },
      "source": [
        "#Download dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu64LC4Wm1oc",
        "colab_type": "code",
        "outputId": "a8a37da3-f420-447b-d4c4-7013d37a38d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.1.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWT-iIqSb_46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8/view?usp=sharing\n",
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8\n",
        "download = drive.CreateFile({'id': '1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8'})\n",
        "download.GetContentFile('weed_dataset_w25.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDlBcilciuY",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRqt2u-MVfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from operator import truediv\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syCzlwNcr-di",
        "colab_type": "code",
        "outputId": "3073f750-f8e9-43c6-80f4-8200709a4a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "LOAD HDF5 FILE\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "hdf5_file = h5py.File('weed_dataset_w25.hdf5', \"r\")\n",
        "train_x = np.array(hdf5_file[\"train_img\"][...])\n",
        "# train_x = train_x / np.max(train_x)\n",
        "# train_x = np.clip(train_x, 0, 1)\n",
        "#train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))\n",
        "train_y = np.array(hdf5_file[\"train_labels\"][...])\n",
        "\n",
        "# Average consecutive bands\n",
        "img2 = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], int(train_x.shape[3]/2)))\n",
        "for n in range(0, train_x.shape[0]):\n",
        "    # Average consecutive bands\n",
        "    for i in range(0, train_x.shape[3], 2):\n",
        "        img2[n, :, :, int(i/2)] = (train_x[n, :, :, i] + train_x[n, :, :, i + 1]) / 2.\n",
        "\n",
        "train_x = img2\n",
        "\n",
        "print(train_x.shape)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3wrK8dNe-T7",
        "colab_type": "code",
        "outputId": "13503747-d81c-4e07-8e44-c18ffd66c9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3]))\n",
        "\n",
        "temp = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], 6))\n",
        "\n",
        "#temp[:, :, :, 0] = train_x[:, :, :, 2]\n",
        "#temp[:, :, :, 0] = train_x[:, :, :, 9] \n",
        "temp[:, :, :, 0] = train_x[:, :, :, 10] \n",
        "temp[:, :, :, 1] = train_x[:, :, :,20] \n",
        "#temp[:, :, :, 4] = train_x[:, :, :, 37]\n",
        "#temp[:, :, :,5] = train_x[:, :, :, 43]\n",
        "#temp[:, :, :,2] = train_x[:, :, :,55]\n",
        "#temp[:, :, :,3] = train_x[:, :, :, 66]\n",
        "#temp[:, :, :,4] = train_x[:, :, :, 101]\n",
        "#temp[:, :, :, 9] = train_x[:, :, :, 105]\n",
        "temp[:, :, :, 2] = train_x[:, :, :, 36]  \n",
        "#temp[:, :, :, 11] = train_x[:, :, :, 107]\n",
        "#temp[:, :, :, 12] = train_x[:, :, :, 111]\n",
        "temp[:, :, :, 3] = train_x[:, :, :, 66] \n",
        "#temp[:, :, :, 14] = train_x[:, :, :, 114]\n",
        "#temp[:, :, :, 7] = train_x[:, :, :, 121]\n",
        "#temp[:, :, :, 16] = train_x[:, :, :, 122]\n",
        "temp[:, :, :, 4] = train_x[:, :, :, 136] \n",
        "#temp[:, :, :, 18] = train_x[:, :, :, 132]\n",
        "temp[:, :, :, 5] = train_x[:, :, :, 144]\n",
        "train_x = temp\n",
        "\n",
        "print(train_x.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvx55MBgcrZJ",
        "colab_type": "text"
      },
      "source": [
        "# Trained pruned network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2xU6uWWAuz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "21c88b19-2d26-4015-b44a-6c428531d4f4"
      },
      "source": [
        "data = 'WEED'\n",
        "loaded_model = tf.keras.models.load_model(\"selected5-weights-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE2WVq7DItg",
        "colab_type": "code",
        "outputId": "94659a7c-346a-4543-b41b-d18c7dc9ec02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.01,\n",
        "                                                   final_sparsity=0.05,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adadelta',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "990\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 6)]       0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 25, 25, 128)       1816      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 128)       513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 128)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 13, 13, 256)       66946     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 13, 13, 256)       1025      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 13, 13, 256)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 7, 7, 256)         133634    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 7, 7, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_global_a (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_fc3 (Pru (None, 3)                 1541      \n",
            "=================================================================\n",
            "Total params: 205,479\n",
            "Trainable params: 104,857\n",
            "Non-trainable params: 100,622\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYBXiSTtaDt",
        "colab_type": "code",
        "outputId": "bc38fcdd-4cf1-4986-8dba-f40b9bd27992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "cva1 = []\n",
        "cva2 = []\n",
        "cva3 = []\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "data = 'WEED'\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "\n",
        "    ytrain = tf.keras.utils.to_categorical(train_y[train]).astype(np.int32)\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.01,\n",
        "                                                      final_sparsity=0.03,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    loaded_model.load_weights(\"selected5-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "    #new_pruned_model.summary()\n",
        "\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.categorical_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # checkpoint\n",
        "    filepath = \"pruned5-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint, sparsity.UpdatePruningStep()]\n",
        "\n",
        "    # Train model on dataset\n",
        "    print(data + \": Training\" + str(ntrain) + \"begins...\")\n",
        "    history = new_pruned_model.fit(x=train_x[train], y=ytrain, validation_data=(train_x[test], ytest),\n",
        "                        batch_size=96, epochs=epochs, callbacks=callbacks_list)\n",
        "    \n",
        "    new_pruned_model.load_weights(\"pruned5-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "    print(np.sum(ypred*ytest)/len(ytest))\n",
        "    \n",
        "    ntrain += 1\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WEED: Training1begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9984\n",
            "Epoch 00001: val_acc improved from -inf to 0.93196, saving model to pruned5-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 526us/sample - loss: 0.0092 - acc: 0.9982 - val_loss: 0.2182 - val_acc: 0.9320\n",
            "Epoch 2/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9989\n",
            "Epoch 00002: val_acc improved from 0.93196 to 0.94462, saving model to pruned5-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 205us/sample - loss: 0.0084 - acc: 0.9986 - val_loss: 0.1984 - val_acc: 0.9446\n",
            "Epoch 3/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9977\n",
            "Epoch 00003: val_acc did not improve from 0.94462\n",
            "5684/5684 [==============================] - 1s 198us/sample - loss: 0.0101 - acc: 0.9977 - val_loss: 0.1937 - val_acc: 0.9446\n",
            "Epoch 4/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9975\n",
            "Epoch 00004: val_acc did not improve from 0.94462\n",
            "5684/5684 [==============================] - 1s 197us/sample - loss: 0.0101 - acc: 0.9975 - val_loss: 0.1964 - val_acc: 0.9430\n",
            "Epoch 5/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9989\n",
            "Epoch 00005: val_acc did not improve from 0.94462\n",
            "5684/5684 [==============================] - 1s 203us/sample - loss: 0.0083 - acc: 0.9988 - val_loss: 0.1942 - val_acc: 0.9446\n",
            "Epoch 6/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9984\n",
            "Epoch 00006: val_acc improved from 0.94462 to 0.94620, saving model to pruned5-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 211us/sample - loss: 0.0093 - acc: 0.9984 - val_loss: 0.1924 - val_acc: 0.9462\n",
            "Epoch 7/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9980\n",
            "Epoch 00007: val_acc did not improve from 0.94620\n",
            "5684/5684 [==============================] - 1s 205us/sample - loss: 0.0098 - acc: 0.9981 - val_loss: 0.1940 - val_acc: 0.9462\n",
            "Epoch 8/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9987\n",
            "Epoch 00008: val_acc improved from 0.94620 to 0.94937, saving model to pruned5-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 209us/sample - loss: 0.0095 - acc: 0.9988 - val_loss: 0.1921 - val_acc: 0.9494\n",
            "Epoch 9/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9993\n",
            "Epoch 00009: val_acc did not improve from 0.94937\n",
            "5684/5684 [==============================] - 1s 197us/sample - loss: 0.0076 - acc: 0.9991 - val_loss: 0.1917 - val_acc: 0.9478\n",
            "Epoch 10/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9984\n",
            "Epoch 00010: val_acc did not improve from 0.94937\n",
            "5684/5684 [==============================] - 1s 216us/sample - loss: 0.0090 - acc: 0.9984 - val_loss: 0.1903 - val_acc: 0.9478\n",
            "0.931730778634447\n",
            "WEED: Training2begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 5.5369e-04 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 0.94778, saving model to pruned5-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 583us/sample - loss: 8.2929e-04 - acc: 0.9998 - val_loss: 0.3100 - val_acc: 0.9478\n",
            "Epoch 2/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 6.9930e-04 - acc: 0.9998\n",
            "Epoch 00002: val_acc improved from 0.94778 to 0.95095, saving model to pruned5-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 212us/sample - loss: 0.0016 - acc: 0.9995 - val_loss: 0.2940 - val_acc: 0.9509\n",
            "Epoch 3/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 5.6966e-04 - acc: 1.0000\n",
            "Epoch 00003: val_acc did not improve from 0.95095\n",
            "5684/5684 [==============================] - 1s 197us/sample - loss: 5.7913e-04 - acc: 1.0000 - val_loss: 0.2875 - val_acc: 0.9494\n",
            "Epoch 4/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 3.7527e-04 - acc: 1.0000\n",
            "Epoch 00004: val_acc did not improve from 0.95095\n",
            "5684/5684 [==============================] - 1s 203us/sample - loss: 3.7056e-04 - acc: 1.0000 - val_loss: 0.2858 - val_acc: 0.9478\n",
            "Epoch 5/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 6.2583e-04 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 0.95095\n",
            "5684/5684 [==============================] - 1s 198us/sample - loss: 6.4441e-04 - acc: 1.0000 - val_loss: 0.2853 - val_acc: 0.9462\n",
            "Epoch 6/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 3.8325e-04 - acc: 0.9998\n",
            "Epoch 00006: val_acc did not improve from 0.95095\n",
            "5684/5684 [==============================] - 1s 198us/sample - loss: 3.8575e-04 - acc: 0.9998 - val_loss: 0.2833 - val_acc: 0.9462\n",
            "Epoch 7/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 2.5109e-04 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 0.95095\n",
            "5684/5684 [==============================] - 1s 198us/sample - loss: 3.6067e-04 - acc: 1.0000 - val_loss: 0.2915 - val_acc: 0.9430\n",
            "Epoch 8/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 5.6235e-04 - acc: 0.9998\n",
            "Epoch 00008: val_acc did not improve from 0.95095\n",
            "5684/5684 [==============================] - 1s 197us/sample - loss: 5.6883e-04 - acc: 0.9998 - val_loss: 0.2861 - val_acc: 0.9446\n",
            "Epoch 9/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 6.9842e-04 - acc: 0.9998\n",
            "Epoch 00009: val_acc did not improve from 0.95095\n",
            "5684/5684 [==============================] - 1s 195us/sample - loss: 6.9155e-04 - acc: 0.9998 - val_loss: 0.2904 - val_acc: 0.9399\n",
            "Epoch 10/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 7.6971e-04 - acc: 0.9998\n",
            "Epoch 00010: val_acc did not improve from 0.95095\n",
            "5684/5684 [==============================] - 1s 199us/sample - loss: 7.5489e-04 - acc: 0.9998 - val_loss: 0.2859 - val_acc: 0.9415\n",
            "0.9434652313532061\n",
            "WEED: Training3begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9975\n",
            "Epoch 00001: val_acc improved from -inf to 0.94937, saving model to pruned5-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 516us/sample - loss: 0.0079 - acc: 0.9974 - val_loss: 0.1828 - val_acc: 0.9494\n",
            "Epoch 2/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9971\n",
            "Epoch 00002: val_acc improved from 0.94937 to 0.95570, saving model to pruned5-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 208us/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.1442 - val_acc: 0.9557\n",
            "Epoch 3/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9982\n",
            "Epoch 00003: val_acc improved from 0.95570 to 0.95886, saving model to pruned5-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 207us/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.1292 - val_acc: 0.9589\n",
            "Epoch 4/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9986\n",
            "Epoch 00004: val_acc improved from 0.95886 to 0.96044, saving model to pruned5-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 200us/sample - loss: 0.0082 - acc: 0.9982 - val_loss: 0.1259 - val_acc: 0.9604\n",
            "Epoch 5/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9989\n",
            "Epoch 00005: val_acc improved from 0.96044 to 0.96203, saving model to pruned5-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 208us/sample - loss: 0.0067 - acc: 0.9988 - val_loss: 0.1246 - val_acc: 0.9620\n",
            "Epoch 6/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9979\n",
            "Epoch 00006: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 1s 196us/sample - loss: 0.0073 - acc: 0.9979 - val_loss: 0.1254 - val_acc: 0.9620\n",
            "Epoch 7/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9984\n",
            "Epoch 00007: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 1s 196us/sample - loss: 0.0069 - acc: 0.9977 - val_loss: 0.1235 - val_acc: 0.9620\n",
            "Epoch 8/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9991\n",
            "Epoch 00008: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 1s 197us/sample - loss: 0.0056 - acc: 0.9988 - val_loss: 0.1241 - val_acc: 0.9604\n",
            "Epoch 9/10\n",
            "5472/5684 [===========================>..] - ETA: 0s - loss: 0.0074 - acc: 0.9982\n",
            "Epoch 00009: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 1s 194us/sample - loss: 0.0076 - acc: 0.9981 - val_loss: 0.1240 - val_acc: 0.9604\n",
            "Epoch 10/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9984\n",
            "Epoch 00010: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 1s 196us/sample - loss: 0.0068 - acc: 0.9984 - val_loss: 0.1238 - val_acc: 0.9604\n",
            "0.9558694198314259\n",
            "WEED: Training4begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
            "Epoch 00001: val_acc improved from -inf to 0.93829, saving model to pruned5-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 508us/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.2424 - val_acc: 0.9383\n",
            "Epoch 2/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
            "Epoch 00002: val_acc did not improve from 0.93829\n",
            "5684/5684 [==============================] - 1s 202us/sample - loss: 0.0030 - acc: 0.9993 - val_loss: 0.2297 - val_acc: 0.9383\n",
            "Epoch 3/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
            "Epoch 00003: val_acc did not improve from 0.93829\n",
            "5684/5684 [==============================] - 1s 195us/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.2280 - val_acc: 0.9383\n",
            "Epoch 4/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9996\n",
            "Epoch 00004: val_acc did not improve from 0.93829\n",
            "5684/5684 [==============================] - 1s 195us/sample - loss: 0.0023 - acc: 0.9995 - val_loss: 0.2296 - val_acc: 0.9335\n",
            "Epoch 5/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
            "Epoch 00005: val_acc did not improve from 0.93829\n",
            "5684/5684 [==============================] - 1s 192us/sample - loss: 0.0027 - acc: 0.9995 - val_loss: 0.2300 - val_acc: 0.9351\n",
            "Epoch 6/10\n",
            "5472/5684 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
            "Epoch 00006: val_acc did not improve from 0.93829\n",
            "5684/5684 [==============================] - 1s 194us/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.2301 - val_acc: 0.9335\n",
            "Epoch 7/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
            "Epoch 00007: val_acc did not improve from 0.93829\n",
            "5684/5684 [==============================] - 1s 195us/sample - loss: 0.0032 - acc: 0.9993 - val_loss: 0.2321 - val_acc: 0.9351\n",
            "Epoch 8/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9998\n",
            "Epoch 00008: val_acc did not improve from 0.93829\n",
            "5684/5684 [==============================] - 1s 194us/sample - loss: 0.0024 - acc: 0.9996 - val_loss: 0.2319 - val_acc: 0.9351\n",
            "Epoch 9/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
            "Epoch 00009: val_acc did not improve from 0.93829\n",
            "5684/5684 [==============================] - 1s 194us/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.2418 - val_acc: 0.9367\n",
            "Epoch 10/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
            "Epoch 00010: val_acc did not improve from 0.93829\n",
            "5684/5684 [==============================] - 1s 200us/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.2372 - val_acc: 0.9367\n",
            "0.93215502821174\n",
            "WEED: Training5begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
            "Epoch 00001: val_acc improved from -inf to 0.87658, saving model to pruned5-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 530us/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6826 - val_acc: 0.8766\n",
            "Epoch 2/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 00002: val_acc improved from 0.87658 to 0.92247, saving model to pruned5-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 205us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.3759 - val_acc: 0.9225\n",
            "Epoch 3/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9996\n",
            "Epoch 00003: val_acc improved from 0.92247 to 0.93987, saving model to pruned5-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 207us/sample - loss: 0.0020 - acc: 0.9996 - val_loss: 0.2965 - val_acc: 0.9399\n",
            "Epoch 4/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 1.0000\n",
            "Epoch 00004: val_acc improved from 0.93987 to 0.94304, saving model to pruned5-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 222us/sample - loss: 0.0051 - acc: 0.9989 - val_loss: 0.2786 - val_acc: 0.9430\n",
            "Epoch 5/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 00005: val_acc improved from 0.94304 to 0.94620, saving model to pruned5-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 203us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.2751 - val_acc: 0.9462\n",
            "Epoch 6/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
            "Epoch 00006: val_acc did not improve from 0.94620\n",
            "5684/5684 [==============================] - 1s 206us/sample - loss: 0.0020 - acc: 0.9996 - val_loss: 0.2752 - val_acc: 0.9446\n",
            "Epoch 7/10\n",
            "5472/5684 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9998\n",
            "Epoch 00007: val_acc did not improve from 0.94620\n",
            "5684/5684 [==============================] - 1s 196us/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.2764 - val_acc: 0.9430\n",
            "Epoch 8/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9991\n",
            "Epoch 00008: val_acc did not improve from 0.94620\n",
            "5684/5684 [==============================] - 1s 193us/sample - loss: 0.0019 - acc: 0.9991 - val_loss: 0.2773 - val_acc: 0.9446\n",
            "Epoch 9/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9995\n",
            "Epoch 00009: val_acc did not improve from 0.94620\n",
            "5684/5684 [==============================] - 1s 192us/sample - loss: 0.0028 - acc: 0.9995 - val_loss: 0.2776 - val_acc: 0.9446\n",
            "Epoch 10/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
            "Epoch 00010: val_acc did not improve from 0.94620\n",
            "5684/5684 [==============================] - 1s 194us/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 0.2781 - val_acc: 0.9446\n",
            "0.9379872361700259\n",
            "WEED: Training6begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 8.0712e-05 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 0.90506, saving model to pruned5-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 534us/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 0.5115 - val_acc: 0.9051\n",
            "Epoch 2/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 1.2231e-04 - acc: 1.0000\n",
            "Epoch 00002: val_acc improved from 0.90506 to 0.91930, saving model to pruned5-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 210us/sample - loss: 1.2105e-04 - acc: 1.0000 - val_loss: 0.4450 - val_acc: 0.9193\n",
            "Epoch 3/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 2.8611e-04 - acc: 1.0000\n",
            "Epoch 00003: val_acc improved from 0.91930 to 0.92405, saving model to pruned5-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 215us/sample - loss: 2.8047e-04 - acc: 1.0000 - val_loss: 0.4240 - val_acc: 0.9241\n",
            "Epoch 4/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 6.1109e-05 - acc: 1.0000\n",
            "Epoch 00004: val_acc improved from 0.92405 to 0.92563, saving model to pruned5-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 210us/sample - loss: 6.1958e-05 - acc: 1.0000 - val_loss: 0.4160 - val_acc: 0.9256\n",
            "Epoch 5/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 1.0836e-04 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 0.92563\n",
            "5684/5684 [==============================] - 1s 204us/sample - loss: 6.5314e-04 - acc: 0.9996 - val_loss: 0.4156 - val_acc: 0.9225\n",
            "Epoch 6/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 1.8600e-04 - acc: 1.0000\n",
            "Epoch 00006: val_acc improved from 0.92563 to 0.93038, saving model to pruned5-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 1s 204us/sample - loss: 2.0301e-04 - acc: 1.0000 - val_loss: 0.4160 - val_acc: 0.9304\n",
            "Epoch 7/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 3.0407e-04 - acc: 0.9998\n",
            "Epoch 00007: val_acc did not improve from 0.93038\n",
            "5684/5684 [==============================] - 1s 204us/sample - loss: 3.0320e-04 - acc: 0.9998 - val_loss: 0.4186 - val_acc: 0.9304\n",
            "Epoch 8/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 1.3123e-04 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 0.93038\n",
            "5684/5684 [==============================] - 1s 198us/sample - loss: 1.2949e-04 - acc: 1.0000 - val_loss: 0.4182 - val_acc: 0.9288\n",
            "Epoch 9/10\n",
            "5472/5684 [===========================>..] - ETA: 0s - loss: 4.7927e-04 - acc: 0.9998\n",
            "Epoch 00009: val_acc did not improve from 0.93038\n",
            "5684/5684 [==============================] - 1s 191us/sample - loss: 6.4037e-04 - acc: 0.9996 - val_loss: 0.4195 - val_acc: 0.9241\n",
            "Epoch 10/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 1.4143e-04 - acc: 1.0000\n",
            "Epoch 00010: val_acc did not improve from 0.93038\n",
            "5684/5684 [==============================] - 1s 195us/sample - loss: 1.5464e-04 - acc: 1.0000 - val_loss: 0.4199 - val_acc: 0.9225\n",
            "0.9235960148649641\n",
            "WEED: Training7begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 5.0412e-04 - acc: 0.9998\n",
            "Epoch 00001: val_acc improved from -inf to 0.92710, saving model to pruned5-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 529us/sample - loss: 5.2554e-04 - acc: 0.9998 - val_loss: 0.3691 - val_acc: 0.9271\n",
            "Epoch 2/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 2.1768e-04 - acc: 1.0000\n",
            "Epoch 00002: val_acc improved from 0.92710 to 0.93502, saving model to pruned5-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 205us/sample - loss: 2.7948e-04 - acc: 1.0000 - val_loss: 0.3161 - val_acc: 0.9350\n",
            "Epoch 3/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 1.6038e-04 - acc: 1.0000\n",
            "Epoch 00003: val_acc improved from 0.93502 to 0.93661, saving model to pruned5-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 206us/sample - loss: 1.5961e-04 - acc: 1.0000 - val_loss: 0.2967 - val_acc: 0.9366\n",
            "Epoch 4/10\n",
            "5472/5685 [===========================>..] - ETA: 0s - loss: 6.4073e-05 - acc: 1.0000\n",
            "Epoch 00004: val_acc did not improve from 0.93661\n",
            "5685/5685 [==============================] - 1s 195us/sample - loss: 7.4709e-05 - acc: 1.0000 - val_loss: 0.2912 - val_acc: 0.9366\n",
            "Epoch 5/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 1.6597e-04 - acc: 1.0000\n",
            "Epoch 00005: val_acc did not improve from 0.93661\n",
            "5685/5685 [==============================] - 1s 196us/sample - loss: 1.6629e-04 - acc: 1.0000 - val_loss: 0.2853 - val_acc: 0.9366\n",
            "Epoch 6/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 2.5186e-04 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 0.93661\n",
            "5685/5685 [==============================] - 1s 198us/sample - loss: 0.0036 - acc: 0.9996 - val_loss: 0.2967 - val_acc: 0.9334\n",
            "Epoch 7/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 1.3470e-04 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 0.93661\n",
            "5685/5685 [==============================] - 1s 196us/sample - loss: 1.3424e-04 - acc: 1.0000 - val_loss: 0.2888 - val_acc: 0.9319\n",
            "Epoch 8/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 5.4629e-05 - acc: 1.0000\n",
            "Epoch 00008: val_acc did not improve from 0.93661\n",
            "5685/5685 [==============================] - 1s 193us/sample - loss: 5.5299e-05 - acc: 1.0000 - val_loss: 0.2852 - val_acc: 0.9319\n",
            "Epoch 9/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 1.9587e-04 - acc: 1.0000\n",
            "Epoch 00009: val_acc did not improve from 0.93661\n",
            "5685/5685 [==============================] - 1s 195us/sample - loss: 1.9515e-04 - acc: 1.0000 - val_loss: 0.2830 - val_acc: 0.9366\n",
            "Epoch 10/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 1.8667e-04 - acc: 1.0000\n",
            "Epoch 00010: val_acc improved from 0.93661 to 0.93819, saving model to pruned5-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 206us/sample - loss: 1.8342e-04 - acc: 1.0000 - val_loss: 0.2825 - val_acc: 0.9382\n",
            "0.9341558321490973\n",
            "WEED: Training8begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/10\n",
            "5472/5685 [===========================>..] - ETA: 0s - loss: 2.0556e-04 - acc: 1.0000\n",
            "Epoch 00001: val_acc improved from -inf to 0.90491, saving model to pruned5-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 510us/sample - loss: 2.0064e-04 - acc: 1.0000 - val_loss: 0.5521 - val_acc: 0.9049\n",
            "Epoch 2/10\n",
            "5472/5685 [===========================>..] - ETA: 0s - loss: 5.5382e-04 - acc: 1.0000\n",
            "Epoch 00002: val_acc improved from 0.90491 to 0.93819, saving model to pruned5-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 205us/sample - loss: 5.3638e-04 - acc: 1.0000 - val_loss: 0.3829 - val_acc: 0.9382\n",
            "Epoch 3/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 1.8620e-04 - acc: 1.0000\n",
            "Epoch 00003: val_acc improved from 0.93819 to 0.94612, saving model to pruned5-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 213us/sample - loss: 1.8517e-04 - acc: 1.0000 - val_loss: 0.3310 - val_acc: 0.9461\n",
            "Epoch 4/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 5.7733e-04 - acc: 0.9998\n",
            "Epoch 00004: val_acc improved from 0.94612 to 0.94770, saving model to pruned5-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 204us/sample - loss: 5.7899e-04 - acc: 0.9998 - val_loss: 0.3070 - val_acc: 0.9477\n",
            "Epoch 5/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 3.0820e-04 - acc: 0.9998\n",
            "Epoch 00005: val_acc improved from 0.94770 to 0.95246, saving model to pruned5-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 201us/sample - loss: 3.1604e-04 - acc: 0.9998 - val_loss: 0.2975 - val_acc: 0.9525\n",
            "Epoch 6/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 2.9906e-04 - acc: 1.0000\n",
            "Epoch 00006: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 1s 200us/sample - loss: 3.2616e-04 - acc: 1.0000 - val_loss: 0.3194 - val_acc: 0.9509\n",
            "Epoch 7/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 2.4457e-04 - acc: 1.0000\n",
            "Epoch 00007: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 1s 208us/sample - loss: 2.4392e-04 - acc: 1.0000 - val_loss: 0.3255 - val_acc: 0.9429\n",
            "Epoch 8/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 9.4159e-04 - acc: 0.9996\n",
            "Epoch 00008: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 1s 200us/sample - loss: 9.2505e-04 - acc: 0.9996 - val_loss: 0.3052 - val_acc: 0.9493\n",
            "Epoch 9/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 3.8143e-04 - acc: 1.0000\n",
            "Epoch 00009: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 1s 192us/sample - loss: 6.1447e-04 - acc: 0.9998 - val_loss: 0.2970 - val_acc: 0.9493\n",
            "Epoch 10/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 1.7635e-04 - acc: 1.0000\n",
            "Epoch 00010: val_acc did not improve from 0.95246\n",
            "5685/5685 [==============================] - 1s 196us/sample - loss: 1.7661e-04 - acc: 1.0000 - val_loss: 0.2952 - val_acc: 0.9525\n",
            "0.9493532525495116\n",
            "WEED: Training9begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/10\n",
            "5376/5685 [===========================>..] - ETA: 0s - loss: 0.0099 - acc: 0.9978\n",
            "Epoch 00001: val_acc improved from -inf to 0.93344, saving model to pruned5-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 500us/sample - loss: 0.0132 - acc: 0.9965 - val_loss: 0.2889 - val_acc: 0.9334\n",
            "Epoch 2/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9973\n",
            "Epoch 00002: val_acc improved from 0.93344 to 0.94295, saving model to pruned5-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 201us/sample - loss: 0.0098 - acc: 0.9974 - val_loss: 0.2508 - val_acc: 0.9429\n",
            "Epoch 3/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9977\n",
            "Epoch 00003: val_acc did not improve from 0.94295\n",
            "5685/5685 [==============================] - 1s 195us/sample - loss: 0.0104 - acc: 0.9977 - val_loss: 0.2412 - val_acc: 0.9429\n",
            "Epoch 4/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9971\n",
            "Epoch 00004: val_acc improved from 0.94295 to 0.94612, saving model to pruned5-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 207us/sample - loss: 0.0117 - acc: 0.9972 - val_loss: 0.2389 - val_acc: 0.9461\n",
            "Epoch 5/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9984\n",
            "Epoch 00005: val_acc improved from 0.94612 to 0.94770, saving model to pruned5-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 205us/sample - loss: 0.0082 - acc: 0.9982 - val_loss: 0.2380 - val_acc: 0.9477\n",
            "Epoch 6/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9980\n",
            "Epoch 00006: val_acc did not improve from 0.94770\n",
            "5685/5685 [==============================] - 1s 193us/sample - loss: 0.0109 - acc: 0.9979 - val_loss: 0.2365 - val_acc: 0.9477\n",
            "Epoch 7/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9982\n",
            "Epoch 00007: val_acc did not improve from 0.94770\n",
            "5685/5685 [==============================] - 1s 198us/sample - loss: 0.0098 - acc: 0.9981 - val_loss: 0.2548 - val_acc: 0.9398\n",
            "Epoch 8/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9973\n",
            "Epoch 00008: val_acc did not improve from 0.94770\n",
            "5685/5685 [==============================] - 1s 197us/sample - loss: 0.0106 - acc: 0.9972 - val_loss: 0.2415 - val_acc: 0.9445\n",
            "Epoch 9/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9986\n",
            "Epoch 00009: val_acc did not improve from 0.94770\n",
            "5685/5685 [==============================] - 1s 198us/sample - loss: 0.0089 - acc: 0.9981 - val_loss: 0.2371 - val_acc: 0.9445\n",
            "Epoch 10/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9977\n",
            "Epoch 00010: val_acc did not improve from 0.94770\n",
            "5685/5685 [==============================] - 1s 197us/sample - loss: 0.0107 - acc: 0.9975 - val_loss: 0.2354 - val_acc: 0.9445\n",
            "0.934723292055423\n",
            "WEED: Training10begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9977\n",
            "Epoch 00001: val_acc improved from -inf to 0.82409, saving model to pruned5-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 532us/sample - loss: 0.0067 - acc: 0.9977 - val_loss: 1.0152 - val_acc: 0.8241\n",
            "Epoch 2/10\n",
            "5472/5685 [===========================>..] - ETA: 0s - loss: 0.0053 - acc: 0.9987\n",
            "Epoch 00002: val_acc improved from 0.82409 to 0.89382, saving model to pruned5-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 201us/sample - loss: 0.0053 - acc: 0.9988 - val_loss: 0.4569 - val_acc: 0.8938\n",
            "Epoch 3/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9986\n",
            "Epoch 00003: val_acc improved from 0.89382 to 0.92552, saving model to pruned5-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 202us/sample - loss: 0.0063 - acc: 0.9986 - val_loss: 0.3285 - val_acc: 0.9255\n",
            "Epoch 4/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
            "Epoch 00004: val_acc improved from 0.92552 to 0.93027, saving model to pruned5-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 203us/sample - loss: 0.0043 - acc: 0.9991 - val_loss: 0.2922 - val_acc: 0.9303\n",
            "Epoch 5/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9993\n",
            "Epoch 00005: val_acc improved from 0.93027 to 0.93819, saving model to pruned5-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 207us/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.2789 - val_acc: 0.9382\n",
            "Epoch 6/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9993\n",
            "Epoch 00006: val_acc did not improve from 0.93819\n",
            "5685/5685 [==============================] - 1s 193us/sample - loss: 0.0043 - acc: 0.9993 - val_loss: 0.2769 - val_acc: 0.9366\n",
            "Epoch 7/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9982\n",
            "Epoch 00007: val_acc did not improve from 0.93819\n",
            "5685/5685 [==============================] - 1s 197us/sample - loss: 0.0063 - acc: 0.9982 - val_loss: 0.2737 - val_acc: 0.9382\n",
            "Epoch 8/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9986\n",
            "Epoch 00008: val_acc did not improve from 0.93819\n",
            "5685/5685 [==============================] - 1s 192us/sample - loss: 0.0060 - acc: 0.9986 - val_loss: 0.2724 - val_acc: 0.9382\n",
            "Epoch 9/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9991\n",
            "Epoch 00009: val_acc improved from 0.93819 to 0.93978, saving model to pruned5-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 1s 203us/sample - loss: 0.0038 - acc: 0.9991 - val_loss: 0.2715 - val_acc: 0.9398\n",
            "Epoch 10/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
            "Epoch 00010: val_acc did not improve from 0.93978\n",
            "5685/5685 [==============================] - 1s 196us/sample - loss: 0.0041 - acc: 0.9991 - val_loss: 0.2714 - val_acc: 0.9398\n",
            "0.9302430613533006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmz9PSRTvPP",
        "colab_type": "code",
        "outputId": "5338473f-39f8-4a5d-9acb-865b8b2cc708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "new_pruned_model.load_weights(\"pruned5-weights-hyper3dnet\" + data + str(3) + \"-best_3layers_4filters.h5\")\n",
        "final_model = sparsity.strip_pruning(new_pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 6)]       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_8 (Separabl (None, 25, 25, 128)       1046      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 25, 25, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 25, 25, 128)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_9 (Separabl (None, 13, 13, 256)       34176     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 13, 13, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_10 (Separab (None, 7, 7, 256)         68096     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 105,625\n",
            "Trainable params: 104,857\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8drkv1mK1CUb",
        "colab_type": "code",
        "outputId": "26afd52e-2fca-4cc7-9823-0d2db624b501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "import numpy as np\n",
        "for i, w in enumerate(final_model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.4f}%\".format(\n",
        "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "separable_conv2d_8/depthwise_kernel:0 -- Total:150, Zeros: 0.0000%\n",
            "separable_conv2d_8/pointwise_kernel:0 -- Total:768, Zeros: 1.9531%\n",
            "separable_conv2d_8/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "separable_conv2d_9/depthwise_kernel:0 -- Total:1152, Zeros: 0.0000%\n",
            "separable_conv2d_9/pointwise_kernel:0 -- Total:32768, Zeros: 1.9836%\n",
            "separable_conv2d_9/bias:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_11/gamma:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_11/beta:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_11/moving_mean:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_11/moving_variance:0 -- Total:256, Zeros: 0.0000%\n",
            "separable_conv2d_10/depthwise_kernel:0 -- Total:2304, Zeros: 0.0000%\n",
            "separable_conv2d_10/pointwise_kernel:0 -- Total:65536, Zeros: 1.9836%\n",
            "separable_conv2d_10/bias:0 -- Total:256, Zeros: 0.0000%\n",
            "fc3/kernel:0 -- Total:768, Zeros: 1.9531%\n",
            "fc3/bias:0 -- Total:3, Zeros: 0.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIRfZDn02rV",
        "colab_type": "code",
        "outputId": "3cc6e0a8-04d2-40e5-8309-1bba7f3843cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Saving pruned model to: ', \"Kochia_hyper3DNet_pruned.h5\")\n",
        "tf.keras.models.save_model(final_model, \"Kochia_hyper3DNet_5bands_pruned.h5\", \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  Kochia_hyper3DNet_pruned.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tkgm44OZDt",
        "colab_type": "text"
      },
      "source": [
        "# Predict and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVT1KBWvEODg",
        "colab_type": "code",
        "outputId": "8fbd5e23-2b4b-4b83-de14-104fcef6aaa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import truediv\n",
        "import h5py\n",
        "import pickle\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Prepare lists to save metrics\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_m):\n",
        "    list_diag = np.diag(confusion_m)\n",
        "    list_raw_sum = np.sum(confusion_m, axis=1)\n",
        "    each_ac = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_ac)\n",
        "    return each_ac, average_acc\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "PREDICT AND CALCULATE METRICS\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "loaded_model = tf.keras.models.load_model(\"selected5-weights-hyper3dnet\" + 'WEED' + str(1) + \"-best_3layers_4filters.h5\")\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "dataset = 'WEED'\n",
        "# Initialize\n",
        "confmatrices = np.zeros((10, int(classes), int(classes)))\n",
        "\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.30,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.load_weights(\"pruned5-weights-hyper3dnet\" + dataset + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        con_mat = tf.math.confusion_matrix(labels=np.argmax(ytest, axis=-1),\n",
        "                                           predictions=np.argmax(ypred, axis=-1)).numpy()\n",
        "\n",
        "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "    classes_list = list(range(0, int(classes)))\n",
        "    con_mat_df = pd.DataFrame(con_mat_norm, index=classes_list, columns=classes_list)\n",
        "\n",
        "    confmatrices[ntrain - 1, :, :] = con_mat_df.values\n",
        "\n",
        "    # Calculate metrics\n",
        "    oa = accuracy_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    confusion = confusion_matrix(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    prec, rec, f1, support = precision_recall_fscore_support(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1),\n",
        "                                                             average='macro')\n",
        "\n",
        "    # Add metrics to the list\n",
        "    cvoa.append(oa * 100)\n",
        "    cvaa.append(aa * 100)\n",
        "    cvka.append(kappa * 100)\n",
        "    cvpre.append(prec * 100)\n",
        "    cvrec.append(rec * 100)\n",
        "    cvf1.append(f1 * 100)\n",
        "\n",
        "    ntrain += 1\n",
        "\n",
        "file_name = \"classification_report_hyper3dnet_pruned5\" + dataset + \".txt\"\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write(\"Overall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvoa)), float(np.std(cvoa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Average accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvaa)), float(np.std(cvaa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Kappa accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvka)), float(np.std(cvka))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Precision accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvpre)), float(np.std(cvpre))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Recall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvrec)), float(np.std(cvrec))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"F1 accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvf1)), float(np.std(cvf1))))\n",
        "\n",
        "# Calculate mean and std\n",
        "means = np.mean(confmatrices * 100, axis=0)\n",
        "stds = np.std(confmatrices * 100, axis=0)\n",
        "\n",
        "def plot_confusion_matrix(cm, cms, classescf,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classescf))\n",
        "    plt.xticks(tick_marks, classescf, rotation=45)\n",
        "    plt.yticks(tick_marks, classescf)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "\n",
        "            if (cm[i, j] == 100 or cm[i, j] == 0) and cms[i, j] == 0:\n",
        "                plt.text(j, i, '{0:.0f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.0f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "            else:\n",
        "                plt.text(j, i, '{0:.2f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.2f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "with open('meanshyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(means, f)\n",
        "with open('stdshyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(stds, f)\n",
        "with open('cvf1hyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(cvf1, f)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "classes_list = list(range(0, int(3)))\n",
        "plt.figure()\n",
        "plot_confusion_matrix(means, stds, classescf=classes_list)\n",
        "\n",
        "plt.savefig('MatrixConfusion_WEEDhyper3dnet_pruned_6p.png', dpi=1200)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAEmCAYAAAA5jbhCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGXOwmpll7P8eyJ2AWl0drAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}