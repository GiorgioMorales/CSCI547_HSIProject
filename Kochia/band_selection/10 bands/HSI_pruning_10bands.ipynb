{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HSI_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyQTRzMcUCB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare tensorflow-model-optimization for pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNPotwubYRu",
        "colab_type": "code",
        "outputId": "ee9bb906-5119-48ea-f8e1-51da63ff8305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tensorflow-gpu==1.14.0\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 39kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (45.2.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c4/4c3d011e432bd9c19f0323f7da7d3f783402615e4c3b5a98416c7da9cb05/tensorflow_model_optimization-0.2.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.7MB/s \n",
            "\u001b[?25hCollecting enum34~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/63/f6/ccb1c83687756aeabbf3ca0f213508fcfb03883ff200d201b3a4c60cedcc/enum34-1.1.10-py3-none-any.whl\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.17.5)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.10 tensorflow-model-optimization-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfBLXz1mCNV",
        "colab_type": "code",
        "outputId": "8de06e04-c4e6-4430-ed75-a1f1b932517b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk1dYHpbicZ",
        "colab_type": "text"
      },
      "source": [
        "#Download dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu64LC4Wm1oc",
        "colab_type": "code",
        "outputId": "18f783c5-185c-4bd9-8af7-6a1a805280e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.2.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWT-iIqSb_46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8/view?usp=sharing\n",
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8\n",
        "download = drive.CreateFile({'id': '1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8'})\n",
        "download.GetContentFile('weed_dataset_w25.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDlBcilciuY",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRqt2u-MVfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from operator import truediv\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syCzlwNcr-di",
        "colab_type": "code",
        "outputId": "390173d2-8b65-43c6-d6d9-0a15e50acba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "LOAD HDF5 FILE\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "hdf5_file = h5py.File('weed_dataset_w25.hdf5', \"r\")\n",
        "train_x = np.array(hdf5_file[\"train_img\"][...])\n",
        "# train_x = train_x / np.max(train_x)\n",
        "# train_x = np.clip(train_x, 0, 1)\n",
        "#train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))\n",
        "train_y = np.array(hdf5_file[\"train_labels\"][...])\n",
        "\n",
        "# Average consecutive bands\n",
        "img2 = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], int(train_x.shape[3]/2)))\n",
        "for n in range(0, train_x.shape[0]):\n",
        "    # Average consecutive bands\n",
        "    for i in range(0, train_x.shape[3], 2):\n",
        "        img2[n, :, :, int(i/2)] = (train_x[n, :, :, i] + train_x[n, :, :, i + 1]) / 2.\n",
        "\n",
        "train_x = img2\n",
        "\n",
        "print(train_x.shape)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3wrK8dNe-T7",
        "colab_type": "code",
        "outputId": "4d2b9cac-8fef-4468-c825-614b12c8bf17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3]))\n",
        "\n",
        "temp = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], 10))\n",
        "\n",
        "temp[:, :, :, 0] = train_x[:, :, :, 10]\n",
        "temp[:, :, :, 1] = train_x[:, :, :, 15] \n",
        "temp[:, :, :, 2] = train_x[:, :, :,20] \n",
        "temp[:, :, :, 3] = train_x[:, :, :, 27]\n",
        "#temp[:, :, :,5] = train_x[:, :, :, 43]\n",
        "#temp[:, :, :,2] = train_x[:, :, :,55]\n",
        "#temp[:, :, :,3] = train_x[:, :, :, 66]\n",
        "#temp[:, :, :,4] = train_x[:, :, :, 101]\n",
        "#temp[:, :, :, 9] = train_x[:, :, :, 105]\n",
        "temp[:, :, :, 4] = train_x[:, :, :, 36]  \n",
        "temp[:, :, :, 5] = train_x[:, :, :, 59]\n",
        "temp[:, :, :, 6] = train_x[:, :, :, 61]    \n",
        "#temp[:, :, :, 11] = train_x[:, :, :, 107]\n",
        "#temp[:, :, :, 12] = train_x[:, :, :, 111]\n",
        "temp[:, :, :, 7] = train_x[:, :, :, 66] \n",
        "#temp[:, :, :, 14] = train_x[:, :, :, 114]\n",
        "#temp[:, :, :, 7] = train_x[:, :, :, 121]\n",
        "#temp[:, :, :, 16] = train_x[:, :, :, 122]\n",
        "temp[:, :, :, 8] = train_x[:, :, :, 136] \n",
        "#temp[:, :, :, 18] = train_x[:, :, :, 132]\n",
        "temp[:, :, :, 9] = train_x[:, :, :, 144]\n",
        "\n",
        "train_x = temp\n",
        "\n",
        "print(train_x.shape)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvx55MBgcrZJ",
        "colab_type": "text"
      },
      "source": [
        "# Trained pruned network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2xU6uWWAuz",
        "colab_type": "code",
        "outputId": "5c469c8e-8801-414f-c0d5-af0e184e0654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data = 'WEED'\n",
        "loaded_model = tf.keras.models.load_model(\"selected10-weights-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE2WVq7DItg",
        "colab_type": "code",
        "outputId": "d406aa47-b320-4a91-f117-d45840b7051a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.5,\n",
        "                                                   final_sparsity=0.9,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adadelta',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2970\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.debugging.assert_greater_equal is deprecated. Please use tf.compat.v1.debugging.assert_greater_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py:59: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 10, 1)]   0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv3d_6 (None, 25, 25, 10, 16)    882       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 10, 16)    65        \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 10, 16)    1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv3d_7 (None, 25, 25, 10, 16)    13842     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 10, 16)    65        \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 10, 16)    1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_reshape_ (None, 25, 25, 160)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 25, 25, 160)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 25, 25, 320)       106722    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 320)       1281      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 320)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 13, 13, 256)       166978    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 13, 13, 256)       1025      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 13, 13, 256)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 7, 7, 256)         133634    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 7, 7, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_global_a (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_fc3 (Pru (None, 3)                 1541      \n",
            "=================================================================\n",
            "Total params: 426,043\n",
            "Trainable params: 218,035\n",
            "Non-trainable params: 208,008\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYBXiSTtaDt",
        "colab_type": "code",
        "outputId": "218e6032-949a-4094-dee8-b1d079e15923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "cva1 = []\n",
        "cva2 = []\n",
        "cva3 = []\n",
        "\n",
        "\n",
        "data = 'WEED'\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "\n",
        "    ytrain = tf.keras.utils.to_categorical(train_y[train]).astype(np.int32)\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.12,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    loaded_model.load_weights(\"selected10-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "    #new_pruned_model.summary()\n",
        "\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.categorical_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # checkpoint\n",
        "    filepath = \"pruned10-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint, sparsity.UpdatePruningStep()]\n",
        "\n",
        "    # Train model on dataset\n",
        "    print(data + \": Training\" + str(ntrain) + \"begins...\")\n",
        "    history = new_pruned_model.fit(x=train_x[train], y=ytrain, validation_data=(train_x[test], ytest),\n",
        "                        batch_size=32, epochs=epochs, callbacks=callbacks_list)\n",
        "    \n",
        "    new_pruned_model.load_weights(\"pruned10-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.categorical_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "    print(np.sum(ypred*ytest)/len(ytest))\n",
        "    \n",
        "    ntrain += 1\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WEED: Training1begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9981\n",
            "Epoch 00001: val_acc improved from -inf to 0.98259, saving model to pruned10-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 10s 2ms/sample - loss: 0.0068 - acc: 0.9981 - val_loss: 0.0595 - val_acc: 0.9826\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9979\n",
            "Epoch 00002: val_acc improved from 0.98259 to 0.98576, saving model to pruned10-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0503 - val_acc: 0.9858\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9989\n",
            "Epoch 00003: val_acc improved from 0.98576 to 0.98734, saving model to pruned10-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0059 - acc: 0.9989 - val_loss: 0.0525 - val_acc: 0.9873\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9974\n",
            "Epoch 00004: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0065 - acc: 0.9974 - val_loss: 0.0522 - val_acc: 0.9810\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9982\n",
            "Epoch 00005: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0500 - val_acc: 0.9858\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9989\n",
            "Epoch 00006: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0050 - acc: 0.9989 - val_loss: 0.0482 - val_acc: 0.9873\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9972\n",
            "Epoch 00007: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0067 - acc: 0.9972 - val_loss: 0.0498 - val_acc: 0.9873\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9977\n",
            "Epoch 00008: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0063 - acc: 0.9977 - val_loss: 0.0510 - val_acc: 0.9826\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9977\n",
            "Epoch 00009: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0065 - acc: 0.9977 - val_loss: 0.0471 - val_acc: 0.9873\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9984\n",
            "Epoch 00010: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0043 - acc: 0.9984 - val_loss: 0.0470 - val_acc: 0.9873\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9979\n",
            "Epoch 00011: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0060 - acc: 0.9979 - val_loss: 0.0465 - val_acc: 0.9873\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9982\n",
            "Epoch 00012: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0072 - acc: 0.9982 - val_loss: 0.0561 - val_acc: 0.9810\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9975\n",
            "Epoch 00013: val_acc improved from 0.98734 to 0.98892, saving model to pruned10-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0055 - acc: 0.9975 - val_loss: 0.0458 - val_acc: 0.9889\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9981\n",
            "Epoch 00014: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0070 - acc: 0.9981 - val_loss: 0.0463 - val_acc: 0.9858\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9979\n",
            "Epoch 00015: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0471 - val_acc: 0.9858\n",
            "0.983997764362426\n",
            "WEED: Training2begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9977\n",
            "Epoch 00001: val_acc improved from -inf to 0.98576, saving model to pruned10-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 10s 2ms/sample - loss: 0.0087 - acc: 0.9977 - val_loss: 0.0329 - val_acc: 0.9858\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9966\n",
            "Epoch 00002: val_acc improved from 0.98576 to 0.98734, saving model to pruned10-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0087 - acc: 0.9967 - val_loss: 0.0265 - val_acc: 0.9873\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9981\n",
            "Epoch 00003: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0048 - acc: 0.9981 - val_loss: 0.0806 - val_acc: 0.9794\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9975\n",
            "Epoch 00004: val_acc improved from 0.98734 to 0.99209, saving model to pruned10-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0063 - acc: 0.9975 - val_loss: 0.0254 - val_acc: 0.9921\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9984\n",
            "Epoch 00005: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0046 - acc: 0.9982 - val_loss: 0.0235 - val_acc: 0.9905\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9975\n",
            "Epoch 00006: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0084 - acc: 0.9975 - val_loss: 0.0229 - val_acc: 0.9905\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9966\n",
            "Epoch 00007: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0101 - acc: 0.9967 - val_loss: 0.0257 - val_acc: 0.9889\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9982\n",
            "Epoch 00008: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0395 - val_acc: 0.9858\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9988\n",
            "Epoch 00009: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0037 - acc: 0.9986 - val_loss: 0.0269 - val_acc: 0.9921\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9984\n",
            "Epoch 00010: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0268 - val_acc: 0.9905\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9979\n",
            "Epoch 00011: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0065 - acc: 0.9979 - val_loss: 0.0271 - val_acc: 0.9921\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9979\n",
            "Epoch 00012: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0062 - acc: 0.9979 - val_loss: 0.0267 - val_acc: 0.9905\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9977\n",
            "Epoch 00013: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0089 - acc: 0.9977 - val_loss: 0.0263 - val_acc: 0.9921\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9982\n",
            "Epoch 00014: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0047 - acc: 0.9982 - val_loss: 0.0271 - val_acc: 0.9921\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9979\n",
            "Epoch 00015: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0055 - acc: 0.9979 - val_loss: 0.0274 - val_acc: 0.9921\n",
            "0.9898434928071294\n",
            "WEED: Training3begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9984\n",
            "Epoch 00001: val_acc improved from -inf to 0.98892, saving model to pruned10-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 10s 2ms/sample - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0563 - val_acc: 0.9889\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9981\n",
            "Epoch 00002: val_acc improved from 0.98892 to 0.99209, saving model to pruned10-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0067 - acc: 0.9981 - val_loss: 0.0526 - val_acc: 0.9921\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9979\n",
            "Epoch 00003: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0496 - val_acc: 0.9905\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9977\n",
            "Epoch 00004: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0064 - acc: 0.9977 - val_loss: 0.0479 - val_acc: 0.9921\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9979\n",
            "Epoch 00005: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0492 - val_acc: 0.9921\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9979\n",
            "Epoch 00006: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0083 - acc: 0.9979 - val_loss: 0.0504 - val_acc: 0.9921\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9979\n",
            "Epoch 00007: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0500 - val_acc: 0.9921\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9975\n",
            "Epoch 00008: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0052 - acc: 0.9975 - val_loss: 0.0507 - val_acc: 0.9921\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9984\n",
            "Epoch 00009: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0028 - acc: 0.9984 - val_loss: 0.0513 - val_acc: 0.9921\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9979\n",
            "Epoch 00010: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0506 - val_acc: 0.9921\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9981\n",
            "Epoch 00011: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0510 - val_acc: 0.9921\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9975\n",
            "Epoch 00012: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0054 - acc: 0.9975 - val_loss: 0.0504 - val_acc: 0.9921\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9982\n",
            "Epoch 00013: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0505 - val_acc: 0.9921\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
            "Epoch 00014: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0509 - val_acc: 0.9921\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9982\n",
            "Epoch 00015: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0068 - acc: 0.9981 - val_loss: 0.0510 - val_acc: 0.9921\n",
            "0.9889415060739672\n",
            "WEED: Training4begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9991\n",
            "Epoch 00001: val_acc improved from -inf to 0.97785, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 10s 2ms/sample - loss: 0.0048 - acc: 0.9991 - val_loss: 0.1092 - val_acc: 0.9778\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9975\n",
            "Epoch 00002: val_acc improved from 0.97785 to 0.98576, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0070 - acc: 0.9975 - val_loss: 0.0606 - val_acc: 0.9858\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9981\n",
            "Epoch 00003: val_acc improved from 0.98576 to 0.98734, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0081 - acc: 0.9981 - val_loss: 0.0709 - val_acc: 0.9873\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9989\n",
            "Epoch 00004: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0048 - acc: 0.9989 - val_loss: 0.0734 - val_acc: 0.9842\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9981\n",
            "Epoch 00005: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0661 - val_acc: 0.9873\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9982\n",
            "Epoch 00006: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0647 - val_acc: 0.9873\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9986\n",
            "Epoch 00007: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0628 - val_acc: 0.9873\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9993\n",
            "Epoch 00008: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0039 - acc: 0.9993 - val_loss: 0.0742 - val_acc: 0.9858\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9986\n",
            "Epoch 00009: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0034 - acc: 0.9986 - val_loss: 0.0662 - val_acc: 0.9873\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9977\n",
            "Epoch 00010: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0057 - acc: 0.9977 - val_loss: 0.0660 - val_acc: 0.9873\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9988\n",
            "Epoch 00011: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0643 - val_acc: 0.9873\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9982\n",
            "Epoch 00012: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0042 - acc: 0.9982 - val_loss: 0.0602 - val_acc: 0.9858\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9986\n",
            "Epoch 00013: val_acc improved from 0.98734 to 0.98892, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0034 - acc: 0.9986 - val_loss: 0.0602 - val_acc: 0.9889\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9977\n",
            "Epoch 00014: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0057 - acc: 0.9977 - val_loss: 0.0630 - val_acc: 0.9889\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9991\n",
            "Epoch 00015: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0614 - val_acc: 0.9889\n",
            "0.9871057566938491\n",
            "WEED: Training5begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9974\n",
            "Epoch 00001: val_acc improved from -inf to 0.98892, saving model to pruned10-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 10s 2ms/sample - loss: 0.0070 - acc: 0.9972 - val_loss: 0.0977 - val_acc: 0.9889\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9989\n",
            "Epoch 00002: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0836 - val_acc: 0.9873\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9984\n",
            "Epoch 00003: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0817 - val_acc: 0.9889\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9975\n",
            "Epoch 00004: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0062 - acc: 0.9975 - val_loss: 0.0843 - val_acc: 0.9889\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9984\n",
            "Epoch 00005: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0821 - val_acc: 0.9889\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9986\n",
            "Epoch 00006: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.0862 - val_acc: 0.9889\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9981\n",
            "Epoch 00007: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0864 - val_acc: 0.9889\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9991\n",
            "Epoch 00008: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0863 - val_acc: 0.9889\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9982\n",
            "Epoch 00009: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0050 - acc: 0.9982 - val_loss: 0.0868 - val_acc: 0.9873\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9988\n",
            "Epoch 00010: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0879 - val_acc: 0.9873\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9977\n",
            "Epoch 00011: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0087 - acc: 0.9977 - val_loss: 0.0870 - val_acc: 0.9873\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9982\n",
            "Epoch 00012: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0865 - val_acc: 0.9873\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9986\n",
            "Epoch 00013: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0855 - val_acc: 0.9873\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9974\n",
            "Epoch 00014: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0077 - acc: 0.9974 - val_loss: 0.0856 - val_acc: 0.9873\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9968\n",
            "Epoch 00015: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0092 - acc: 0.9968 - val_loss: 0.0846 - val_acc: 0.9873\n",
            "0.9816473264220811\n",
            "WEED: Training6begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9989\n",
            "Epoch 00001: val_acc improved from -inf to 0.97943, saving model to pruned10-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 10s 2ms/sample - loss: 0.0038 - acc: 0.9989 - val_loss: 0.1196 - val_acc: 0.9794\n",
            "Epoch 2/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9995\n",
            "Epoch 00002: val_acc improved from 0.97943 to 0.98259, saving model to pruned10-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0952 - val_acc: 0.9826\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9995\n",
            "Epoch 00003: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.1055 - val_acc: 0.9826\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9991\n",
            "Epoch 00004: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0738 - val_acc: 0.9826\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9993\n",
            "Epoch 00005: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0022 - acc: 0.9989 - val_loss: 0.0717 - val_acc: 0.9810\n",
            "Epoch 6/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9989\n",
            "Epoch 00006: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0732 - val_acc: 0.9810\n",
            "Epoch 7/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9986\n",
            "Epoch 00007: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0036 - acc: 0.9986 - val_loss: 0.0719 - val_acc: 0.9826\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9989\n",
            "Epoch 00008: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0716 - val_acc: 0.9810\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
            "Epoch 00009: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0743 - val_acc: 0.9826\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9998\n",
            "Epoch 00010: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0735 - val_acc: 0.9826\n",
            "Epoch 11/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
            "Epoch 00011: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0741 - val_acc: 0.9810\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9991\n",
            "Epoch 00012: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0028 - acc: 0.9991 - val_loss: 0.1520 - val_acc: 0.9778\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
            "Epoch 00013: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0933 - val_acc: 0.9778\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9991\n",
            "Epoch 00014: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0037 - acc: 0.9991 - val_loss: 0.0883 - val_acc: 0.9810\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9982\n",
            "Epoch 00015: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0093 - acc: 0.9982 - val_loss: 0.0871 - val_acc: 0.9810\n",
            "0.980176661474298\n",
            "WEED: Training7begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9972\n",
            "Epoch 00001: val_acc improved from -inf to 0.98098, saving model to pruned10-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 11s 2ms/sample - loss: 0.0069 - acc: 0.9972 - val_loss: 0.0508 - val_acc: 0.9810\n",
            "Epoch 2/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9981\n",
            "Epoch 00002: val_acc did not improve from 0.98098\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0425 - val_acc: 0.9810\n",
            "Epoch 3/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9975\n",
            "Epoch 00003: val_acc improved from 0.98098 to 0.98415, saving model to pruned10-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0068 - acc: 0.9975 - val_loss: 0.0500 - val_acc: 0.9842\n",
            "Epoch 4/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9970\n",
            "Epoch 00004: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0106 - acc: 0.9970 - val_loss: 0.0430 - val_acc: 0.9826\n",
            "Epoch 5/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9972\n",
            "Epoch 00005: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0091 - acc: 0.9972 - val_loss: 0.0408 - val_acc: 0.9826\n",
            "Epoch 6/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9974\n",
            "Epoch 00006: val_acc improved from 0.98415 to 0.98574, saving model to pruned10-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0072 - acc: 0.9974 - val_loss: 0.0384 - val_acc: 0.9857\n",
            "Epoch 7/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9988\n",
            "Epoch 00007: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0054 - acc: 0.9988 - val_loss: 0.0389 - val_acc: 0.9826\n",
            "Epoch 8/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9974\n",
            "Epoch 00008: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0067 - acc: 0.9974 - val_loss: 0.0669 - val_acc: 0.9731\n",
            "Epoch 9/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9975\n",
            "Epoch 00009: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0068 - acc: 0.9975 - val_loss: 0.0430 - val_acc: 0.9794\n",
            "Epoch 10/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9981\n",
            "Epoch 00010: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0404 - val_acc: 0.9794\n",
            "Epoch 11/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9981\n",
            "Epoch 00011: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0045 - acc: 0.9981 - val_loss: 0.0405 - val_acc: 0.9810\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9981\n",
            "Epoch 00012: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0052 - acc: 0.9979 - val_loss: 0.0407 - val_acc: 0.9826\n",
            "Epoch 13/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9981\n",
            "Epoch 00013: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0364 - val_acc: 0.9826\n",
            "Epoch 14/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9981\n",
            "Epoch 00014: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0072 - acc: 0.9981 - val_loss: 0.0378 - val_acc: 0.9826\n",
            "Epoch 15/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9977\n",
            "Epoch 00015: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0064 - acc: 0.9977 - val_loss: 0.0382 - val_acc: 0.9810\n",
            "0.9845172185037527\n",
            "WEED: Training8begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9977\n",
            "Epoch 00001: val_acc improved from -inf to 0.98891, saving model to pruned10-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 10s 2ms/sample - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0784 - val_acc: 0.9889\n",
            "Epoch 2/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9984\n",
            "Epoch 00002: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0073 - acc: 0.9982 - val_loss: 0.0878 - val_acc: 0.9842\n",
            "Epoch 3/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9984\n",
            "Epoch 00003: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0850 - val_acc: 0.9857\n",
            "Epoch 4/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9979\n",
            "Epoch 00004: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0052 - acc: 0.9979 - val_loss: 0.0846 - val_acc: 0.9842\n",
            "Epoch 5/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9979\n",
            "Epoch 00005: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0051 - acc: 0.9979 - val_loss: 0.0859 - val_acc: 0.9842\n",
            "Epoch 6/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9984\n",
            "Epoch 00006: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0850 - val_acc: 0.9842\n",
            "Epoch 7/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9982\n",
            "Epoch 00007: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0042 - acc: 0.9982 - val_loss: 0.0850 - val_acc: 0.9842\n",
            "Epoch 8/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9989\n",
            "Epoch 00008: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0828 - val_acc: 0.9857\n",
            "Epoch 9/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9977\n",
            "Epoch 00009: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0063 - acc: 0.9977 - val_loss: 0.0858 - val_acc: 0.9857\n",
            "Epoch 10/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9993\n",
            "Epoch 00010: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0042 - acc: 0.9991 - val_loss: 0.0847 - val_acc: 0.9842\n",
            "Epoch 11/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9991\n",
            "Epoch 00011: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0846 - val_acc: 0.9842\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9991\n",
            "Epoch 00012: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0032 - acc: 0.9991 - val_loss: 0.0849 - val_acc: 0.9857\n",
            "Epoch 13/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9982\n",
            "Epoch 00013: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0041 - acc: 0.9982 - val_loss: 0.0839 - val_acc: 0.9842\n",
            "Epoch 14/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9982\n",
            "Epoch 00014: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0049 - acc: 0.9982 - val_loss: 0.0841 - val_acc: 0.9842\n",
            "Epoch 15/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9984\n",
            "Epoch 00015: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0838 - val_acc: 0.9842\n",
            "0.9860468693548847\n",
            "WEED: Training9begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9982\n",
            "Epoch 00001: val_acc improved from -inf to 0.97940, saving model to pruned10-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 11s 2ms/sample - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0839 - val_acc: 0.9794\n",
            "Epoch 2/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9970\n",
            "Epoch 00002: val_acc improved from 0.97940 to 0.98415, saving model to pruned10-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0087 - acc: 0.9968 - val_loss: 0.0740 - val_acc: 0.9842\n",
            "Epoch 3/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9979\n",
            "Epoch 00003: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0066 - acc: 0.9979 - val_loss: 0.0720 - val_acc: 0.9826\n",
            "Epoch 4/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9981\n",
            "Epoch 00004: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0054 - acc: 0.9981 - val_loss: 0.0622 - val_acc: 0.9826\n",
            "Epoch 5/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9968\n",
            "Epoch 00005: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0076 - acc: 0.9968 - val_loss: 0.0713 - val_acc: 0.9826\n",
            "Epoch 6/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9974\n",
            "Epoch 00006: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0068 - acc: 0.9974 - val_loss: 0.0721 - val_acc: 0.9826\n",
            "Epoch 7/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9984\n",
            "Epoch 00007: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0048 - acc: 0.9984 - val_loss: 0.0736 - val_acc: 0.9826\n",
            "Epoch 8/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9988\n",
            "Epoch 00008: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0720 - val_acc: 0.9810\n",
            "Epoch 9/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9968\n",
            "Epoch 00009: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0090 - acc: 0.9968 - val_loss: 0.0720 - val_acc: 0.9842\n",
            "Epoch 10/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9981\n",
            "Epoch 00010: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0733 - val_acc: 0.9842\n",
            "Epoch 11/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9979\n",
            "Epoch 00011: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0726 - val_acc: 0.9826\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9986\n",
            "Epoch 00012: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0914 - val_acc: 0.9746\n",
            "Epoch 13/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9968\n",
            "Epoch 00013: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0101 - acc: 0.9968 - val_loss: 0.0775 - val_acc: 0.9810\n",
            "Epoch 14/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9984\n",
            "Epoch 00014: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0044 - acc: 0.9984 - val_loss: 0.0759 - val_acc: 0.9826\n",
            "Epoch 15/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9979\n",
            "Epoch 00015: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0056 - acc: 0.9979 - val_loss: 0.0761 - val_acc: 0.9826\n",
            "0.9792943072525405\n",
            "WEED: Training10begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9996\n",
            "Epoch 00001: val_acc improved from -inf to 0.98257, saving model to pruned10-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 11s 2ms/sample - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0682 - val_acc: 0.9826\n",
            "Epoch 2/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9982\n",
            "Epoch 00002: val_acc improved from 0.98257 to 0.98732, saving model to pruned10-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0046 - acc: 0.9982 - val_loss: 0.0615 - val_acc: 0.9873\n",
            "Epoch 3/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9989\n",
            "Epoch 00003: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0022 - acc: 0.9989 - val_loss: 0.0597 - val_acc: 0.9857\n",
            "Epoch 4/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9991\n",
            "Epoch 00004: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0576 - val_acc: 0.9857\n",
            "Epoch 5/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
            "Epoch 00005: val_acc improved from 0.98732 to 0.98891, saving model to pruned10-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0025 - acc: 0.9989 - val_loss: 0.0577 - val_acc: 0.9889\n",
            "Epoch 6/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9984\n",
            "Epoch 00006: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0040 - acc: 0.9984 - val_loss: 0.0578 - val_acc: 0.9889\n",
            "Epoch 7/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
            "Epoch 00007: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0570 - val_acc: 0.9889\n",
            "Epoch 8/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9991\n",
            "Epoch 00008: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 7s 1ms/sample - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0590 - val_acc: 0.9889\n",
            "Epoch 9/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9982\n",
            "Epoch 00009: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0030 - acc: 0.9982 - val_loss: 0.0596 - val_acc: 0.9889\n",
            "Epoch 10/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9991\n",
            "Epoch 00010: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0585 - val_acc: 0.9889\n",
            "Epoch 11/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
            "Epoch 00011: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.0580 - val_acc: 0.9889\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9988\n",
            "Epoch 00012: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0035 - acc: 0.9986 - val_loss: 0.0559 - val_acc: 0.9889\n",
            "Epoch 13/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9991\n",
            "Epoch 00013: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0566 - val_acc: 0.9889\n",
            "Epoch 14/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
            "Epoch 00014: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0548 - val_acc: 0.9889\n",
            "Epoch 15/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
            "Epoch 00015: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0571 - val_acc: 0.9889\n",
            "0.9866508456323357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmz9PSRTvPP",
        "colab_type": "code",
        "outputId": "1b273f2e-dab5-44a3-c55e-bd9792d05880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "new_pruned_model.load_weights(\"pruned10-weights-hyper3dnet\" + data + str(2) + \"-best_3layers_4filters.h5\")\n",
        "final_model = sparsity.strip_pruning(new_pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 10, 1)]   0         \n",
            "_________________________________________________________________\n",
            "conv3d_6 (Conv3D)            (None, 25, 25, 10, 16)    448       \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 25, 25, 10, 16)    64        \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 25, 25, 10, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_7 (Conv3D)            (None, 25, 25, 10, 16)    6928      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 25, 25, 10, 16)    64        \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 25, 25, 10, 16)    0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 25, 25, 160)       0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 25, 25, 160)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_8 (Separabl (None, 25, 25, 320)       55520     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 25, 25, 320)       1280      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 25, 25, 320)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_9 (Separabl (None, 13, 13, 256)       85056     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 13, 13, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_10 (Separab (None, 7, 7, 256)         68096     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 219,251\n",
            "Trainable params: 218,035\n",
            "Non-trainable params: 1,216\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8drkv1mK1CUb",
        "colab_type": "code",
        "outputId": "2b6c18b6-eb69-41ba-83b2-062ae23e9b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "import numpy as np\n",
        "for i, w in enumerate(final_model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.4f}%\".format(\n",
        "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv3d_6/kernel:0 -- Total:432, Zeros: 11.1111%\n",
            "conv3d_6/bias:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_12/gamma:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_12/beta:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_12/moving_mean:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_12/moving_variance:0 -- Total:16, Zeros: 0.0000%\n",
            "conv3d_7/kernel:0 -- Total:6912, Zeros: 11.1111%\n",
            "conv3d_7/bias:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_13/gamma:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_13/beta:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_13/moving_mean:0 -- Total:16, Zeros: 0.0000%\n",
            "batch_normalization_13/moving_variance:0 -- Total:16, Zeros: 0.0000%\n",
            "separable_conv2d_8/depthwise_kernel:0 -- Total:4000, Zeros: 0.0000%\n",
            "separable_conv2d_8/pointwise_kernel:0 -- Total:51200, Zeros: 11.1074%\n",
            "separable_conv2d_8/bias:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_14/gamma:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_14/beta:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_14/moving_mean:0 -- Total:320, Zeros: 0.0000%\n",
            "batch_normalization_14/moving_variance:0 -- Total:320, Zeros: 0.0000%\n",
            "separable_conv2d_9/depthwise_kernel:0 -- Total:2880, Zeros: 0.0000%\n",
            "separable_conv2d_9/pointwise_kernel:0 -- Total:81920, Zeros: 11.1072%\n",
            "separable_conv2d_9/bias:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_15/gamma:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_15/beta:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_15/moving_mean:0 -- Total:256, Zeros: 0.0000%\n",
            "batch_normalization_15/moving_variance:0 -- Total:256, Zeros: 0.0000%\n",
            "separable_conv2d_10/depthwise_kernel:0 -- Total:2304, Zeros: 0.0000%\n",
            "separable_conv2d_10/pointwise_kernel:0 -- Total:65536, Zeros: 11.1069%\n",
            "separable_conv2d_10/bias:0 -- Total:256, Zeros: 0.0000%\n",
            "fc3/kernel:0 -- Total:768, Zeros: 11.0677%\n",
            "fc3/bias:0 -- Total:3, Zeros: 0.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIRfZDn02rV",
        "colab_type": "code",
        "outputId": "4bf53a3c-cda2-4a49-8629-b6920e4f3175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Saving pruned model to: ', \"Kochia_hyper3DNet_pruned.h5\")\n",
        "tf.keras.models.save_model(final_model, \"Kochia_hyper3DNet_10bands_pruned.h5\", \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  Kochia_hyper3DNet_pruned.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tkgm44OZDt",
        "colab_type": "text"
      },
      "source": [
        "# Predict and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVT1KBWvEODg",
        "colab_type": "code",
        "outputId": "a9576210-c8c5-4427-cb14-87f84ff8489b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import truediv\n",
        "import h5py\n",
        "import pickle\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Prepare lists to save metrics\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_m):\n",
        "    list_diag = np.diag(confusion_m)\n",
        "    list_raw_sum = np.sum(confusion_m, axis=1)\n",
        "    each_ac = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_ac)\n",
        "    return each_ac, average_acc\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "PREDICT AND CALCULATE METRICS\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "loaded_model = tf.keras.models.load_model(\"selected10-weights-hyper3dnet\" + 'WEED' + str(1) + \"-best_3layers_4filters.h5\")\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "dataset = 'WEED'\n",
        "# Initialize\n",
        "confmatrices = np.zeros((10, int(classes), int(classes)))\n",
        "\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.30,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.load_weights(\"pruned10-weights-hyper3dnet\" + dataset + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        con_mat = tf.math.confusion_matrix(labels=np.argmax(ytest, axis=-1),\n",
        "                                           predictions=np.argmax(ypred, axis=-1)).numpy()\n",
        "\n",
        "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "    classes_list = list(range(0, int(classes)))\n",
        "    con_mat_df = pd.DataFrame(con_mat_norm, index=classes_list, columns=classes_list)\n",
        "\n",
        "    confmatrices[ntrain - 1, :, :] = con_mat_df.values\n",
        "\n",
        "    # Calculate metrics\n",
        "    oa = accuracy_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    confusion = confusion_matrix(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    prec, rec, f1, support = precision_recall_fscore_support(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1),\n",
        "                                                             average='macro')\n",
        "\n",
        "    # Add metrics to the list\n",
        "    cvoa.append(oa * 100)\n",
        "    cvaa.append(aa * 100)\n",
        "    cvka.append(kappa * 100)\n",
        "    cvpre.append(prec * 100)\n",
        "    cvrec.append(rec * 100)\n",
        "    cvf1.append(f1 * 100)\n",
        "\n",
        "    ntrain += 1\n",
        "\n",
        "file_name = \"classification_report_hyper3dnet_pruned10\" + dataset + \".txt\"\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write(\"Overall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvoa)), float(np.std(cvoa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Average accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvaa)), float(np.std(cvaa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Kappa accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvka)), float(np.std(cvka))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Precision accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvpre)), float(np.std(cvpre))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Recall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvrec)), float(np.std(cvrec))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"F1 accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvf1)), float(np.std(cvf1))))\n",
        "\n",
        "# Calculate mean and std\n",
        "means = np.mean(confmatrices * 100, axis=0)\n",
        "stds = np.std(confmatrices * 100, axis=0)\n",
        "\n",
        "def plot_confusion_matrix(cm, cms, classescf,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classescf))\n",
        "    plt.xticks(tick_marks, classescf, rotation=45)\n",
        "    plt.yticks(tick_marks, classescf)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "\n",
        "            if (cm[i, j] == 100 or cm[i, j] == 0) and cms[i, j] == 0:\n",
        "                plt.text(j, i, '{0:.0f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.0f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "            else:\n",
        "                plt.text(j, i, '{0:.2f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.2f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "with open('meanshyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(means, f)\n",
        "with open('stdshyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(stds, f)\n",
        "with open('cvf1hyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(cvf1, f)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "classes_list = list(range(0, int(3)))\n",
        "plt.figure()\n",
        "plot_confusion_matrix(means, stds, classescf=classes_list)\n",
        "\n",
        "plt.savefig('MatrixConfusion_WEEDhyper3dnet_pruned_10p.png', dpi=1200)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAEmCAYAAAA5jbhCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd1wURxvA8d9SlCJIU5JYwAqW2Hvv\nYjexxESTaOzG3nvU2HvvJrEl9jemqNh711hiQQW7EhVBUFDavn8cnJ60E+/gDp9vPvdJdm52b2dz\nPs7Nzs6jqKqKEEIIw7FI7xMQQoiMRgKrEEIYmARWIYQwMAmsQghhYBJYhRDCwKzS+wTepFjZqkpm\nx/Q+DbNXwjtXep9ChqGk9wlkELdv3+LJkycGvZyWjh6qGh2hd3014rGvqqo+hjyHpJhWYM3sSGbv\nNul9Gmbv8LHZ6X0KGYaFhYRWQ6hcvozBj6lGR5DZq7Xe9V+eW+Bm8JNIgkkFViGE0J8CimmOZkpg\nFUKYJwVQTPMXhQRWIYT5kh6rEEIYmPRYhRDCkGSMVQghDE96rEIIYUAK0mMVQgjDUqTHKoQQBic9\nViGEMDDpsQohhCHJrAAhhDAsBbCwTO+zSJQEViGEmZIeqxBCGJ6Jrj4mgVUIYZ5kHqsQQhiBzAoQ\nQghDkjFWIYQwPOmxCiGEgUmPVQghDEiRtQKEEMLwpMcqhBAGJj1WIYQwJJkVIIQQhmeiPVbTDPcG\n1qTGp5xcN4SQYzO48sdoeretkaDOR26OLPnhK/y3j+XxoakcWzuINg1Kp3hsaytLhnWuz7+/j+Tp\nkWn8+/tIRnZtQCbrxBeHUBSFw6sHEHFmDg2qFnnfpqW7K1cu07B+Hdyc7MnnmYMfx44mJiYmxf0u\nX75Ek4b1cXOyJ/cn2ejTszvPnz9Psv5ff2zFPrMFVSqWNeTpm4wrly/ToF5tXBztyJP7E8aNSfk6\nnj51ii4dO1DEOz8ujnYUK+LFhB/H8vLlS516y5cuoZFPXTxyuOPumpWa1Sqze9dOYzYnbcQ/eaXv\nKw1l+B5rxeJ5WDftO1b+cYJhs3+nbFEPxvduSmysyvzfDgCaYLdpZmdcstoxYu4fBD4J47M6xfl5\n/DdEvIxi674LSR5/fK8mdGpZmbEL/+ac3z1Keufih+4NcXKwZeD0LQnqd2hegRzZnYzW3rQUHBxM\n4wZ18fYuzPpNv3MzwJ9hQwYSGxvLD2PHJ7nfs2fPaFi/NgUKFGTVmnUEPQ1i5PAhBAYGsn7T/xLU\nf/nyJUMG9Se7u7sxm5NugoODaehTh0KFCrNxy1YC/P0ZOngAsbGxjBmX9HXctHE9AQH+DBg4hPwF\nCnDxwgXGjRnFxYsXWLdhs7belMkTqFfPh67dv8fe3p7f1q6haSMfNmz+ncZNmqZFE41EhgLSzbDO\n9Tl2/iY9flwHwJ7jfjg52DGsc32WbDxMVHQMBTyyUbpIblr0Xcq2Q5cA2H/qGmWLetCyXslkA2tr\nn9Is23SEuWv3A3Dw9A0+yZaVLxqUSRBYnRxsGfN9Y0bN+5PFo780ToPT0PJli3kZEcFvGzbj6OgI\n1CU0NJSJ48fSb8DguLKEli5ZyMuICDZu+QMnJ81fMq4urrRq0YyzZ05TqnQZnfqzZ07jkxw5yJM3\nH5cv/WvsZqW55Us113Hdxi04OjpSu05dQsNCmTBuDP0HJn0dBw4eipubm3a7WvUa2NjY0LNHV27f\nvo2HhwcAx06e1alXu05dbty4zrw5s8w8sCJDAemlWMGc7Dnhp1O2+/hVXLLaU76YJ6D5OQ/w7Lnu\nT6hnYREoKfyPs7ay5NnzCJ2ykOcRif7//qF7Q46fD2DfyWvv2ArTtMt3B3Xq1tf5g9+qdRsiIiI4\nfPBAkvtdPH+ekqXLaIMqQK06dVEUhR3b/9ape/fOHWbNmMbUGbMN3wAT4btjO3XqJX4dDyVzHd8M\nlvGKlygJwMMHD5KtV6JESR4+fJCg3OyY6FBAhg+sNpmtiIrSHauKjIoGwDvPRwBcuvGQkxdvMbpb\nA/LlyoaDfWbaNSlHxeJ5Wbb5SLLH/+X3Y3T6vBIVi+fB3jYTlUvkpXPLyizecEinXtH8n/BNswoM\nnbXVgK1LX35+Vyno5aVTlit3buzs7PDzu5rkfi9fviSTdSadMisrKywsLPC7qrvfsCED+bxlK0qW\nLGW4Ezcx1/yu4uXlrVOWW4/rmJgTx49hYWFB3nz5UqxXoEDBdz5XkxP/kIA+rzSU4YcC/O8+oXTh\n3DplZYtofiI5O9ppy5r1WszGmZ349/eRgCb4dh37KwdOXU/2+CPn/YmtjTV7f+qrLVu84RCTlvnq\n1Js5uAWL1x8i4N4Tcn/s8l5tMhUhwcFkzZpwvNjJ2ZmQkOAk98ubLx8b1v9GVFQU1tbWAPxz9gwx\nMTE8ffpUW2//vr3s2b2Tc//6JXWoDCE4uesYnPR1fFtgYCBTJo3nq7Zfkz179iTrrfz5J86d+4fJ\n02ak6nxNhmK6Y6ymeVYGtHzzEZrU+JQOn1XEycGWOhW96dW2JgCqqgKam1fLx7XDJas97Yb+TL0u\n85j/6wEWjfqSuhW9kzs8/b+pRZsGZeg3ZRN1Os2l/9RNtGlQmlHdGmjrtKpXkgIe2Zm8wjeZI304\nOnzXmSePHzOgby8CAwO5fPkSfXt/j6WlJRYWmq9kdHQ0A/v3YdCQ4bhn0JtWhhQZGUm7L1tjnyUL\nU2fMSrLe2TNn6N+3F9/36kP1GjXT8AyNRHqs6WPl1uMUK/AJc4e2YuHINryIeMXIuX8ya0hLAoNC\nAWhYtQiNqhWlaPPx+N99DMChMzfI6e7EhD7N2HUs8Z9jrk72/NCjEX2nbOLn/x0D4Mg//kRGxTBr\nSEsWrz9EcFg4E/s0Y+bKPVgoFmTNYoujvQ0A9jaZyGKXmefhr9LgShiek7MzoaHPEpSHBAfj5OSc\n5H5e3t7MW7iEoYP6s2L5UiwsLPiuY2cURcH9I83wzM8rlhEa+ox237QnJCQE0ASPmJgYQkJCsLe3\n1/Z2zZ1zctfROenrGE9VVTp2+IYrly+x98ARnJPY52ZAAJ83a0TNWrWZYu691Tgp3QNJLxk+sMbG\nqvSbupmxi7aRw92JW/eD8PLU9IBOXrwFgJdndl5EvNIG1Xjn/e7RqHrRJI+dJ4crmaytuOB3P8F+\n1laW5P7YhcioGHJ+5MzUAZ8xdcBnOvVWT26P/93HFG2e9JQaU+bl5Y2fn+7P9Ht37xIeHp5gzPBt\n37b/ji/afMWNG9fJli07bm5u5PrYjfYdOgJw7Zof9+/dI0+ujxLsm8PdheU/r+LLr9oZrjHpqKCX\nd4Kx1Lt6XkeAgf378tcfW/lr+y68vBOv/+jRI5o0qk+u3B6sWrsOS0vTTML3LhQksKa7kLAIQsI0\nd++7tKrCsXMBXLv1CIA7D4Oxt81MAY/sXL/9SLtPyUK5uP3gaaLHi98PoIR3Ts5cvqOzH8DtB0E8\nj3hFvS7zdPb7yNWBVZPaM2r+nymO4ZqyuvV9mDNzOmFhYTg4OACauZW2trZUqVY9xf1tbGwoWvRT\nANasXklsbCyft2wNQLfuPWnStLlO/RnTpnD71k3mLliMl3chA7cm/dT3acCsGdMSvY5VU7iO06ZM\nYvHC+az5bQOVq1RJtM7z58/5rElDALZs/Qs7O7tE65kdJe5lgjJ8YC1X1INKJfNy3u8+jvY2tPYp\nRZ0KhajdcY62zo4jl7nz8CkbZnRk0jJfHgc/p0GVIrSsV4o+kzdq633VqCxLRn9JkWY/cicwmEdP\nw/hj3wXG926CTWZrLl6/T/GCORnR1YfNu/7hScgLQDOs8Kb4m1eXbjzk1L+30+AqGEenzt1YtGAe\nX7ZuQf+Bg7l1M4CJ48fSq08/nalDnxYqQJVq1Vi0ZAUAoaGhTJ08gSpVqmFpZcXBA/uYO3sm8xct\nxcVFc23y5c9Pvvz5dT5vzeqVBAU9oVr1GmnWxrTQqUs3Fs6fS5tWnzNg0BBuBgQwYdwYevftr3Md\ni3jnp2rV6ixeprmO6377ldEjh/P1N+355JMcnDh+XFs3b758ZMuWDYA2rT7n4sULLF3xCwH+/gT4\n+2vrla9QIY1aaQyK9FjTS1R0DC3rlmJElwbExqocOedPrY6zuXTjobbO8/BXNOi2gB97NWZyv+Y4\n2NsQcO8JPSesZ8WWo9p6FhYKVlaWOgPhnX5Yw/DOPvRoU42P3Rx58PgZKzYfZdLyjH+jytnZmb93\n7KZ/3160+rwpWZ2c6Nm7LyNGjdGpFx0TTUxMrHbb0tKS8+fO8ctPy4mIiKBwkaKs+XUDTZo150Pk\n7OzMNt899OvTkxbNm+Dk5ESvPv0YOXqMTr3o6Gidx1z3xD2WunrVL6xe9YtO3aXLf+brb9tr6u3e\nBUCHb9om+OyIKNVwDUkHphpYlfg746bAwt5dzezdJr1Pw+wFHcu4k+nTmoWJplc2N5XLl+HMmdMG\nvZiWLnlU+3pj9a4ftv7bM6qqlkm55vsz6nQrRVF8FEXxUxTlhqIoQ435WUKID4+iKHq/0pLRAqui\nKJbAAqABUBj4UlGUwsb6PCHEB0Z5x1caMmaPtRxwQ1XVAFVVI4F1QDMjfp4Q4gOioH9vNa17rMa8\neZUDuPvG9j2g/NuVFEXpAnQBIJODEU9HCJHRmOrNq3SfFaCq6lJgKWhuXqXz6QghzEj8I9Cmxphn\ndR/I9cZ2zrgyk+adx51ti74n6Mg0AnaMY1S3BnrdGW5VryRH1w7k8aGp+G8fy/KxbfnYTXcdzbw5\n3Zg3vDUn1w3h+clZ+C7paaxmpLvUZBbwqVsT+8wWib5OHD/23sc3V6nJLrBl8yZqVK1EDndXnLLY\nUKyIF5MnjicyMjJB3ejoaKZNnUzRQgXIap+ZfJ45GTSgn7GaYzgmPMZqzB7rKaCAoih50ATUNsBX\nRvy89+bkYMu2Rd9zJSCQVv2XkzenG5P7NcNCURi7aFuS+zWqVpRVk9qzeP1Bhs/+g4/cHBnToyFb\n5nSlUrvp2sVeCuf7CJ/KhTn57y2srUzzb1pDSG1mgVlzFxAWGqpT9uPYH7hw/h9Kl3mdkiW1xzdH\nqc0u8DQoiBo1a9FvwCCcnJw4deokE8aNITAwkNlz5+vU7fxde/bv38uIkT/g5e3Nvbt3uXLlsrGb\nZhAf3FCAqqrRiqL0BHwBS+AnVVUvGevz9FG1dH52Lu2Fbek+ib7fqWVlbDJb02bQCsJevGLvCT8c\n7TMzomsDZq7aQ9iLxBdL+cKnNGev3KXf1NfpMMJevGTTrM4U9MiO363/APj74CX+OqBZAf/XKR1w\ndbI3cAvTxsED+2lQrxYvXsUm+n5qMwsUKqQ7aSQyMpJ/zp6mRcvWWFm9/qqm9vim6OCB/dSvUzPJ\nifqpzS7QqUtXne3qNWoSFhrKkkULmDVnnjYg7fTdwaaN6zl55jyFCpvXpB3FCE9eKYrSD+gEqMBF\noAPwMZqb767AGeDruBvySTJqt0lV1W2qqhZUVTWfqqoTjPlZhlC/UiF2H7uqE0A37vwHO5tMVC2V\nP8n9rK0sCX07i0DcugRv/n83pYcxjCm1mQUSO05wcDCtvvgyQbkhjm8OUptdIDEuLq4JhgJW/vIT\nNWrWMrugGs+QswIURckB9AbKqKpaFE2HsA0wBZilqmp+IBjomNKxMu7v0TiWlhavX3ED3Tpllq8v\nQUFPd23vMt7dwGBeRLzSroiVmJV/HKdyyXx81agsDvaZyZ87G2N6NGTfyWtcvflfkvuZC1VViY6O\n1r7ix/feLIuOjtbWT21mgbdt3LieHDlzUrlKVZ1yQx0/PbzrtXzf7AIxMTGEh4dz5PBhFi6YS+eu\n3XWCzKmTJ8hfoCB9e/cku4sjLo52fNHqcx48MJO0LYYfY7UCbBVFsQLsgIdALWBT3PsrgRSfvU73\nWQHG1K5JOZaNSfh89POTugsBxw8NODva8SwsIkH9kNAInByTXhFox+HLdBmzlkWjvmTFOM1SdsfO\nBdBy0PL3OX2TsWb1Srp1/i5BeVZ73fQq8UMDqc0s8Kbw8HC2/fUH33XqkqC3YYjjp5c1q1bSpVOH\nBOUOtrpry8YPDbxvdgHXrPa8eqX5Bda23TdMmjJN5/3/AgNZs+oXPi1WnFVr1xEWFsaIYYP5ouVn\nHDxy3GTHMAFNklYDnp+qqvcVRZkO3AEigJ1ofvqHqKoa/7fdPTRTSZOVoQPrtoP/UrnddO12yUK5\nmD/iC50yQ6hWJj9zh7VmwW8H8D16BXcXB0Z09WH9jI407L6A2FjzHgJo2KgJh46e1G7/c/YMvXt2\n1ykztG1//8mLFy9o/YX5Z7N9U8PGTTh87JR2+5+zZ+j1fTedMkPad/Ao4eHhnD51kkkTxtGvd0/m\nzF+ofV9VVVRVZeOWrbi6ugLw8ccfU7dWdfbv20vNWrWNcl6G8o6B1U1RlNNvbC+Nm+4ZfyxnNA8x\n5QFCgI2AT2rOK0MH1qfPwnn6LFy7bW+XGYCzV+4mWj84NBzHLLYJyp0cbQkJDU9kD43J/Zrz98F/\nGTnvT23Z+Wv3ubBlBE2qf5ps+mxz4Orqqv1DB5r1PYEEaarjpTazwJs2bVhPvnz5E/0MQxw/vbx9\nLV+80FzL0mUSv5bvm12gZClNEsbKVarg5uZGp+++pU+/Adpkg87OznjmyatzTpUqVyFTpkxcuXw5\nowXWJykswlIHuKmq6uO4Y28BKgNOiqJYxfVa9Zo2muHHWN/FtVv/4eWpm4Qtp7sT9raZE4y9vsnL\n0z1BFoHrtx8R/jKSvDkTph7O6N4nswDAs2fP2Om7nVZfJL7S2fse35y8b3aBN5WIy3R769ZNbZmX\nd6FEb6qqqmqyk+/jGeGR1jtABUVR7BTNDrWBy8A+oGVcnW+BFFMtm/aVM7BDZ24kOdUKwPfoFepU\n9CZLXM8WoGW9koS/jOTQ2RtJ7nfn4VNKeOfUKfPydMfOJhO3HyadgcBcVateI8mpVqDJLLBnly9h\nYWHasnfJLPDH1v/x6tUrWrVOfBjgfY9vSqpVr5Hsmqj1fRqwe2fibU0pu8Dbjh3VpHL39MyjLWvQ\nqDGX/r3IkydPtGWHDx0kKiqKT4sVf6fjpwsD3rxSVfUEmptUZ9FMtbJA81ToEKC/oig30Ey5WpHS\nsTL0UICbk71ePcaTcav4L990hB5tqrFuWkdmrNxNnhyujOjSgLlr9ulMwXo7k8DyzUeZ2r85D5+E\n4nvkMu6uDgzrVJ9b94PYcfj1RGtbG2t8KmumtXySPSsO9jZ8Vlvz5d1x5DIRL6MM2XyDefz4MTcD\n/FOsV668ZjV6fTMLrF2ziu5dOvLvlRvk9vDQlm/asJ5PixXHu1Di6Vf0Pb4pevz4sc4K/kmJX9lf\n3+wCa1evomvn77jk54+HhwdNG/lQs3YdChcugqWlJceOHmHOrBm0bP2FdhgAoGOnLiycP5cWzZsw\neOhwwsLCGDl8CLVq10ky1YvJMPDNKwBVVX8AfnirOADNolJ6y9CB1adqkURnBbwtvhcbEhZBw24L\nmDWkJZtndSbkeQTzft3P+CXbdeq/nUlgwW8HiIyKpnPLKnRqUYlnYREcPRfAqPl/Ef7y9bzBbM4O\n/DpV9+56/LZX47HcMdHe7Y7tfyc6K+Bt8b1YfTMLxMbGEhMTo/NT9MmTJ+zft4dRY8Yl+Tn6Ht8U\n7dj2d6KzAt4W34vVN7tA/LUk7lqWLlOWNat+4fatW1hZWZEnT17GjZ9E567ddPZzdHRkx869DOjX\nm2/atiFTpkw0btIs2RTapsRUZy1IBoEMSDIIGI5kEDAMY2QQyJQ9v+reSv803vcWNk+zDAIZuscq\nhMjgTPTvPQmsQgizZapDARJYhRBmKT0yA+hLAqsQwmxJYBVCCAOTwCqEEIZmmnFVAqsQwnxJj1UI\nIQzJCE9eGYoEViGEWVLQzdBhSiSwCiHMlEy3EkIIgzPRuCqBVQhhvqTHKoQQhqRIj1UIIQxKwXRX\nH5PAKoQwW9JjFUIIA5MxViGEMCQZYxVCCMNSUEw2k6wEViGE2ZIeqxBCGJiMsQohhCHJGKsQQhiW\nZhEW04ysEliFEGbLROOqBFYhhPmSHqsQQhiYicZVCaxCCDMlGQT0U8I7F4eOzU7v0zB7rpX6pfcp\nZBjBx+X7aKokg4AQQhicZBAQQgiDM9G4KoFVCGG+pMcqhBCGJE9eCSGEYcmTV0IIYQQSWIUQwsBM\nNK5KYBVCmC/psQohhCHJzSshhDAsRR4QEEIIwzPRuCqBVQhhvixMNLKaZopDIYTQg6Lo/9LveIqT\noiibFEW5qijKFUVRKiqK4qIoyi5FUa7H/ds5peNIYBVCmCUlbtlAfV96mgPsUFXVGygOXAGGAntU\nVS0A7InbTpYEViGE2bJQ9H+lRFGUrEA1YAWAqqqRqqqGAM2AlXHVVgLNUzyv1DZICCHS2zv2WN0U\nRTn9xqvLW4fLAzwGflYU5R9FUZYrimIPuKuq+jCuTiDgntJ5yc0rIYTZesd7V09UVS2TzPtWQCmg\nl6qqJxRFmcNbP/tVVVUVRVFT+qAkA6uiKI7J7aiqamhKBxdCCGNR0MxlNaB7wD1VVU/EbW9CE1j/\nUxTlY1VVHyqK8jHwKKUDJddjvQSooHPm8dsqkDs1Zy6EEIaiz9ipvlRVDVQU5a6iKF6qqvoBtYHL\nca9vgclx/96a0rGSDKyqquYy0PkKIYThvdvdfn31AtYqipIJCAA6oLkXtUFRlI7AbaB1SgfRa4xV\nUZQ2QF5VVScqipITzWDumVSfuhBCvCcFsDRklxVQVfUckNg4bO13OU6KswIURZkP1AS+jisKBxa/\ny4cIIYQxGPoBAUPRp8daSVXVUoqi/AOgqurTuG6yEEKkK3NehCVKURQLNDesUBTFFYg16lkJIUQK\n0qMnqi99HhBYAGwGsimKMhY4DEwx6lkZmf+NG/Tq0ZXypYvjaGuFT92aKe5z+fIlmjduQH7PHLg4\n2OCd34Pvu3Ui8OFDbZ2YmBhmTp9C3VrVyP2xG7k/dqNpw/qcOX3KmM1JM02qf8rJ3wYTcnQ6V7aO\nonfbGgnqfOTqyJLRX+K/bQyPD07h2NqBtPEp/U6f07h6USJOz+bwqv4J3nO0t2HJ6C95sHcigfsn\n8fOP7XDJapfaJpmMK5cv06BebVwc7ciT+xPGjRlNTExMsvucPnWKLh07UMQ7Py6OdhQr4sWEH8fy\n8uXLBHV/Wr6MTwsXJKt9Zkp8Wojf1q4xVlPSlIWi6P1KSyn2WFVVXaUoyhmgTlxRK1VV/zXuaRnX\nlcuX2Om7nbLlKhAVFaXXPqHPnuHh6clX7b7mo48/4fatm0yaMI5/zp7l4NGTWFlZERERwcxpU2j3\nTXsGDhqKoigsWbSAujWrsufAEUqWercAY0oqFs/DumkdWPnHCYbN3krZoh6M79WE2FiV+b8dADQ/\nyzbN7IRLVjtGzP2TwKBQPqtdgp/Hf03Eqyi27ruQ4udkzmTF1H7NCXyS+DTpNZPbUyB3NnqMX0ds\nrMr4Xk3YML0jdTrPM2h701JwcDANfepQqFBhNm7ZSoC/P0MHDyA2NpYx48Ynud+mjesJCPBnwMAh\n5C9QgIsXLjBuzCguXrzAug2btfXWr/uNnj260n/gYGrUrIXvju107PAN9lmy0LRZik9nmjQT7bDq\n/eSVJRCFZjjA7B+Dbdi4CY2bNgOgbZtWBAU9SXGfChUrUaFipdcF1WuQI0dOmjaqz78XL1CiZCls\nbW25eNUfZ+fXi9/UqFWbEkW9WLJoAYuX/WTwtqSVYZ3qc+z8TXqMXw/AnhN+ODnYMqxTPZZsPExU\ndAwFcmejdJHctOi3jG2HLgGw/9R1yhbNTcu6JfUKrP2+rsWDx88IuBdE4Xwf6bxX/lNP6lb0pk7n\nuRz5JwCAB4+fcWhlf2qWK8i+k9cM3Oq0sXzpYl5GRLBu4xYcHR2pXacuoWGhTBg3hv4DB+PomPiz\nOgMHD8XNzU27Xa16DWxsbOjZoyu3b9/Gw8MDgAk/jqHNl20ZP3EyAHXq1uPu3TuM/WGk+QdWEx0L\n0GdWwAjgN+ATICfwq6Iow4x9YsZkYWGYvxtcXF0BiIyMBMDS0lInqAJkypSJQoWK8PDBA4N8Znop\nVjAHe07oBq7dx/1wyWpP+WKeAFhbWQLw7HmETr1nYRF6jYXlcnei/ze1GDj9f4m+X69SIQKfhGqD\nKsDpS3e4ee8J9SsVeofWmBbfHdupU6++TgBt1boNERERHDp4IMn93gyq8YqXKAmg/b6Fh4dz4/p1\natepq1OvTp16XL50idu3bxuiCelCwbCLsBiSPhHmG6CsqqojVVUdAZQD2hv1rExYbGwskZGRXPPz\nY/SIYZQuU5YyZcslWf/Vq1ecO3eW/AUKpOFZGp5NZiuioqJ1yiLjtr3zaNakuOT/kJMXbzG6W0Py\n5XLDwT4z7RqXo2LxvCzbfDTFz5jcrzmbd5/jnN+9RN/38szOtdsJnya8eus/Cnpmf9cmmYxrflfx\n8vLWKcudOzd2dnb4+V19p2OdOH4MCwsL8ubLB2i+f6qqYp1JdyJPprhtv6tX3uPM09k7LMCS1j1b\nfQLrQ3SHDKziyj5InzdthIuDDaWKFSI4+Ckb//dnsj3gqZMnEPz0KV2790zDszQ8/7tPKF1E9ynm\nskU1PzWdHV/fPGrWewkWisK//xvJowNTWDCiNd3G/caB09eTPX71MgWoXcGLHxb8lWQdJ0c7noVF\nJCgPCY3A2cF8b2AFBweTNatTgnInZ2dCgoP1Pk5gYCBTJo3nq7Zfkz275i8aZ2dnXFxcEtxAPX3q\nJABPnz59jzNPf2Y3j1VRlFloxlSfApcURfGN264HZIzb3KkwfdZcgoOf4n/jOlMmTeDzpg3Zvf8w\nNjY2Ceru2PY30yZPZNKU6RT08kqHszWc5ZuPMG9Yazo0r8D/9pynTBEPen1VAwA1VrPYj6IoLB/b\nFpesdrQb+guPgp/jU7kQi0a1IejZC3YdS7z3ZWlpwYxBnzP1p108evo8rZqUoURGRtLuy9bYZ8nC\n1BmzdN7r1KUbC+bNoWKlylSvUZOdvjv4de1qwHDDYunFVMdYk7t5FX/n/xLw9xvlx413OqYv/id9\n2XLlqVS5KkW88rJh3a98077Ws8MAACAASURBVP47nXpnTp/i23Zt6Ni5K9/37psep2pQK/84QbGC\nOZg7tBULR7bhRcQrRs77k1mDWxIYFAZAw6qFaVStKEU/G4//Xc0NwUNnbpDT3ZkJvZsmGVi/a16R\nrPY2rP7zJFmz2AKQydoSS0sLsmax5UXEK6JjYgkJDcfNOUuC/Z0cbQkOCzdSy43P2dmZ0NBnCcpD\ngoNxck4xCwiqqtKxwzdcuXyJvQeOJBjnHzJsBDduXOfL1i0AcHFxYcToMQwfMoiPPvoosUOahfgx\nVlOU3CIsK9LyRMxRbg8PnF1cuHkzQKf8+rVrtGjemBo1azN91tx0OjvDio1V6Td1M2MXbSNHdidu\nPQjCy1Mztnry4i0AvDzdeRHxShtU4533u0ejakWSPHZBj+zk/MiZO7sSTi0K3D+JDqNWs277Gfxu\nPaJSybwJ6nh5uvPn/ovv0br0VdDLO8FY6t27dwkPD08w9pqYgf378tcfW/lr+y68vBPWt7OzY+1v\nG/hv9n88efyYfPnzs+3vv8iUKRMlSpYyWDvSgzn2WAFQFCUfMAEoDGh/76qqWtCI52UWrvn58TQo\nCE/PPNqywIcPad7Yh7x58/Hz6l+xtLRMxzM0vJCwCELixjm7tKrCsfMB2htKdx4GY2+bmQIe2bn+\nxk2mkt65uP0g6bG8RRsO8ccB3cA48NvaeOZwpefEDfjdDARg59ErDO9cn0rF83D0/E0AShXKRd6c\nbvgeNd+bMPV9GjBrxjTCwsJwcHAANHNUbW1tqVqterL7TpsyicUL57Pmtw1UrlIl2bru7u64u7sT\nGxvL8qWL+ezzlklO5TIXphlW9ZvH+gswHpgONECzjFaKK2ibsvDwcHx3bAPg4YP7hIaG8r8tmwCo\n79MQOzs7ihUqQJVq1Vi4RNNxHz5kIFZWVpQpV56sWZ3wu3qF2TOnkTdvPlq2bgNAREQEnzVtSEhI\nMDNmz+Pfi6/nbWbOnFk7FcYclSvqQaUSeTl/7T6O9ja0rl+KOhW8qd1pjrbOjiOXufPwKRumd2TS\ncl8eBz+nQZXCtKxXkj6TN2rrfdWoLEtGtaFI8/HcCQwm4N4TAu7p9nK/blwOVyd7Dp25oS07cfEW\nu45dZfnYtgyb84fmAYHeTTjyj7/ZzmEFzRjowvlzadPqcwYMGsLNgAAmjBtD7779dQJfEe/8VK1a\nncXLNN/Jdb/9yuiRw/n6m/Z88kkOThx/PUqXN18+smXLBsC2v//izu3beBcqxKNHj/h5xTL8/K6y\n7KeVmDNFMd301/oEVjtVVX0VRZmuqqo/MFJRlNPAKCOfm9E8fvSIr7/UXVIxfvuSXwAenp5Ex0QT\nE/N6SYSSpcuwZOF8fl6xjJcvX5IzV26aNf+cAYOHYW9vD8Cj//7j4oXzALT8rInO8XN7eHD52k1j\nNsuooqJjaFm3JCO6+BAbq3LknD+1Os7hkv/rCSLPw1/RoPtCfuzZmMl9m+Fgb0PAvSf0nLiBFVte\nT7eyUBSsrCxTdav262Ermdq/OYtHf4mForD98CUGTNtikDamF2dnZ7b57qFfn560aN4EJycnevXp\nx8jRY3TqRUdH6zzmumfXTgBWr/qF1at+0am7dPnPfP1tewCsrKxYsWwJ/v43sLGxoU69+ixZ/jM5\ncuQwZrPShInGVRRVTb7zqSjKUaAKsAXYAdwHpquqavDb3KVKl1EPHftgJxwYjFulful9ChlG8PHZ\n6X0KGULl8mU4c+a0QcNgtnxF1M8mb9C7/rLWRc+kkPPKYPSZa9EPsAd6A5WBzsB3ye4BKIryk6Io\njxRFMet1BYQQpsvs5rHGeyOxVhivF7vWxy/AfGDVu5+WEEIkTyHtV63SV3IPCPyPZG5Sqar6eXIH\nVlX1oKIonqk+MyGESI4Jr8eaXI91flqcgKIoXYAuALlyS+JXIYT+zG4eq6qqe9LiBFRVXQosBc3N\nq7T4TCFExmCqD+Tqux6rEEKYFAXT7bGaasBPF1euXKZR/Tpkc7Inv2cOfhybcnqMNz24fx93Fwey\nZLbg+fOEi4n8ufV3ypcujouDDUUK5mXe7JmGPH2T4p3HnW0LexB0eCoB28cyqmsDLFJ4sNt3SU8i\nTs9O9FX+U0+duq3qleTomgE8PjgF/21jWD62LR+7mfdTRElJTdqWN92/fx83pyzYWiuJfi/ftZ4p\nMdX1WPXusSqKkllV1VfvUP83oAbgpijKPeAHU15/IDg4mCYN6uLtXZj1m34nIMCf4UMGEhsbyw9j\nk06P8aYRwwZjnyULL168SPDesaNH+OqLFnzTvgMTJ0/j1KkTjBoxFAsLiwyxSMubnBxs2bawB1cC\nAmk1YDl5c7oxuW8zLCwUxi7aluR+fSZvxNFed5WwUd0aUNwrJ6cv39GWNapWhFUTv2XxhkMMn/MH\nH7k5MqZ7I7bM6UKldjNIaW62OUlt2pY3DR86iCxJfC9TU8+UmN0iLPEURSkHrACyArkVRSkOdFJV\ntVdy+6mq+qVhTtEwDh7YT8N6tXj+KvEEsyuWadJj/LphM46OjtSiLmGhoUwcP5Z+A5JOjxHv8KGD\n7N65g4GDhzFi2OAE70+e8CMVKlVmweLlANSuW49nISFMnvgjnbv10C48bA6qls7PziU9sS2T+F8I\nnVpUxiazNW0G/0TYi1fsPXENR3sbRnTxYeaqPYS9SPzv56s3/9PZtraypFShXGzadU7nKbgvfEpz\n9spd+k19ndcp7MVLNs3sTEGP7Pjd0j2OKTt4YD/169QkIirxvwxSm7Yl3uFDB9nlu4NBQ4czfMig\n965nSjTzU00zsuozFDAXaAwEAaiqeh5IOa2pmdnpu4PadXXTY7SMS49xOJn0GKDJzjqwX2+GDh+F\nayLpMgAuXDhHrdp1dMpq16lHcHAwJ44fe/8GmJD6lQqx+/hVnQC60fcsdjaZqFoqv97HqVfJG5es\n9mzwPaNTbm1lSehb6V/iF4Yx0T9nqZbatC2g+V7279OLYSNG4+aa+PfyXeqZIlMdCtAnsFqoqvp2\nYhz9B3jSiaqqREdHa1/xY1JvlkVHv041cs3vaoLFqHPFpce4lkJ6jOVLF/Pq1Su6dP8+yTqvXr5M\n0Cu1NqP0GJaWFq9fcd9SnTLL11+lgp7Z8bulm0Ll7n8hvIh4hdc7pFBpVa8U9wKDdXJcAazceoLK\nJfPxVaOyONhnJn/ubIzp3oh9J68l6PWamtR8L1ObtmXZksW8inxFtx5Jfy/fpZ4pMtsnr4C7ccMB\nqqIolkAvwOSXElq7eiXdOid88tbJXje4xQ8NhAQH45RUeoyQpNNjBAUFMX7saJb/vBpra+sk6+XN\nl58zp0/rlJ05rUmPERxs2ukx2jUux7IxXyUof35C9+Zb/NCAczIpVJz0TKFim9maRtWK6izeEm/H\nkct0GfMri0a1YcXYtgAcOx9Ay/4/63Xs9LRm1Uq6dOqQoNzBVve7Ez80kNq0LUFBQYwbM4qfVq5J\n9nupbz1TpFno2jR/ougTWLujGQ7IDfwH7I4rM2kNGjXh4NGT2u1/zp6hT8/uOmWGMHb0CMqWq0D9\nBg2Trdexc1f69OzOzyuW0fzzlpw5dZJ5czQpNEw9Pca2Q/9S+esZ2u2ShXIxf3hrnTJDa1StKFns\nMrPB92yC96qVzs/cYa1YsO4gvkev4O7iwIguPqyf/h0NeywkNtZ0b141bNyEw28sNPTP2TP0+r6b\nTpkhjBk1gnLlK+CTwvdS33qmylT/5OizVsAjoE0anItBubq64hqXnhrQTh8pVTrxxW2cnJ15llR6\nDKfE02NcvnyJ1St/xnfPAUJCQgDNWq8Aoc+eYWlpia2tJtXIN+2/4+KF8/Tt1YNePbpiZ2fHuAmT\nGdivN+7upp0e4+mzcJ4+e536xN4uMwBnr9xNtH5waDiOWRLmAHNytCVEzxQqreqV5Madx4l+xuR+\nzfj74L+MnPentuz8tftc2DycJtU/Zeu+Cwn2MRVvfy9fvNB8L0uXSfx7mZq0LZcvXWLlLz+xa+/B\nBN/LZ298L/WtZ6oURdEOS5kafWYFLCORNQNUVe1ilDNKJwW9vLnm56dTdi8uPUbBJNJj+F+/TlRU\nFLWqVUp4vLy5+LbDd9pZAJaWlsycM59RY37k/v17eHrm0Y7dli1XwcCtSV/Xbj3Spm2Jl9PdCXvb\nzAnGXhPjaG9DvUqFmLlqb6Lve3m6s/Gtnuz1248IfxlJ3pyuie5jrlKTtuXGDc33skbVigney++Z\nk/YdOrJo6XK965kyEx0J0GsoYPcb/20DfAYk3lUxYdWq10hyqhVAvfo+zJk5XSc9xua49BhVkkiP\nUbFyFbbt1P3Dv3vnDmZOn8qWrX/jmSdhfiZnZ2dtsrdlSxZSoWKlRPMUmbJDZ24kOdUKwPfoFfp9\nXZMsdpl5Hq6ZGdCybknCX0Zy6OyNJPeL17RmMWwyWyc6DACaFDAlvHPplHl5umNnkynZFDCmqFr1\nGklOtYLUpW2pVLkKvrv36ZTt9N3BjGlT+P3PbeSJ+17qW8+UmWiHVa+hgPVvbiuKsho4bLQzMpDH\njx9zM8A/xXrlymt6ix07d2PRgnl81boF/QcO5ubNACaOH0vPPv10prr8umYV3bt05OKVG+T28KBa\n9Ro6x7tz+xYAlapUJUuW1xlFT544zrEjhylWvAShYaFsXL+OPbt82bXv0Ps31sjcnOzJmzPlaTgn\n/9VMHlm++Qg92lRl3bTvmLFyD3lyuDKiiw9z1+7XmYL1doqWeK3qleS83/0k56Mu33yEqf2b8/Dx\nM+0Y67DO9bl1P4gdR0x7hsXjx48J8E/5e1m+guZ7qW/alrWrV9G183dc8vPHI5Hv5e1btwCo/Mb3\n0s3NTa96psrcb169LQ/gnmKtdOa7/e9EZwW8Lb4X6+zszF87djOgby9afd6UrE5OfN+7LyNGjdGp\nHxsbS0xMzDs/3WNtbc3mTRuYOH4sFhYWVKpclV37D1O06KfvdJz04FOlSKKzAt4W34sNCYugYfeF\nzBrcgs0zOxHyPIJ5v+5n/NIdOvUTS9HimtWemuUKMi6ZJ7QWrDtIZFQMnVtWplOLSjwLi+DouZuM\nWvAX4S8jU9fINLJj29+Jzgp4W3wvVt+0LfHfSzLQU2f6MNG4qldqlmBej7FaAE+Boaqq6p8TQU+S\nmsUwJDWL4UhqFsMwRmqWHF6fqj0W/k/v+iPrFEiz1CzJ9lgVzfNixdHkuQKIVTPSg9hCCLOmmGgC\n7GSngcUF0W2qqsbEvSSoCiFMgmaM1XwfaT2nKEpJo5+JEEK8I1MNrMnlvLJSVTUaKAmcUhTFH3iB\n5i8KVVXVUml0jkIIkShTXd0quTHWk0ApoGkanYsQQugtfijAFCUXWBUAVVVTnnQnhBBpzUyztGZT\nFKV/Um+qqppx84oIIcyCMR4QiFvF7zRwX1XVxoqi5AHWAa7AGeBrVVWTnTCd3M0rSyAL4JDESwgh\n0o0RZwX0Ad58hG8KMEtV1fxAMNAxpQMk12N9qKrquHc6HSGESEOG7rAqipITaARMAPrHzeWvBcQ/\nergSGAMsSu44KY6xCiGEaVKweLcw5aYoypurzS9VVXXpW3VmA4N5/avcFQiJmyEFcA/IkdIHJRdY\na+t5skIIkeYU3rnH+iS5R1oVRWkMPFJV9YyiKDXe59ySDKyqqprX+mtCiA+L4Sf+VwaaKorSEM0S\nqY7AHMDpjXn9OXn9iH+STDWzgRBCpMhCUfR+pURV1WGqquZUVdUTTdaUvaqqtgX2AS3jqn0LbE3x\nvFLfJCGESD/xQwFpkKV1CJobWTfQjLmuSGmH1KzHKoQQJsFYC12rqrof2B/33wFAuXfZXwKrEMJs\nmeOTV0IIYbIUTHcsUwKrEMI8Kea5upUQQpg00wyrEliFEGYqo2VpFUIIk2CaYVUCqxDCjJloh1UC\nqxDCXCly80oIIQxJplsJIYQRyM0rPSiApalmBzMjwcdnp/cpZBjOZXum9ylkCK/87hj+oDKPVQgh\nDEuGAoQQwgikxyqEEAZmmmFVAqsQwoyZaIdVAqsQwjxpxlhNM7JKYBVCmC3psQohhEEpKNJjFUII\nw5IeqxBCGJCMsQohhKG9f/ZVo5HAKoQwWxJYhRDCwOTmlRBCGJAmNUt6n0XiJLAKIcyW9FiFEMLA\nZIxVCCEMTHqsQghhQDLGKoQQBiePtAohhGHJAwJCCGF4JhpXJbAKIcyTZozVNEOrBFYhhNkyzbAq\ngVUIYc5MNLJKYBVCmC2ZFSCEEAZmokOsWKT3CaSHK5cv06BebVwc7ciT+xPGjRlNTExMsvtERkYy\nbMggateoirODLbbWSf8fDQoKomf3rnjm/AhnB1uKF/Vm7epVhm6GSUjNtQR49uwZXTp24ONszri7\nZqX9120JCgrSqRMZGcnE8eMo4p0fZwdbinjn58exP/Dq1StjNSfNNKlRjJPrhxFyYhZX/hpD73a1\nEtTJmsWWxT+05f7+KTw+MoPf53cnby63FI8d8c/8RF8hJ2YlqNusVnEOrxnE02MzubdvClvn98DO\nJpNB2pgWlHd4paUPrscaHBxMQ586FCpUmI1bthLg78/QwQOIjY1lzLjxSe4XHh7OLz8tp0zZclSo\nWIn9+/YmWi80NJS6taphb5+FGbPn4ebmxpXLl4mMjDRWk9JNaq8lQLsvW3P9+jUWLlmOhYUFI4cP\noXWL5uzZf0hbZ+TwoSxfupgfxo6nRMmS/PPPWcaOHklISAgzZs0xdvOMpmLxvKyb0YmVW48zbNb/\nKPupJ+N7NyM2Npb5v+7X1ls95TuK5P+YgdM2Efr8JUM71Wf7kt6UaTWRsBcvkzx+9W+mJyjbNKcr\nx88F6JS1/6wis4a0ZubK3Qyb9TvOjrbUKOeFlZUZ9bdMtMf6wQXW5UsX8zIignUbt+Do6EjtOnUJ\nDQtlwrgx9B84GEdHx0T3c3Jy4sGjpyiKwqIF85MMrFMnT+TVq1ccOX4aW1tbAKrXqGm09qSn1F7L\n48eOsXvXTnbtPUCVqtUA+OSTHFSrXJ69e3ZTq3YdADas+5XOXbvTp19/QHMdH9y/z/rf1pp1YB3W\npQHHzgXQY9yvAOw5fhUnB1uGdWnAkg2HiIqOoXyxPNStVIgGXeey/+Q1AE5dvMWVv8bS8fPKzF69\nJ8njn7x4S2e7dOHcZHN2YMOOM9oyVyd7pg5oQf8pG/n5f0e15X/su2DAlhqXpidqmpHVjP5qMgzf\nHdupU6++zh/6Vq3bEBERwaGDB5LdV9FjQGf1yp9p36GjNqhmZKm9ljt9t+Pu7q4NqgBly5XDM08e\nfHds15ZFRUWRNWtWnX2dnJxQVdWArUh7xbxysOfEVZ2y3ceu4pLVnvLF82jrREZFc/D0dW2dR0/D\nuHj9Pj5Vi7zT57VuUIbn4a/4++BFbVmLuqUAWPPnidQ2I/3FPXml7ystfXCB9ZrfVby8vHXKcufO\njZ2dHX5+V5PYSz+3bt7k0aNHZM3qRPMmDXG0y0Suj7MxeGD/DDkUkNpr6ed3lYJv7Qfg7V2Ia2/s\n1/67TixftoSjR47w/PlzDh8+xLIli+jWo6fhGpEObDJZExWlOw4dGRUNgHeej7R1YmJiiY1VE9Tz\niqujrxZ1S/LX/gtEvIzSlpX91JNrtx/R/rOK3NjxI6En53Bw1UAqxAV2c2HIwKooSi5FUfYpinJZ\nUZRLiqL0iSt3URRll6Io1+P+7ZzSsT64wBocHEzWrE4Jyp2cnQkJDn6vYwcGBgIwYthgPvkkB3/8\nvYNBQ4azbMkixowe+V7HNkWpvZYhwcFkdUpkPydngt/Yb/zEyTT/rAW1a1Qhm7MDdWtWo1nzzxk+\ncrRhGpBO/O8+pnQRD52yskU1286Odto6tjaZKJL/E20dm8zWFM73CS5Z7fT+rMql8pHD3ZmNvmd0\nyt1dHSnokZ2hnXwYOWcrLfou5kXEK7bO70F2F4fUNi2NKe/0jx6igQGqqhYGKgDfK4pSGBgK7FFV\ntQCwJ247WR9cYDWm+J+ohQoXYeGSZdSoWYveffsxaMgwFs6fS3h4eDqfoXmZOWMa635dw8zZ89i1\n9wAzZs1l3W9rGTfGvAPr8k2HaVKjGB0+q4STgy11KhaiV9ysgPjv0K6jV7h57wnzR7ahgEd2PnJz\nZN6INmTNYpOgF5uc1j5lePrsBbuOXtEpVxRwsLeh29i1rNt+ml1Hr9C6/zJiYlW6tamWxNFMjyF7\nrKqqPlRV9Wzcf4cBV4AcQDNgZVy1lUDzlI71wQVWZ2dnQkOfJSgPCQ7GyTnFHn6Kx4aEN6uq16jF\nq1evCPD3f6/jm5rUXksnZ2dCnyWyX0iw9ho+efKEsaNHMn7iFLp/35MqVavRo2cvxk+cwrQpk3j0\n6JHhGpLGVm49xrKNh5g7/AseHpzGuhmdmLxsBwCBT0IBiIqO4ZuhP5PdxYELv4/m5q6J5Mnhytq/\nTvJfUKhen2NpaUHz2iX4fc85oqJ1hx5CQsOJjY3VGcMNe/GSf67coVDejw3UUuN6l6lW7zrEqiiK\nJ1ASOAG4q6r6MO6tQMA9pf0/uFkBBb28E4z/3b17l/Dw8ATjhe8qb758ZMqUKcHNFRXNtoVFxvp7\nLLXX0svLm6OHDyUo9/O7SpOmms7AzYAAoqKiKFa8hE6d4iVKEh0dzZ3bt8mePbsBWpH2YmNV+k3Z\nyNiFf5HD3Zlb959ox03fvKN/+tJtijQdSwGP7ETHxHLz3hM2z+mW4K5/UmqW8yK7i+5sgHhXb/6H\nhYVFghuyCgqxsbGpbluae7eI6aYoyuk3tpeqqro0wSEVJQuwGeirqmrom9dIVVVVUZQUfzJkrD/p\neqjv04DdO30JCwvTlm3auB5bW1uqVqv+XsfOlCkTtevU5cD+fTrl+/fuwc7Ojnz587/X8U1Naq9l\nvfoNCAwM5Mjhw9qyM6dPczMggPo+DQDI7aEZczz3z1mdff85qwkSHp6ehmpGugkJi+DSjQe8iIik\nS6uqHDvnz7Vb/yWod/32I27ee0K+3NmoVd6LX/53TK/jt/YpzcPHz3R6pfG2H/oXgOplC2jLHLPY\nULJwLi5cu5/KFqW9dxxjfaKqapk3XokFVWs0QXWtqqpb4or/UxTl47j3PwZS/Ln0wfVYO3XpxsL5\nc2nT6nMGDBrCzYAAJowbQ+++/XWmDRXxzk/VqtVZvGyFtsx3x3ZevHjBhfPnANiyeRMApcuUxSMu\nEAwbMZraNarQpWMHWrf5kosXLzB96mSGjRhF5syZ07Clxpfaa1mhYkXq1K1Hp+++YdKU6doHBCpV\nrqKdw+ru7k6TZs0ZOXwIL1++5NNixTh/7hwTfhzD5y1bkS1btnRpsyGU+9STSiXycf7aPRztbWjt\nU5o6FQtR+zvdJ6OGdvbh2s3/eBLynKIFPmFoZx82+p5h7xtTtb5qXI4lP7SlSNMx3Hn4+sZfJmsr\nmtQsxpo/TiQ6Pe3s5Tv8ue88i0a3ZdS8PwgKfk7/9nWIio5hyfqDxmu8gRlyGpWi6ZquAK6oqjrz\njbf+AL4FJsf9e2tKx/rgAquzszPbfPfQr09PWjRvgpOTE7369GPk6DE69aKjoxM8mtm7Z3fu3L6t\n3W7bphUAS5f/zNfftgc08zE3//4no0cMY/26X8mWPTtDho1g0JBhRm1Xenifa7n61/UMHtCPbp2/\nIzY2lgaNGjNj1lydOst/WsnE8eNYuGAuDx884JMcOejYuSvDRowydtOMKio6hpb1SzGiW0NiY2M5\n8o8/tTrM4tKNBzr1XLPaM21QC1yd7LkXGMKcVXuYvVr3wRQLRcHKypK3fxPXr1wYJwe7BLMB3tRh\nxEom9vuMKf0/x87GmmPnA2jQZR4hYREGa6uxGXh6amXga+Cioijn4sqGowmoGxRF6QjcBlqneF6m\nNNm6dOky6pETp1OuKEQacS5r3nNmTcUrvw3Ehj8yaBwsUryUun6b/r3rT3M6nFFVtYwhzyEpRhtj\nTWqyrRBCGIqB57EajDGHAuIn255VFMUBOKMoyi5VVS8b8TOFEB8IhQ9w2cBkJtsKIYRBmOqygWky\n3eqtybZvv9dFUZTTiqKcfvzkcVqcjhAiozDRyGr0wPr2ZNu331dVdWn8vLJsbuY7hUYIkfY+xDHW\npCbbCiGEQXxwY6zJTLY1aalJNbJl8yZqVK1EDndXnLLYUKyIF5Mnjk+wVODmTRtp+VlT8nrkwM0p\nC5XKlWb9ut+M2Zx0k5rrmJrrc//+fdycsmBrrfD8+XNDNsFkeOf9iG2LexF0dCYBOycwqnsjLCyS\njyif1SnBvl/6c2/fFIKPz+L8/0YxpFN9rK0sder5LuuTZCqX8sVMfwlBEx0JMGqPNdHJtqqqbjPi\nZ76X1KYaeRoURI2ateg3YBBOTk6cOnWSCePGEBgYyOy587X15s6eiWeePEydPgs3Nzd2bN9G+6+/\nIujJE3r07JUWTUwTqb2Oqbk+w4cOIkuWLLx48cJYzUlXTg62bFvciysBD2nVbyl5c7kxuf9nWCgK\nYxf+leR+Llnt2X/yGrNW7iYkLIKyRT0Y0bUhH7k60m/KRm29PpPW42hvo7PvqO6NKO6dk9OXbr99\nWNNjoj1WowVWVVUPY2LNPnhgP/Xr1CQiKvGHIlKbaqRTl64629Vr1CQsNJQlixYwa8487UIXm3//\nEze318ngatSsxcOHD5g7Z6ZZBVZjXcd3vT6HDx1kl+8OBg0dzvAhgwzTuDRWtXQBdi7vg23JxB9E\n6NSqKjaZrWkzYDlhL16y9wQ42tswomtDZq7cnWTuqxWbj+hsHzx9HQd7W7p+UVUnsF4NCNSpZ21l\nSanCudm08ywxMaa9GIukZjET75O25W0uLq4JhgLeDBrxipcoycMHDxKUm7PUXsd3uT4xMTH079OL\nYSNG4+aacuZSc1W/cmF2H7uiE0A3+p7BzjYTVUu/26I+T5+9IJNV8n2pepUL45LVng07zOAJSEnN\nkj5UVSU6Olr7ih/jZuIsiQAAC69JREFUe7MsOjpaW/9907bExMQQHh7OkcOHWbhgLp27dk8xT9aJ\n48coUKBgKlqXdtL6Or4pqeuzbMliXkW+oluP71PRovRlaWnxxktJpOz1H8uCnu743dRd8epuYDAv\nIl7h5ZnisqBYWCjY2lhTqUReenxZnWWbEi7X+KZW9UtzLzCYI2fNY+3gD3GMNd2tWbWSLp06JCh3\nsLXW2Y7/Sfu+aVtcs9prc963bfcNk6ZMS7b+vr17+HPr7yxZ9lOKx05PaX0d4yV1fYKCghg3ZhQ/\nrVyDtbV1EnubpnZNyrNs3NcJyp+f1l2AJn5owNnBjmdhCTNPhISG4+SYcoqWoKMzscmsuUZr/jzB\nsFm/J1nX1saaRtU/ZcXmw0nWMTmmORKQsQNrw8ZNOHzslHb7n7Nn6PV9N50yQ9p38Cjh4eGcPnWS\nSRPG0a93T+bMX5ho3du3btH+669o3LSZdmUsU5XW1xGSvz5jRo2gXPkK+DRoaLTPN5ZtB/+lctup\n2u2ShXIxf+SXOmWGVLP9DOxsMlGmqCfDuvgwa2gr+k7akGjdRtU+JYtd5kQXxjZNaT8/VV8ZOrC6\nurri6uqq3X7xQjMdp3SZxBe4ed+0LSVLaVIKV65SBTc3Nzp99y19+g0gb758OvWePn1KsyYNyJXb\ng19WrdW7Peklra9jctfn8qVLrPzlJ3btPUhISAiANpfYs2fPsLS0NOnU40+fveDps9czGOxtNWv0\nnr18J9H6wWHhOGZJ2B4nRztCQlPOoXbu6j0Ajp4L4EnIc1b8+A1zVu/l5r0nCeq2ql+aG3ceJXku\npuiDm8dqjgyZtqVESU2QvXXrpk55eHg4nzdrTGRkJFu2/oWdnf4ZN83F+1zHlK7PjRvXiYqKokbV\ninyczZmPsznTt7dmnDW/Z0769zGf2RX6uHbrP7zy6I6l5nR3wt42M36JZBtIzrkrdwHwzOGa4D3H\nLDbUq1zYjHqrxs159b4ydI/1bdWq10hyihBoUo3MmjGNsLAwHBw0KYBTm7bl2FHNdBdPz9eTrKOj\no2nbphX+N66z7+BRs83ZZKzrqM/1qVS5Cr67dVPf7PTdwYxpU/j9z23kyZM3la1KH4fOXE9yqhWA\n75HL9PumNlnsMvM8XDN+37JeacIjIjl05sY7fVbFEpprc+t+UIL3mtYsjk1ma/OYDfAmE+2xZujA\n+vjxY70yo5avUAHQP9XI2tWr6Nr5Oy75+ePh4UHTRj7UrF2HwoWLYGlpybGjR5gzawYtW3+hMwzQ\np2cPdmzfxvSZcwgKCiIo6PUXvETJkiabuiWtrqM+18fNzY1q1WvofO7tW7cAqFylKlmyZHn/BhuR\nm3MW8uZMeXpYfMLA5RsP0aNNddbN6MyMX3aRJ4cbI7o1ZO6avTpTsN5O0bJ1fg/2nfDjcsBDYmJi\nqVgiL32+rs1G3zNJDgOc97uXYAaCqZMx1nSwY9vfid7Nflt870vfVCOxsbGaKUdx2RdKlynLmlW/\ncPvWLaysrMiTJy/jxk+ic9duOvvt3r0TgIH9E675ffX6TZNNkJdW19Fcr8+78KlSJNFZAW+L78WG\nhEXQsNs8Zg1pxebZXQkJi2De2r2MX6z7AOPbKVrOXL5Nu6bl8fjEleiYGG7eC2L0vD8SnW7l6mRP\nzXJejFuU9JNcpspUx1glNYsQyZDULIZhjNQsxUqUVv/ee1Tv+rldbdIsNUuG7rEKITKwdHiiSl8S\nWIUQZsw0I6sEViGEWVKAFFZPTDcSWIUQZkuGAoQQwsBkupUQQhiaacZVCaxCCPNlonFVAqsQwjyl\nxwLW+pLAKoQwWzLGKoQQhmaacVUCqxDCfJloXJXAKoQwXzLGKoQQBiWpWYQQwqAUTLfHKqlZhBDC\nwKTHKoQwW6baY5XAKoQwWzLGKoQQhiRPXgkhhGGlR1prfUlgFUKYLxONrBJYhRBmS8ZYhRDCwGSM\nVQghDMxE46oEViGEGTPRyCqBVQhhtv7f3t2H6lnXcRx/fzouN918qHzAEKfOaSU5NhdmKZp6MDUZ\nQymzh+HQuUiUUkxULJA0Bv0hEqUFQ3SSoiNRfKI/csrUjbWtWXNSouXj5mObW+bp4x/X76zb487j\nLrjua+fzgpv74nr4/b7Xj/t8+fE79/W9u3WNVbabjmE7SRuBF5qOYxifATY1HcQuIONYnzaM5SG2\n96uzQUkPUd37SG2yfXqdMQymqxJrG0haafvYpuNou4xjfTKW3SdFWCIiapbEGhFRsyTW0bul6QB2\nERnH+mQsu0zWWCMiapYZa0REzZJYIyJqlsQaEVGzJNZhSDpS0pclTZDU03Q8bZcxrIekaZKOlbR7\n07HEx+WfV0OQNBf4OfBSea0EFtt+t9HAWkjSdNsbynaP7b6mY2orSWdRfS7fAF4Frusf2+gOmbEO\nQtIE4JvAfNunAH8ADgaulLRXo8G1TEkEqyUtAbDdl5nr2Eg6HlgEfN/2ycBbwE+ajSoGSmId2l7A\nEWV7KXA/MAH4ttStlSC7i6Q9gR8ClwHvS7odklx30i9s/7lsXwd8KksC3SWJdRC2/wv8Epgr6QTb\n/wMeB1YDX200uBaxvQW4AFgCXA5M7EyuTcbWUk8B98L29erdgUOoJgFI+nRzoUW/JNahLQMeAb4r\n6UTbfbaXAAcBxzQbWnvYftn2ZtubgAXApP7kKmmmpKOajbA9ymewf41fwNvAm7Y3SjofuF7SpOYi\nDEg91iHZ3ibpDsDAVSUB/Ac4AHil0eBayvYbkhYAiyStB3qAkxsOq5VsfwBslvRPSTcAvcA821sb\nDm3cS2Idhu23JN0K/JVqtrUN+I7t15qNrL1sb5K0Fvg6cJrtfzUdUxuVdf4JwAnl/RTbzzUbVUC+\nbjUqZU3LZb01xkjSvsBdwI9tr206nraTNA9YYfuZpmOJShJrNELSRNvbmo5jVyBJzh9yV0lijYio\nWb4VEBFRsyTWiIiaJbFGRNQsiTUiomZJrOOEpD5JqyWtk3S3pD12oq2TJN1fts+WNGgREEn7SPrB\nGPr4qaTLR7p/wDmLJZ0zir6mSlo32hgjBpPEOn5stT3D9tHA+8DFnQdVGfXnwfZ9tm8c4pR9gFEn\n1og2S2Idn5YB08pM7VlJtwHrgIMl9UpaLmlVmdlOBpB0uqT1klYBc/sbkjRP0s1l+wBJSyWtKa/j\ngRuBw8tseVE57wpJKyStlfSzjraulrRB0uPAkcPdhKQLSztrJN0zYBZ+qqSVpb2zyvk9khZ19L1g\nZwcyYkeSWMcZSbtRPUr6l7LrCOBXtr8AbAGuAU61PZOqsPePJE0EbgW+AcwCDhyk+ZuAP9k+BpgJ\nPENVK/TvZbZ8haTe0ueXgBnALEknSpoFfKvsOwOYPYLbudf27NLf34D5Hcemlj7OBH5d7mE+8I7t\n2aX9CyUdOoJ+IkYltQLGj0mSVpftZcDvqKp0vWD7ybL/OODzwBOl3OwngeXAUcDz/c+hl8pUF+2g\nj68B34PtJQHfKY+vduotr/56opOpEu0UYKnt90of943gno6WdD3VcsNk4OGOY3eVR4+fk/SPcg+9\nwBc71l/3Ln2n+n7UKol1/Nhqe0bnjpI8t3TuAh61fd6A8z5y3U4ScIPt3wzo47IxtLUYmGN7TXle\n/qSOYwMfKXTp+xLbnQkYSVPH0HfEoLIUEJ2eBL4iaRpU1f8lTQfWA1MlHV7OO2+Q6/8ILCzX9kja\nG/g31Wy038PABR1rt5+VtD/wGDBH0iRJU6iWHYYzBXhF1c/onD/g2LmSPlFiPgx4tvS9sJyPpOmq\nfuEgolaZscZ2pVjyPOBO/f+nPq6xvUHSRcADkt6jWkqYsoMmLgVukTQf6AMW2l4u6YnydaYHyzrr\n54DlZca8maoM4ypJvwfWAK8DK0YQ8rVUFfU3lvfOmF4EnqaqrH9xqa37W6q111Wl5N5GYM7IRidi\n5FKEJSKiZlkKiIioWRJrRETNklgjImqWxBoRUbMk1oiImiWxRkTULIk1IqJmHwIYe8J9lLl0RgAA\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}