{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HSI_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyQTRzMcUCB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare tensorflow-model-optimization for pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNPotwubYRu",
        "colab_type": "code",
        "outputId": "565c9845-a936-4d96-86f4-d0b760a786f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tensorflow-gpu==1.14.0\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c4/4c3d011e432bd9c19f0323f7da7d3f783402615e4c3b5a98416c7da9cb05/tensorflow_model_optimization-0.2.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.17.5)\n",
            "Collecting enum34~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/df/6e/46a0304561a07bda942c4283b67228d5b87d1ebd1189c866ede4b4cd6a0a/enum34-1.1.9-py3-none-any.whl\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.9 tensorflow-model-optimization-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfBLXz1mCNV",
        "colab_type": "code",
        "outputId": "a72e326f-93bf-47d0-83d7-1e8a2c574533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk1dYHpbicZ",
        "colab_type": "text"
      },
      "source": [
        "#Download dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu64LC4Wm1oc",
        "colab_type": "code",
        "outputId": "7af7ecd7-2da9-4537-96dd-89250cc00eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWT-iIqSb_46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8/view?usp=sharing\n",
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8\n",
        "download = drive.CreateFile({'id': '1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8'})\n",
        "download.GetContentFile('weed_dataset_w25.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDlBcilciuY",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRqt2u-MVfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from operator import truediv\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syCzlwNcr-di",
        "colab_type": "code",
        "outputId": "b9c3e907-8889-4062-e285-6c82d5e0a63c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "LOAD HDF5 FILE\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "hdf5_file = h5py.File('weed_dataset_w25.hdf5', \"r\")\n",
        "train_x = np.array(hdf5_file[\"train_img\"][...])\n",
        "# train_x = train_x / np.max(train_x)\n",
        "# train_x = np.clip(train_x, 0, 1)\n",
        "#train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))\n",
        "train_y = np.array(hdf5_file[\"train_labels\"][...])\n",
        "\n",
        "# Average consecutive bands\n",
        "img2 = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], int(train_x.shape[3]/2)))\n",
        "for n in range(0, train_x.shape[0]):\n",
        "    # Average consecutive bands\n",
        "    for i in range(0, train_x.shape[3], 2):\n",
        "        img2[n, :, :, int(i/2)] = (train_x[n, :, :, i] + train_x[n, :, :, i + 1]) / 2.\n",
        "\n",
        "train_x = img2\n",
        "\n",
        "print(train_x.shape)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3wrK8dNe-T7",
        "colab_type": "code",
        "outputId": "48c4328a-9934-4092-9441-e9d357b35fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3]))\n",
        "\n",
        "temp = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], 10))\n",
        "\n",
        "temp[:, :, :, 0] = train_x[:, :, :, 10]\n",
        "temp[:, :, :, 1] = train_x[:, :, :, 15] \n",
        "temp[:, :, :, 2] = train_x[:, :, :,20] \n",
        "temp[:, :, :, 3] = train_x[:, :, :, 27]\n",
        "#temp[:, :, :,5] = train_x[:, :, :, 43]\n",
        "#temp[:, :, :,2] = train_x[:, :, :,55]\n",
        "#temp[:, :, :,3] = train_x[:, :, :, 66]\n",
        "#temp[:, :, :,4] = train_x[:, :, :, 101]\n",
        "#temp[:, :, :, 9] = train_x[:, :, :, 105]\n",
        "temp[:, :, :, 4] = train_x[:, :, :, 36]  \n",
        "temp[:, :, :, 5] = train_x[:, :, :, 59]\n",
        "temp[:, :, :, 6] = train_x[:, :, :, 61]    \n",
        "#temp[:, :, :, 11] = train_x[:, :, :, 107]\n",
        "#temp[:, :, :, 12] = train_x[:, :, :, 111]\n",
        "temp[:, :, :, 7] = train_x[:, :, :, 66] \n",
        "#temp[:, :, :, 14] = train_x[:, :, :, 114]\n",
        "#temp[:, :, :, 7] = train_x[:, :, :, 121]\n",
        "#temp[:, :, :, 16] = train_x[:, :, :, 122]\n",
        "temp[:, :, :, 8] = train_x[:, :, :, 136] \n",
        "#temp[:, :, :, 18] = train_x[:, :, :, 132]\n",
        "temp[:, :, :, 9] = train_x[:, :, :, 144]\n",
        "train_x = temp\n",
        "\n",
        "print(train_x.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvx55MBgcrZJ",
        "colab_type": "text"
      },
      "source": [
        "# Trained pruned network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2xU6uWWAuz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a88daaae-3137-4ea9-871b-f97bfc8ee7c6"
      },
      "source": [
        "data = 'WEED'\n",
        "loaded_model = tf.keras.models.load_model(\"selected10-weights-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE2WVq7DItg",
        "colab_type": "code",
        "outputId": "efe672eb-ea9c-4411-9036-6d53cebc9b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.01,\n",
        "                                                   final_sparsity=0.05,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adadelta',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1485\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 10)]      0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 25, 25, 96)        2268      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 96)        385       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 96)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 13, 13, 96)        19394     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 13, 13, 96)        385       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 13, 13, 96)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 7, 7, 96)          19394     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 7, 7, 96)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_global_a (None, 96)                1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_fc3 (Pru (None, 3)                 581       \n",
            "=================================================================\n",
            "Total params: 42,411\n",
            "Trainable params: 22,333\n",
            "Non-trainable params: 20,078\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYBXiSTtaDt",
        "colab_type": "code",
        "outputId": "fea42b65-54cf-4bb9-abc2-56ee9d43bd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "cva1 = []\n",
        "cva2 = []\n",
        "cva3 = []\n",
        "\n",
        "\n",
        "data = 'WEED'\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "\n",
        "    ytrain = tf.keras.utils.to_categorical(train_y[train]).astype(np.int32)\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.01,\n",
        "                                                      final_sparsity=0.05,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    loaded_model.load_weights(\"selected10-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "    #new_pruned_model.summary()\n",
        "\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.categorical_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # checkpoint\n",
        "    filepath = \"pruned10-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint, sparsity.UpdatePruningStep()]\n",
        "\n",
        "    # Train model on dataset\n",
        "    print(data + \": Training\" + str(ntrain) + \"begins...\")\n",
        "    history = new_pruned_model.fit(x=train_x[train], y=ytrain, validation_data=(train_x[test], ytest),\n",
        "                        batch_size=64, epochs=epochs, callbacks=callbacks_list)\n",
        "    \n",
        "    new_pruned_model.load_weights(\"pruned10-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "    print(np.sum(ypred*ytest)/len(ytest))\n",
        "    \n",
        "    ntrain += 1\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WEED: Training1begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9989\n",
            "Epoch 00001: val_acc improved from -inf to 0.96677, saving model to pruned10-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 769us/sample - loss: 0.0033 - acc: 0.9989 - val_loss: 0.1904 - val_acc: 0.9668\n",
            "Epoch 2/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9991\n",
            "Epoch 00002: val_acc improved from 0.96677 to 0.97943, saving model to pruned10-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 314us/sample - loss: 0.0019 - acc: 0.9991 - val_loss: 0.1698 - val_acc: 0.9794\n",
            "Epoch 3/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9984\n",
            "Epoch 00003: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 295us/sample - loss: 0.0048 - acc: 0.9984 - val_loss: 0.1726 - val_acc: 0.9715\n",
            "Epoch 4/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9991\n",
            "Epoch 00004: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 291us/sample - loss: 0.0033 - acc: 0.9991 - val_loss: 0.1847 - val_acc: 0.9763\n",
            "Epoch 5/15\n",
            "5440/5684 [===========================>..] - ETA: 0s - loss: 0.0046 - acc: 0.9985\n",
            "Epoch 00005: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 299us/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.1791 - val_acc: 0.9778\n",
            "Epoch 6/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
            "Epoch 00006: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 298us/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1758 - val_acc: 0.9763\n",
            "Epoch 7/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
            "Epoch 00007: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 307us/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1746 - val_acc: 0.9763\n",
            "Epoch 8/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9985\n",
            "Epoch 00008: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 297us/sample - loss: 0.0051 - acc: 0.9984 - val_loss: 0.3621 - val_acc: 0.9399\n",
            "Epoch 9/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
            "Epoch 00009: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 299us/sample - loss: 0.0036 - acc: 0.9988 - val_loss: 0.1938 - val_acc: 0.9731\n",
            "Epoch 10/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9988\n",
            "Epoch 00010: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 303us/sample - loss: 0.0029 - acc: 0.9984 - val_loss: 0.1779 - val_acc: 0.9763\n",
            "Epoch 11/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9984\n",
            "Epoch 00011: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 300us/sample - loss: 0.0059 - acc: 0.9984 - val_loss: 0.1775 - val_acc: 0.9763\n",
            "Epoch 12/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9980\n",
            "Epoch 00012: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 299us/sample - loss: 0.0094 - acc: 0.9975 - val_loss: 0.1789 - val_acc: 0.9763\n",
            "Epoch 13/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9988\n",
            "Epoch 00013: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 295us/sample - loss: 0.0028 - acc: 0.9988 - val_loss: 0.1759 - val_acc: 0.9778\n",
            "Epoch 14/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9991\n",
            "Epoch 00014: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 308us/sample - loss: 0.0046 - acc: 0.9991 - val_loss: 0.1696 - val_acc: 0.9763\n",
            "Epoch 15/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
            "Epoch 00015: val_acc did not improve from 0.97943\n",
            "5684/5684 [==============================] - 2s 301us/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1727 - val_acc: 0.9763\n",
            "0.9738970517378758\n",
            "WEED: Training2begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993\n",
            "Epoch 00001: val_acc improved from -inf to 0.95728, saving model to pruned10-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 692us/sample - loss: 0.0021 - acc: 0.9993 - val_loss: 0.1749 - val_acc: 0.9573\n",
            "Epoch 2/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9991\n",
            "Epoch 00002: val_acc improved from 0.95728 to 0.97785, saving model to pruned10-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 317us/sample - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0916 - val_acc: 0.9778\n",
            "Epoch 3/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9995\n",
            "Epoch 00003: val_acc improved from 0.97785 to 0.98259, saving model to pruned10-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 310us/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0816 - val_acc: 0.9826\n",
            "Epoch 4/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9987\n",
            "Epoch 00004: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 301us/sample - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0783 - val_acc: 0.9826\n",
            "Epoch 5/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9993\n",
            "Epoch 00005: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 316us/sample - loss: 0.0037 - acc: 0.9993 - val_loss: 0.0796 - val_acc: 0.9810\n",
            "Epoch 6/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
            "Epoch 00006: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 296us/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0787 - val_acc: 0.9810\n",
            "Epoch 7/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9987\n",
            "Epoch 00007: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 289us/sample - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0879 - val_acc: 0.9794\n",
            "Epoch 8/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9987\n",
            "Epoch 00008: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 303us/sample - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0896 - val_acc: 0.9778\n",
            "Epoch 9/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9984\n",
            "Epoch 00009: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 305us/sample - loss: 0.0043 - acc: 0.9984 - val_loss: 0.0803 - val_acc: 0.9810\n",
            "Epoch 10/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
            "Epoch 00010: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 294us/sample - loss: 0.0032 - acc: 0.9993 - val_loss: 0.0782 - val_acc: 0.9810\n",
            "Epoch 11/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9993\n",
            "Epoch 00011: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 301us/sample - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0764 - val_acc: 0.9826\n",
            "Epoch 12/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
            "Epoch 00012: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 301us/sample - loss: 0.0024 - acc: 0.9996 - val_loss: 0.0802 - val_acc: 0.9826\n",
            "Epoch 13/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9989\n",
            "Epoch 00013: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 299us/sample - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0797 - val_acc: 0.9826\n",
            "Epoch 14/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993\n",
            "Epoch 00014: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 305us/sample - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0870 - val_acc: 0.9794\n",
            "Epoch 15/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9984\n",
            "Epoch 00015: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 309us/sample - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0813 - val_acc: 0.9826\n",
            "0.9791549568668341\n",
            "WEED: Training3begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9975\n",
            "Epoch 00001: val_acc improved from -inf to 0.95886, saving model to pruned10-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 723us/sample - loss: 0.0081 - acc: 0.9970 - val_loss: 0.1535 - val_acc: 0.9589\n",
            "Epoch 2/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9970\n",
            "Epoch 00002: val_acc improved from 0.95886 to 0.96994, saving model to pruned10-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 325us/sample - loss: 0.0080 - acc: 0.9970 - val_loss: 0.1098 - val_acc: 0.9699\n",
            "Epoch 3/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9980\n",
            "Epoch 00003: val_acc did not improve from 0.96994\n",
            "5684/5684 [==============================] - 2s 316us/sample - loss: 0.0057 - acc: 0.9981 - val_loss: 0.1014 - val_acc: 0.9699\n",
            "Epoch 4/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9986\n",
            "Epoch 00004: val_acc improved from 0.96994 to 0.97310, saving model to pruned10-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 306us/sample - loss: 0.0052 - acc: 0.9986 - val_loss: 0.1047 - val_acc: 0.9731\n",
            "Epoch 5/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9973\n",
            "Epoch 00005: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 2s 318us/sample - loss: 0.0086 - acc: 0.9974 - val_loss: 0.1108 - val_acc: 0.9684\n",
            "Epoch 6/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9973\n",
            "Epoch 00006: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 2s 323us/sample - loss: 0.0079 - acc: 0.9974 - val_loss: 0.1077 - val_acc: 0.9699\n",
            "Epoch 7/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9979\n",
            "Epoch 00007: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 2s 319us/sample - loss: 0.0071 - acc: 0.9979 - val_loss: 0.1209 - val_acc: 0.9715\n",
            "Epoch 8/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9986\n",
            "Epoch 00008: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 2s 302us/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.1211 - val_acc: 0.9715\n",
            "Epoch 9/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9986\n",
            "Epoch 00009: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 2s 307us/sample - loss: 0.0058 - acc: 0.9982 - val_loss: 0.1108 - val_acc: 0.9731\n",
            "Epoch 10/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9973\n",
            "Epoch 00010: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 2s 314us/sample - loss: 0.0085 - acc: 0.9974 - val_loss: 0.1114 - val_acc: 0.9731\n",
            "Epoch 11/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
            "Epoch 00011: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 2s 296us/sample - loss: 0.0030 - acc: 0.9993 - val_loss: 0.1107 - val_acc: 0.9731\n",
            "Epoch 12/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9984\n",
            "Epoch 00012: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 2s 300us/sample - loss: 0.0051 - acc: 0.9984 - val_loss: 0.1104 - val_acc: 0.9731\n",
            "Epoch 13/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9982\n",
            "Epoch 00013: val_acc did not improve from 0.97310\n",
            "5684/5684 [==============================] - 2s 308us/sample - loss: 0.0079 - acc: 0.9982 - val_loss: 0.1113 - val_acc: 0.9731\n",
            "Epoch 14/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9979\n",
            "Epoch 00014: val_acc improved from 0.97310 to 0.97468, saving model to pruned10-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 307us/sample - loss: 0.0069 - acc: 0.9979 - val_loss: 0.1151 - val_acc: 0.9747\n",
            "Epoch 15/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9986\n",
            "Epoch 00015: val_acc did not improve from 0.97468\n",
            "5684/5684 [==============================] - 2s 295us/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.1126 - val_acc: 0.9715\n",
            "0.9662964220869462\n",
            "WEED: Training4begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9984\n",
            "Epoch 00001: val_acc improved from -inf to 0.95411, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 714us/sample - loss: 0.0052 - acc: 0.9984 - val_loss: 0.1768 - val_acc: 0.9541\n",
            "Epoch 2/15\n",
            "5440/5684 [===========================>..] - ETA: 0s - loss: 0.0071 - acc: 0.9974\n",
            "Epoch 00002: val_acc improved from 0.95411 to 0.96044, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 322us/sample - loss: 0.0069 - acc: 0.9975 - val_loss: 0.1336 - val_acc: 0.9604\n",
            "Epoch 3/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9968\n",
            "Epoch 00003: val_acc improved from 0.96044 to 0.96203, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 310us/sample - loss: 0.0100 - acc: 0.9960 - val_loss: 0.1365 - val_acc: 0.9620\n",
            "Epoch 4/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9973\n",
            "Epoch 00004: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 2s 299us/sample - loss: 0.0105 - acc: 0.9974 - val_loss: 0.1329 - val_acc: 0.9620\n",
            "Epoch 5/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9985\n",
            "Epoch 00005: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 2s 303us/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.1323 - val_acc: 0.9604\n",
            "Epoch 6/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9996\n",
            "Epoch 00006: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 2s 298us/sample - loss: 0.0043 - acc: 0.9993 - val_loss: 0.1283 - val_acc: 0.9620\n",
            "Epoch 7/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9949\n",
            "Epoch 00007: val_acc did not improve from 0.96203\n",
            "5684/5684 [==============================] - 2s 299us/sample - loss: 0.0123 - acc: 0.9951 - val_loss: 0.1498 - val_acc: 0.9620\n",
            "Epoch 8/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9980\n",
            "Epoch 00008: val_acc improved from 0.96203 to 0.96361, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 318us/sample - loss: 0.0054 - acc: 0.9981 - val_loss: 0.1322 - val_acc: 0.9636\n",
            "Epoch 9/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9982\n",
            "Epoch 00009: val_acc improved from 0.96361 to 0.96677, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 307us/sample - loss: 0.0056 - acc: 0.9982 - val_loss: 0.1160 - val_acc: 0.9668\n",
            "Epoch 10/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9980\n",
            "Epoch 00010: val_acc improved from 0.96677 to 0.96835, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 302us/sample - loss: 0.0068 - acc: 0.9981 - val_loss: 0.1166 - val_acc: 0.9684\n",
            "Epoch 11/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9975\n",
            "Epoch 00011: val_acc did not improve from 0.96835\n",
            "5684/5684 [==============================] - 2s 296us/sample - loss: 0.0101 - acc: 0.9975 - val_loss: 0.1153 - val_acc: 0.9636\n",
            "Epoch 12/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9968\n",
            "Epoch 00012: val_acc did not improve from 0.96835\n",
            "5684/5684 [==============================] - 2s 302us/sample - loss: 0.0091 - acc: 0.9968 - val_loss: 0.1749 - val_acc: 0.9541\n",
            "Epoch 13/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9988\n",
            "Epoch 00013: val_acc did not improve from 0.96835\n",
            "5684/5684 [==============================] - 2s 305us/sample - loss: 0.0054 - acc: 0.9988 - val_loss: 0.1395 - val_acc: 0.9589\n",
            "Epoch 14/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9976\n",
            "Epoch 00014: val_acc did not improve from 0.96835\n",
            "5684/5684 [==============================] - 2s 302us/sample - loss: 0.0075 - acc: 0.9977 - val_loss: 0.1354 - val_acc: 0.9636\n",
            "Epoch 15/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9986\n",
            "Epoch 00015: val_acc did not improve from 0.96835\n",
            "5684/5684 [==============================] - 2s 288us/sample - loss: 0.0063 - acc: 0.9984 - val_loss: 0.1327 - val_acc: 0.9652\n",
            "0.9623676814448522\n",
            "WEED: Training5begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9979\n",
            "Epoch 00001: val_acc improved from -inf to 0.96203, saving model to pruned10-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 730us/sample - loss: 0.0065 - acc: 0.9979 - val_loss: 0.1149 - val_acc: 0.9620\n",
            "Epoch 2/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9975\n",
            "Epoch 00002: val_acc improved from 0.96203 to 0.97785, saving model to pruned10-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 309us/sample - loss: 0.0080 - acc: 0.9975 - val_loss: 0.1041 - val_acc: 0.9778\n",
            "Epoch 3/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9989\n",
            "Epoch 00003: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 297us/sample - loss: 0.0057 - acc: 0.9989 - val_loss: 0.1035 - val_acc: 0.9747\n",
            "Epoch 4/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9975\n",
            "Epoch 00004: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 296us/sample - loss: 0.0065 - acc: 0.9975 - val_loss: 0.1063 - val_acc: 0.9731\n",
            "Epoch 5/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9985\n",
            "Epoch 00005: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 294us/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.1106 - val_acc: 0.9699\n",
            "Epoch 6/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9980\n",
            "Epoch 00006: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 294us/sample - loss: 0.0064 - acc: 0.9981 - val_loss: 0.0968 - val_acc: 0.9699\n",
            "Epoch 7/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9978\n",
            "Epoch 00007: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 301us/sample - loss: 0.0077 - acc: 0.9979 - val_loss: 0.1046 - val_acc: 0.9684\n",
            "Epoch 8/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9977\n",
            "Epoch 00008: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 300us/sample - loss: 0.0071 - acc: 0.9977 - val_loss: 0.1304 - val_acc: 0.9636\n",
            "Epoch 9/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9986\n",
            "Epoch 00009: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 307us/sample - loss: 0.0068 - acc: 0.9984 - val_loss: 0.1084 - val_acc: 0.9715\n",
            "Epoch 10/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9989\n",
            "Epoch 00010: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 299us/sample - loss: 0.0054 - acc: 0.9989 - val_loss: 0.1027 - val_acc: 0.9715\n",
            "Epoch 11/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9987\n",
            "Epoch 00011: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 291us/sample - loss: 0.0061 - acc: 0.9988 - val_loss: 0.1030 - val_acc: 0.9715\n",
            "Epoch 12/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9984\n",
            "Epoch 00012: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 297us/sample - loss: 0.0066 - acc: 0.9982 - val_loss: 0.1046 - val_acc: 0.9715\n",
            "Epoch 13/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9980\n",
            "Epoch 00013: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 300us/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.1020 - val_acc: 0.9731\n",
            "Epoch 14/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9991\n",
            "Epoch 00014: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 297us/sample - loss: 0.0052 - acc: 0.9989 - val_loss: 0.1031 - val_acc: 0.9715\n",
            "Epoch 15/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9989\n",
            "Epoch 00015: val_acc did not improve from 0.97785\n",
            "5684/5684 [==============================] - 2s 306us/sample - loss: 0.0059 - acc: 0.9989 - val_loss: 0.1014 - val_acc: 0.9715\n",
            "0.9716730693422805\n",
            "WEED: Training6begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9965\n",
            "Epoch 00001: val_acc improved from -inf to 0.96203, saving model to pruned10-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 718us/sample - loss: 0.0083 - acc: 0.9967 - val_loss: 0.1880 - val_acc: 0.9620\n",
            "Epoch 2/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9980\n",
            "Epoch 00002: val_acc improved from 0.96203 to 0.97785, saving model to pruned10-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 314us/sample - loss: 0.0059 - acc: 0.9981 - val_loss: 0.1079 - val_acc: 0.9778\n",
            "Epoch 3/15\n",
            "5440/5684 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9974\n",
            "Epoch 00003: val_acc improved from 0.97785 to 0.97943, saving model to pruned10-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 314us/sample - loss: 0.0047 - acc: 0.9975 - val_loss: 0.0929 - val_acc: 0.9794\n",
            "Epoch 4/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9973\n",
            "Epoch 00004: val_acc improved from 0.97943 to 0.98101, saving model to pruned10-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 316us/sample - loss: 0.0086 - acc: 0.9974 - val_loss: 0.0922 - val_acc: 0.9810\n",
            "Epoch 5/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9964\n",
            "Epoch 00005: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 295us/sample - loss: 0.0106 - acc: 0.9963 - val_loss: 0.0877 - val_acc: 0.9778\n",
            "Epoch 6/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9977\n",
            "Epoch 00006: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 303us/sample - loss: 0.0066 - acc: 0.9977 - val_loss: 0.0856 - val_acc: 0.9778\n",
            "Epoch 7/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9987\n",
            "Epoch 00007: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 299us/sample - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0851 - val_acc: 0.9810\n",
            "Epoch 8/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9984\n",
            "Epoch 00008: val_acc improved from 0.98101 to 0.98259, saving model to pruned10-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 314us/sample - loss: 0.0048 - acc: 0.9984 - val_loss: 0.0893 - val_acc: 0.9826\n",
            "Epoch 9/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9985\n",
            "Epoch 00009: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 301us/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0913 - val_acc: 0.9826\n",
            "Epoch 10/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9982\n",
            "Epoch 00010: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 298us/sample - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0897 - val_acc: 0.9810\n",
            "Epoch 11/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9980\n",
            "Epoch 00011: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 305us/sample - loss: 0.0072 - acc: 0.9981 - val_loss: 0.1071 - val_acc: 0.9763\n",
            "Epoch 12/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9980\n",
            "Epoch 00012: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 300us/sample - loss: 0.0049 - acc: 0.9981 - val_loss: 0.0924 - val_acc: 0.9794\n",
            "Epoch 13/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9973\n",
            "Epoch 00013: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 308us/sample - loss: 0.0075 - acc: 0.9974 - val_loss: 0.0899 - val_acc: 0.9794\n",
            "Epoch 14/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9980\n",
            "Epoch 00014: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 298us/sample - loss: 0.0062 - acc: 0.9981 - val_loss: 0.0905 - val_acc: 0.9794\n",
            "Epoch 15/15\n",
            "5504/5684 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9973\n",
            "Epoch 00015: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 2s 301us/sample - loss: 0.0073 - acc: 0.9974 - val_loss: 0.0893 - val_acc: 0.9778\n",
            "0.9768476414369717\n",
            "WEED: Training7begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9993\n",
            "Epoch 00001: val_acc improved from -inf to 0.97464, saving model to pruned10-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 4s 735us/sample - loss: 0.0040 - acc: 0.9993 - val_loss: 0.1061 - val_acc: 0.9746\n",
            "Epoch 2/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9986\n",
            "Epoch 00002: val_acc improved from 0.97464 to 0.97940, saving model to pruned10-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 304us/sample - loss: 0.0046 - acc: 0.9982 - val_loss: 0.0762 - val_acc: 0.9794\n",
            "Epoch 3/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9991\n",
            "Epoch 00003: val_acc did not improve from 0.97940\n",
            "5685/5685 [==============================] - 2s 294us/sample - loss: 0.0037 - acc: 0.9991 - val_loss: 0.0730 - val_acc: 0.9762\n",
            "Epoch 4/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9996\n",
            "Epoch 00004: val_acc did not improve from 0.97940\n",
            "5685/5685 [==============================] - 2s 298us/sample - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0698 - val_acc: 0.9746\n",
            "Epoch 5/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9982\n",
            "Epoch 00005: val_acc improved from 0.97940 to 0.98732, saving model to pruned10-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 310us/sample - loss: 0.0051 - acc: 0.9981 - val_loss: 0.0592 - val_acc: 0.9873\n",
            "Epoch 6/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9989\n",
            "Epoch 00006: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 308us/sample - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0610 - val_acc: 0.9873\n",
            "Epoch 7/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9982\n",
            "Epoch 00007: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 303us/sample - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0596 - val_acc: 0.9826\n",
            "Epoch 8/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9982\n",
            "Epoch 00008: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 293us/sample - loss: 0.0049 - acc: 0.9981 - val_loss: 0.0608 - val_acc: 0.9826\n",
            "Epoch 9/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
            "Epoch 00009: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 301us/sample - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0791 - val_acc: 0.9731\n",
            "Epoch 10/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9988\n",
            "Epoch 00010: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 301us/sample - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0652 - val_acc: 0.9778\n",
            "Epoch 11/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9975\n",
            "Epoch 00011: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 303us/sample - loss: 0.0066 - acc: 0.9975 - val_loss: 0.0604 - val_acc: 0.9826\n",
            "Epoch 12/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
            "Epoch 00012: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 296us/sample - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0587 - val_acc: 0.9826\n",
            "Epoch 13/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9982\n",
            "Epoch 00013: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 301us/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0585 - val_acc: 0.9810\n",
            "Epoch 14/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9984\n",
            "Epoch 00014: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 301us/sample - loss: 0.0067 - acc: 0.9984 - val_loss: 0.0588 - val_acc: 0.9810\n",
            "Epoch 15/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9986\n",
            "Epoch 00015: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 300us/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0588 - val_acc: 0.9826\n",
            "0.9825492248434969\n",
            "WEED: Training8begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9958\n",
            "Epoch 00001: val_acc improved from -inf to 0.95721, saving model to pruned10-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 4s 717us/sample - loss: 0.0119 - acc: 0.9956 - val_loss: 0.2003 - val_acc: 0.9572\n",
            "Epoch 2/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9968\n",
            "Epoch 00002: val_acc improved from 0.95721 to 0.97940, saving model to pruned10-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 315us/sample - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0879 - val_acc: 0.9794\n",
            "Epoch 3/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9968\n",
            "Epoch 00003: val_acc improved from 0.97940 to 0.98415, saving model to pruned10-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 301us/sample - loss: 0.0096 - acc: 0.9967 - val_loss: 0.0781 - val_acc: 0.9842\n",
            "Epoch 4/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9966\n",
            "Epoch 00004: val_acc improved from 0.98415 to 0.98574, saving model to pruned10-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 317us/sample - loss: 0.0115 - acc: 0.9967 - val_loss: 0.0754 - val_acc: 0.9857\n",
            "Epoch 5/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9968\n",
            "Epoch 00005: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 2s 306us/sample - loss: 0.0101 - acc: 0.9967 - val_loss: 0.0745 - val_acc: 0.9857\n",
            "Epoch 6/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9963\n",
            "Epoch 00006: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 2s 302us/sample - loss: 0.0106 - acc: 0.9963 - val_loss: 0.0726 - val_acc: 0.9857\n",
            "Epoch 7/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9971\n",
            "Epoch 00007: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 2s 298us/sample - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0748 - val_acc: 0.9826\n",
            "Epoch 8/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9976\n",
            "Epoch 00008: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 2s 302us/sample - loss: 0.0083 - acc: 0.9975 - val_loss: 0.0751 - val_acc: 0.9842\n",
            "Epoch 9/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9962\n",
            "Epoch 00009: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 2s 305us/sample - loss: 0.0103 - acc: 0.9963 - val_loss: 0.0890 - val_acc: 0.9842\n",
            "Epoch 10/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9970\n",
            "Epoch 00010: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 2s 304us/sample - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0798 - val_acc: 0.9842\n",
            "Epoch 11/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9973\n",
            "Epoch 00011: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 2s 298us/sample - loss: 0.0096 - acc: 0.9972 - val_loss: 0.0779 - val_acc: 0.9857\n",
            "Epoch 12/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9969\n",
            "Epoch 00012: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 2s 292us/sample - loss: 0.0091 - acc: 0.9968 - val_loss: 0.0764 - val_acc: 0.9857\n",
            "Epoch 13/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9961\n",
            "Epoch 00013: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 2s 299us/sample - loss: 0.0120 - acc: 0.9961 - val_loss: 0.0751 - val_acc: 0.9857\n",
            "Epoch 14/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9982\n",
            "Epoch 00014: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 2s 296us/sample - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0750 - val_acc: 0.9857\n",
            "Epoch 15/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9964\n",
            "Epoch 00015: val_acc improved from 0.98574 to 0.98732, saving model to pruned10-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 315us/sample - loss: 0.0084 - acc: 0.9965 - val_loss: 0.0752 - val_acc: 0.9873\n",
            "0.9803893502500435\n",
            "WEED: Training9begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9980\n",
            "Epoch 00001: val_acc improved from -inf to 0.96197, saving model to pruned10-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 4s 754us/sample - loss: 0.0067 - acc: 0.9981 - val_loss: 0.1360 - val_acc: 0.9620\n",
            "Epoch 2/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9978\n",
            "Epoch 00002: val_acc improved from 0.96197 to 0.97147, saving model to pruned10-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 313us/sample - loss: 0.0095 - acc: 0.9974 - val_loss: 0.1017 - val_acc: 0.9715\n",
            "Epoch 3/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9986\n",
            "Epoch 00003: val_acc improved from 0.97147 to 0.97464, saving model to pruned10-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 304us/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.1023 - val_acc: 0.9746\n",
            "Epoch 4/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9987\n",
            "Epoch 00004: val_acc did not improve from 0.97464\n",
            "5685/5685 [==============================] - 2s 291us/sample - loss: 0.0057 - acc: 0.9988 - val_loss: 0.1040 - val_acc: 0.9731\n",
            "Epoch 5/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9979\n",
            "Epoch 00005: val_acc did not improve from 0.97464\n",
            "5685/5685 [==============================] - 2s 299us/sample - loss: 0.0081 - acc: 0.9979 - val_loss: 0.1078 - val_acc: 0.9731\n",
            "Epoch 6/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9964\n",
            "Epoch 00006: val_acc did not improve from 0.97464\n",
            "5685/5685 [==============================] - 2s 307us/sample - loss: 0.0123 - acc: 0.9965 - val_loss: 0.1248 - val_acc: 0.9715\n",
            "Epoch 7/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9972\n",
            "Epoch 00007: val_acc did not improve from 0.97464\n",
            "5685/5685 [==============================] - 2s 315us/sample - loss: 0.0086 - acc: 0.9968 - val_loss: 0.1014 - val_acc: 0.9731\n",
            "Epoch 8/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9958\n",
            "Epoch 00008: val_acc improved from 0.97464 to 0.97623, saving model to pruned10-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 325us/sample - loss: 0.0121 - acc: 0.9960 - val_loss: 0.1011 - val_acc: 0.9762\n",
            "Epoch 9/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9980\n",
            "Epoch 00009: val_acc did not improve from 0.97623\n",
            "5685/5685 [==============================] - 2s 313us/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0966 - val_acc: 0.9731\n",
            "Epoch 10/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9975\n",
            "Epoch 00010: val_acc did not improve from 0.97623\n",
            "5685/5685 [==============================] - 2s 311us/sample - loss: 0.0079 - acc: 0.9975 - val_loss: 0.1004 - val_acc: 0.9762\n",
            "Epoch 11/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9964\n",
            "Epoch 00011: val_acc did not improve from 0.97623\n",
            "5685/5685 [==============================] - 2s 309us/sample - loss: 0.0088 - acc: 0.9965 - val_loss: 0.1068 - val_acc: 0.9762\n",
            "Epoch 12/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9975\n",
            "Epoch 00012: val_acc did not improve from 0.97623\n",
            "5685/5685 [==============================] - 2s 299us/sample - loss: 0.0076 - acc: 0.9975 - val_loss: 0.1032 - val_acc: 0.9762\n",
            "Epoch 13/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9976\n",
            "Epoch 00013: val_acc did not improve from 0.97623\n",
            "5685/5685 [==============================] - 2s 298us/sample - loss: 0.0071 - acc: 0.9977 - val_loss: 0.1023 - val_acc: 0.9762\n",
            "Epoch 14/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9985\n",
            "Epoch 00014: val_acc did not improve from 0.97623\n",
            "5685/5685 [==============================] - 2s 298us/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.1023 - val_acc: 0.9762\n",
            "Epoch 15/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9979\n",
            "Epoch 00015: val_acc did not improve from 0.97623\n",
            "5685/5685 [==============================] - 2s 297us/sample - loss: 0.0077 - acc: 0.9979 - val_loss: 0.1024 - val_acc: 0.9762\n",
            "0.9701188940355097\n",
            "WEED: Training10begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9971\n",
            "Epoch 00001: val_acc improved from -inf to 0.96513, saving model to pruned10-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 4s 697us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.1441 - val_acc: 0.9651\n",
            "Epoch 2/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9950\n",
            "Epoch 00002: val_acc improved from 0.96513 to 0.96830, saving model to pruned10-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 317us/sample - loss: 0.0113 - acc: 0.9951 - val_loss: 0.1282 - val_acc: 0.9683\n",
            "Epoch 3/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9967\n",
            "Epoch 00003: val_acc improved from 0.96830 to 0.96989, saving model to pruned10-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 317us/sample - loss: 0.0100 - acc: 0.9968 - val_loss: 0.1225 - val_acc: 0.9699\n",
            "Epoch 4/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9982\n",
            "Epoch 00004: val_acc did not improve from 0.96989\n",
            "5685/5685 [==============================] - 2s 302us/sample - loss: 0.0076 - acc: 0.9982 - val_loss: 0.1182 - val_acc: 0.9699\n",
            "Epoch 5/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9968\n",
            "Epoch 00005: val_acc improved from 0.96989 to 0.97147, saving model to pruned10-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 308us/sample - loss: 0.0119 - acc: 0.9967 - val_loss: 0.1196 - val_acc: 0.9715\n",
            "Epoch 6/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9968\n",
            "Epoch 00006: val_acc improved from 0.97147 to 0.97306, saving model to pruned10-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 2s 315us/sample - loss: 0.0109 - acc: 0.9968 - val_loss: 0.1195 - val_acc: 0.9731\n",
            "Epoch 7/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9973\n",
            "Epoch 00007: val_acc did not improve from 0.97306\n",
            "5685/5685 [==============================] - 2s 306us/sample - loss: 0.0101 - acc: 0.9974 - val_loss: 0.1203 - val_acc: 0.9699\n",
            "Epoch 8/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9968\n",
            "Epoch 00008: val_acc did not improve from 0.97306\n",
            "5685/5685 [==============================] - 2s 305us/sample - loss: 0.0105 - acc: 0.9967 - val_loss: 0.1204 - val_acc: 0.9699\n",
            "Epoch 9/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9970\n",
            "Epoch 00009: val_acc did not improve from 0.97306\n",
            "5685/5685 [==============================] - 2s 299us/sample - loss: 0.0090 - acc: 0.9970 - val_loss: 0.1244 - val_acc: 0.9715\n",
            "Epoch 10/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9964\n",
            "Epoch 00010: val_acc did not improve from 0.97306\n",
            "5685/5685 [==============================] - 2s 299us/sample - loss: 0.0120 - acc: 0.9965 - val_loss: 0.1217 - val_acc: 0.9699\n",
            "Epoch 11/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9971\n",
            "Epoch 00011: val_acc did not improve from 0.97306\n",
            "5685/5685 [==============================] - 2s 296us/sample - loss: 0.0080 - acc: 0.9968 - val_loss: 0.1206 - val_acc: 0.9699\n",
            "Epoch 12/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9973\n",
            "Epoch 00012: val_acc did not improve from 0.97306\n",
            "5685/5685 [==============================] - 2s 295us/sample - loss: 0.0103 - acc: 0.9972 - val_loss: 0.1209 - val_acc: 0.9699\n",
            "Epoch 13/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9976\n",
            "Epoch 00013: val_acc did not improve from 0.97306\n",
            "5685/5685 [==============================] - 2s 297us/sample - loss: 0.0080 - acc: 0.9975 - val_loss: 0.1209 - val_acc: 0.9683\n",
            "Epoch 14/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9973\n",
            "Epoch 00014: val_acc did not improve from 0.97306\n",
            "5685/5685 [==============================] - 2s 294us/sample - loss: 0.0080 - acc: 0.9974 - val_loss: 0.1209 - val_acc: 0.9683\n",
            "Epoch 15/15\n",
            "5504/5685 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9975\n",
            "Epoch 00015: val_acc did not improve from 0.97306\n",
            "5685/5685 [==============================] - 2s 296us/sample - loss: 0.0079 - acc: 0.9975 - val_loss: 0.1206 - val_acc: 0.9683\n",
            "0.9657471926392109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmz9PSRTvPP",
        "colab_type": "code",
        "outputId": "28281935-61fd-4cf8-c606-c97f342c851e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "new_pruned_model.load_weights(\"pruned10-weights-hyper3dnet\" + data + str(7) + \"-best_3layers_4filters.h5\")\n",
        "final_model = sparsity.strip_pruning(new_pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 10)]      0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_8 (Separabl (None, 25, 25, 96)        1306      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 25, 25, 96)        384       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 25, 25, 96)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_9 (Separabl (None, 13, 13, 96)        10176     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 13, 13, 96)        384       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 13, 13, 96)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_10 (Separab (None, 7, 7, 96)          10176     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 7, 7, 96)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 96)                0         \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 3)                 291       \n",
            "=================================================================\n",
            "Total params: 22,717\n",
            "Trainable params: 22,333\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8drkv1mK1CUb",
        "colab_type": "code",
        "outputId": "4cca217f-124b-4d3a-bb57-399cce2b8e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "import numpy as np\n",
        "for i, w in enumerate(final_model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.4f}%\".format(\n",
        "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "separable_conv2d_8/depthwise_kernel:0 -- Total:250, Zeros: 0.0000%\n",
            "separable_conv2d_8/pointwise_kernel:0 -- Total:960, Zeros: 3.4375%\n",
            "separable_conv2d_8/bias:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_10/gamma:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_10/beta:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_10/moving_mean:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_10/moving_variance:0 -- Total:96, Zeros: 0.0000%\n",
            "separable_conv2d_9/depthwise_kernel:0 -- Total:864, Zeros: 0.0000%\n",
            "separable_conv2d_9/pointwise_kernel:0 -- Total:9216, Zeros: 3.4397%\n",
            "separable_conv2d_9/bias:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_11/gamma:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_11/beta:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_11/moving_mean:0 -- Total:96, Zeros: 0.0000%\n",
            "batch_normalization_11/moving_variance:0 -- Total:96, Zeros: 0.0000%\n",
            "separable_conv2d_10/depthwise_kernel:0 -- Total:864, Zeros: 0.0000%\n",
            "separable_conv2d_10/pointwise_kernel:0 -- Total:9216, Zeros: 3.4397%\n",
            "separable_conv2d_10/bias:0 -- Total:96, Zeros: 0.0000%\n",
            "fc3/kernel:0 -- Total:288, Zeros: 3.4722%\n",
            "fc3/bias:0 -- Total:3, Zeros: 0.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIRfZDn02rV",
        "colab_type": "code",
        "outputId": "036f1189-842d-4723-b503-9ac82d573f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Saving pruned model to: ', \"Kochia_hyper3DNet_pruned.h5\")\n",
        "tf.keras.models.save_model(final_model, \"Kochia_hyper3DNet_10bands_pruned.h5\", \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  Kochia_hyper3DNet_pruned.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tkgm44OZDt",
        "colab_type": "text"
      },
      "source": [
        "# Predict and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVT1KBWvEODg",
        "colab_type": "code",
        "outputId": "2cdfcf57-e8de-4bd6-e950-09ff47db33c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import truediv\n",
        "import h5py\n",
        "import pickle\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Prepare lists to save metrics\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_m):\n",
        "    list_diag = np.diag(confusion_m)\n",
        "    list_raw_sum = np.sum(confusion_m, axis=1)\n",
        "    each_ac = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_ac)\n",
        "    return each_ac, average_acc\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "PREDICT AND CALCULATE METRICS\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "loaded_model = tf.keras.models.load_model(\"selected10-weights-hyper3dnet\" + 'WEED' + str(1) + \"-best_3layers_4filters.h5\")\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "dataset = 'WEED'\n",
        "# Initialize\n",
        "confmatrices = np.zeros((10, int(classes), int(classes)))\n",
        "\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.30,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.load_weights(\"pruned10-weights-hyper3dnet\" + dataset + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        con_mat = tf.math.confusion_matrix(labels=np.argmax(ytest, axis=-1),\n",
        "                                           predictions=np.argmax(ypred, axis=-1)).numpy()\n",
        "\n",
        "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "    classes_list = list(range(0, int(classes)))\n",
        "    con_mat_df = pd.DataFrame(con_mat_norm, index=classes_list, columns=classes_list)\n",
        "\n",
        "    confmatrices[ntrain - 1, :, :] = con_mat_df.values\n",
        "\n",
        "    # Calculate metrics\n",
        "    oa = accuracy_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    confusion = confusion_matrix(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    prec, rec, f1, support = precision_recall_fscore_support(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1),\n",
        "                                                             average='macro')\n",
        "\n",
        "    # Add metrics to the list\n",
        "    cvoa.append(oa * 100)\n",
        "    cvaa.append(aa * 100)\n",
        "    cvka.append(kappa * 100)\n",
        "    cvpre.append(prec * 100)\n",
        "    cvrec.append(rec * 100)\n",
        "    cvf1.append(f1 * 100)\n",
        "\n",
        "    ntrain += 1\n",
        "\n",
        "file_name = \"classification_report_hyper3dnet_pruned10\" + dataset + \".txt\"\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write(\"Overall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvoa)), float(np.std(cvoa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Average accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvaa)), float(np.std(cvaa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Kappa accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvka)), float(np.std(cvka))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Precision accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvpre)), float(np.std(cvpre))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Recall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvrec)), float(np.std(cvrec))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"F1 accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvf1)), float(np.std(cvf1))))\n",
        "\n",
        "# Calculate mean and std\n",
        "means = np.mean(confmatrices * 100, axis=0)\n",
        "stds = np.std(confmatrices * 100, axis=0)\n",
        "\n",
        "def plot_confusion_matrix(cm, cms, classescf,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classescf))\n",
        "    plt.xticks(tick_marks, classescf, rotation=45)\n",
        "    plt.yticks(tick_marks, classescf)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "\n",
        "            if (cm[i, j] == 100 or cm[i, j] == 0) and cms[i, j] == 0:\n",
        "                plt.text(j, i, '{0:.0f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.0f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "            else:\n",
        "                plt.text(j, i, '{0:.2f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.2f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "with open('meanshyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(means, f)\n",
        "with open('stdshyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(stds, f)\n",
        "with open('cvf1hyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(cvf1, f)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "classes_list = list(range(0, int(3)))\n",
        "plt.figure()\n",
        "plot_confusion_matrix(means, stds, classescf=classes_list)\n",
        "\n",
        "plt.savefig('MatrixConfusion_WEEDhyper3dnet_pruned_10p.png', dpi=1200)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAEmCAYAAAA5jbhCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}