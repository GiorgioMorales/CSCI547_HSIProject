{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HSI_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyQTRzMcUCB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare tensorflow-model-optimization for pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNPotwubYRu",
        "colab_type": "code",
        "outputId": "20f3b5f7-f3f6-49c5-b42b-f0048ebac7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tensorflow-gpu==1.14.0\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 71.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c4/4c3d011e432bd9c19f0323f7da7d3f783402615e4c3b5a98416c7da9cb05/tensorflow_model_optimization-0.2.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.17.5)\n",
            "Collecting enum34~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/df/6e/46a0304561a07bda942c4283b67228d5b87d1ebd1189c866ede4b4cd6a0a/enum34-1.1.9-py3-none-any.whl\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.9 tensorflow-model-optimization-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfBLXz1mCNV",
        "colab_type": "code",
        "outputId": "62fc0868-0600-49c3-e2ed-43904f8a4b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk1dYHpbicZ",
        "colab_type": "text"
      },
      "source": [
        "#Download dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu64LC4Wm1oc",
        "colab_type": "code",
        "outputId": "95f05042-dcf1-4e7b-c8f8-e1d560ffaee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWT-iIqSb_46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8/view?usp=sharing\n",
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8\n",
        "download = drive.CreateFile({'id': '1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8'})\n",
        "download.GetContentFile('weed_dataset_w25.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDlBcilciuY",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRqt2u-MVfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from operator import truediv\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syCzlwNcr-di",
        "colab_type": "code",
        "outputId": "e94d6763-4f44-405d-ba97-b66359cd5abb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "LOAD HDF5 FILE\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "hdf5_file = h5py.File('weed_dataset_w25.hdf5', \"r\")\n",
        "train_x = np.array(hdf5_file[\"train_img\"][...])\n",
        "# train_x = train_x / np.max(train_x)\n",
        "# train_x = np.clip(train_x, 0, 1)\n",
        "#train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))\n",
        "train_y = np.array(hdf5_file[\"train_labels\"][...])\n",
        "\n",
        "# Average consecutive bands\n",
        "img2 = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], int(train_x.shape[3]/2)))\n",
        "for n in range(0, train_x.shape[0]):\n",
        "    # Average consecutive bands\n",
        "    for i in range(0, train_x.shape[3], 2):\n",
        "        img2[n, :, :, int(i/2)] = (train_x[n, :, :, i] + train_x[n, :, :, i + 1]) / 2.\n",
        "\n",
        "train_x = img2\n",
        "\n",
        "print(train_x.shape)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3wrK8dNe-T7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b224d2ea-c04c-4330-9245-e251e6eba885"
      },
      "source": [
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3]))\n",
        "\n",
        "temp = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], 10))\n",
        "\n",
        "#temp[:, :, :, 0] = train_x[:, :, :, 2]\n",
        "temp[:, :, :, 0] = train_x[:, :, :, 9]\n",
        "#temp[:, :, :, 2] = train_x[:, :, :, 10]\n",
        "temp[:, :, :, 1] = train_x[:, :, :, 36]\n",
        "#temp[:, :, :, 4] = train_x[:, :, :, 37]\n",
        "#temp[:, :, :,5] = train_x[:, :, :, 43]\n",
        "temp[:, :, :,2] = train_x[:, :, :,55]\n",
        "temp[:, :, :,3] = train_x[:, :, :, 66]\n",
        "temp[:, :, :,4] = train_x[:, :, :, 101]\n",
        "#temp[:, :, :, 9] = train_x[:, :, :, 105]\n",
        "temp[:, :, :, 5] = train_x[:, :, :, 106]\n",
        "#temp[:, :, :, 11] = train_x[:, :, :, 107]\n",
        "#temp[:, :, :, 12] = train_x[:, :, :, 111]\n",
        "temp[:, :, :, 6] = train_x[:, :, :, 113]\n",
        "#temp[:, :, :, 14] = train_x[:, :, :, 114]\n",
        "temp[:, :, :, 7] = train_x[:, :, :, 121]\n",
        "#temp[:, :, :, 16] = train_x[:, :, :, 122]\n",
        "temp[:, :, :, 8] = train_x[:, :, :, 131]\n",
        "#temp[:, :, :, 18] = train_x[:, :, :, 132]\n",
        "temp[:, :, :, 9] = train_x[:, :, :, 144]\n",
        "\n",
        "train_x = temp\n",
        "\n",
        "print(train_x.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvx55MBgcrZJ",
        "colab_type": "text"
      },
      "source": [
        "# Trained pruned network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2xU6uWWAuz",
        "colab_type": "code",
        "outputId": "60f0f62a-73a1-46ea-e46e-d90c3580add5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "data = 'WEED'\n",
        "loaded_model = tf.keras.models.load_model(\"selected10-weights-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE2WVq7DItg",
        "colab_type": "code",
        "outputId": "4d1dbb53-3235-4707-f3d3-9b10e03abfa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                   final_sparsity=0.90,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adadelta',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "990\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.debugging.assert_greater_equal is deprecated. Please use tf.compat.v1.debugging.assert_greater_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py:59: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 10)]      0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 25, 25, 128)       2940      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 128)       513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 128)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 13, 13, 128)       34050     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 13, 13, 128)       513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 13, 13, 128)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 7, 7, 128)         34050     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_global_a (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_fc3 (Pru (None, 3)                 773       \n",
            "=================================================================\n",
            "Total params: 72,843\n",
            "Trainable params: 37,885\n",
            "Non-trainable params: 34,958\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYBXiSTtaDt",
        "colab_type": "code",
        "outputId": "aa4bafb8-01f4-43ab-b37f-13239da9ca24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "cva1 = []\n",
        "cva2 = []\n",
        "cva3 = []\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "data = 'WEED'\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "\n",
        "    ytrain = tf.keras.utils.to_categorical(train_y[train]).astype(np.int32)\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.01,\n",
        "                                                      final_sparsity=0.02,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    loaded_model.load_weights(\"selected10-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "    #new_pruned_model.summary()\n",
        "\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.categorical_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # checkpoint\n",
        "    filepath = \"pruned10-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint, sparsity.UpdatePruningStep()]\n",
        "\n",
        "    # Train model on dataset\n",
        "    print(data + \": Training\" + str(ntrain) + \"begins...\")\n",
        "    history = new_pruned_model.fit(x=train_x[train], y=ytrain, validation_data=(train_x[test], ytest),\n",
        "                        batch_size=32, epochs=epochs, callbacks=callbacks_list)\n",
        "    \n",
        "    new_pruned_model.load_weights(\"pruned10-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "    print(np.sum(ypred*ytest)/len(ytest))\n",
        "    \n",
        "    ntrain += 1\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WEED: Training1begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9925\n",
            "Epoch 00001: val_acc improved from -inf to 0.98101, saving model to pruned10-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 757us/sample - loss: 0.0345 - acc: 0.9924 - val_loss: 0.1222 - val_acc: 0.9810\n",
            "Epoch 2/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9922\n",
            "Epoch 00002: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 405us/sample - loss: 0.0313 - acc: 0.9917 - val_loss: 0.1182 - val_acc: 0.9794\n",
            "Epoch 3/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9904\n",
            "Epoch 00003: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 422us/sample - loss: 0.0352 - acc: 0.9903 - val_loss: 0.1195 - val_acc: 0.9810\n",
            "Epoch 4/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9913\n",
            "Epoch 00004: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 396us/sample - loss: 0.0224 - acc: 0.9914 - val_loss: 0.1176 - val_acc: 0.9794\n",
            "Epoch 5/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9913\n",
            "Epoch 00005: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 399us/sample - loss: 0.0264 - acc: 0.9914 - val_loss: 0.1169 - val_acc: 0.9794\n",
            "Epoch 6/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9894\n",
            "Epoch 00006: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 403us/sample - loss: 0.0340 - acc: 0.9893 - val_loss: 0.1164 - val_acc: 0.9810\n",
            "Epoch 7/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9915\n",
            "Epoch 00007: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 418us/sample - loss: 0.0255 - acc: 0.9916 - val_loss: 0.1173 - val_acc: 0.9810\n",
            "Epoch 8/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9937\n",
            "Epoch 00008: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 401us/sample - loss: 0.0213 - acc: 0.9938 - val_loss: 0.1171 - val_acc: 0.9810\n",
            "Epoch 9/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9906\n",
            "Epoch 00009: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 411us/sample - loss: 0.0353 - acc: 0.9903 - val_loss: 0.1157 - val_acc: 0.9810\n",
            "Epoch 10/10\n",
            "5536/5684 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9917\n",
            "Epoch 00010: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 2s 408us/sample - loss: 0.0328 - acc: 0.9914 - val_loss: 0.1162 - val_acc: 0.9810\n",
            "0.9769327026481479\n",
            "WEED: Training2begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9918\n",
            "Epoch 00001: val_acc improved from -inf to 0.96519, saving model to pruned10-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 7s 1ms/sample - loss: 0.0236 - acc: 0.9919 - val_loss: 0.2189 - val_acc: 0.9652\n",
            "Epoch 2/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9931\n",
            "Epoch 00002: val_acc improved from 0.96519 to 0.97152, saving model to pruned10-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 5s 835us/sample - loss: 0.0288 - acc: 0.9926 - val_loss: 0.1998 - val_acc: 0.9715\n",
            "Epoch 3/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9943\n",
            "Epoch 00003: val_acc improved from 0.97152 to 0.97627, saving model to pruned10-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 5s 815us/sample - loss: 0.0258 - acc: 0.9944 - val_loss: 0.1902 - val_acc: 0.9763\n",
            "Epoch 4/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9952\n",
            "Epoch 00004: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 5s 810us/sample - loss: 0.0227 - acc: 0.9949 - val_loss: 0.1936 - val_acc: 0.9747\n",
            "Epoch 5/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9912\n",
            "Epoch 00005: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 4s 789us/sample - loss: 0.0389 - acc: 0.9910 - val_loss: 0.1949 - val_acc: 0.9763\n",
            "Epoch 6/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9950\n",
            "Epoch 00006: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 4s 767us/sample - loss: 0.0142 - acc: 0.9951 - val_loss: 0.1949 - val_acc: 0.9747\n",
            "Epoch 7/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9937\n",
            "Epoch 00007: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 4s 779us/sample - loss: 0.0178 - acc: 0.9938 - val_loss: 0.1937 - val_acc: 0.9763\n",
            "Epoch 8/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9917\n",
            "Epoch 00008: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 5s 801us/sample - loss: 0.0253 - acc: 0.9917 - val_loss: 0.1961 - val_acc: 0.9747\n",
            "Epoch 9/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9952\n",
            "Epoch 00009: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 4s 786us/sample - loss: 0.0227 - acc: 0.9952 - val_loss: 0.1953 - val_acc: 0.9763\n",
            "Epoch 10/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9915\n",
            "Epoch 00010: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 5s 804us/sample - loss: 0.0247 - acc: 0.9916 - val_loss: 0.1972 - val_acc: 0.9747\n",
            "0.9713325116114284\n",
            "WEED: Training3begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9906\n",
            "Epoch 00001: val_acc improved from -inf to 0.97943, saving model to pruned10-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0422 - acc: 0.9905 - val_loss: 0.1190 - val_acc: 0.9794\n",
            "Epoch 2/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9915\n",
            "Epoch 00002: val_acc improved from 0.97943 to 0.98259, saving model to pruned10-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 767us/sample - loss: 0.0258 - acc: 0.9914 - val_loss: 0.1061 - val_acc: 0.9826\n",
            "Epoch 3/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9895\n",
            "Epoch 00003: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 5s 794us/sample - loss: 0.0378 - acc: 0.9894 - val_loss: 0.1106 - val_acc: 0.9810\n",
            "Epoch 4/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9893\n",
            "Epoch 00004: val_acc improved from 0.98259 to 0.98576, saving model to pruned10-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 761us/sample - loss: 0.0429 - acc: 0.9891 - val_loss: 0.1031 - val_acc: 0.9858\n",
            "Epoch 5/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9906\n",
            "Epoch 00005: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 4s 758us/sample - loss: 0.0285 - acc: 0.9907 - val_loss: 0.1028 - val_acc: 0.9858\n",
            "Epoch 6/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9899\n",
            "Epoch 00006: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 4s 763us/sample - loss: 0.0365 - acc: 0.9900 - val_loss: 0.1011 - val_acc: 0.9858\n",
            "Epoch 7/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9898\n",
            "Epoch 00007: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 4s 742us/sample - loss: 0.0304 - acc: 0.9898 - val_loss: 0.1018 - val_acc: 0.9842\n",
            "Epoch 8/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9899\n",
            "Epoch 00008: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 4s 756us/sample - loss: 0.0410 - acc: 0.9894 - val_loss: 0.1010 - val_acc: 0.9858\n",
            "Epoch 9/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9908\n",
            "Epoch 00009: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 4s 752us/sample - loss: 0.0326 - acc: 0.9909 - val_loss: 0.1023 - val_acc: 0.9858\n",
            "Epoch 10/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9898\n",
            "Epoch 00010: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 4s 742us/sample - loss: 0.0333 - acc: 0.9898 - val_loss: 0.1006 - val_acc: 0.9858\n",
            "0.9836225997261305\n",
            "WEED: Training4begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9835\n",
            "Epoch 00001: val_acc improved from -inf to 0.97152, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 773us/sample - loss: 0.0490 - acc: 0.9836 - val_loss: 0.1307 - val_acc: 0.9715\n",
            "Epoch 2/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9782\n",
            "Epoch 00002: val_acc improved from 0.97152 to 0.97627, saving model to pruned10-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 397us/sample - loss: 0.0681 - acc: 0.9777 - val_loss: 0.1147 - val_acc: 0.9763\n",
            "Epoch 3/10\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9844\n",
            "Epoch 00003: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 2s 389us/sample - loss: 0.0513 - acc: 0.9842 - val_loss: 0.1123 - val_acc: 0.9747\n",
            "Epoch 4/10\n",
            "5536/5684 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9814\n",
            "Epoch 00004: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 2s 399us/sample - loss: 0.0563 - acc: 0.9817 - val_loss: 0.1123 - val_acc: 0.9763\n",
            "Epoch 5/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9828\n",
            "Epoch 00005: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 2s 390us/sample - loss: 0.0486 - acc: 0.9826 - val_loss: 0.1132 - val_acc: 0.9747\n",
            "Epoch 6/10\n",
            "5536/5684 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9819\n",
            "Epoch 00006: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 2s 402us/sample - loss: 0.0535 - acc: 0.9822 - val_loss: 0.1135 - val_acc: 0.9747\n",
            "Epoch 7/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9857\n",
            "Epoch 00007: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 2s 390us/sample - loss: 0.0450 - acc: 0.9856 - val_loss: 0.1127 - val_acc: 0.9763\n",
            "Epoch 8/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9857\n",
            "Epoch 00008: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 2s 392us/sample - loss: 0.0472 - acc: 0.9857 - val_loss: 0.1145 - val_acc: 0.9763\n",
            "Epoch 9/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9811\n",
            "Epoch 00009: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 2s 393us/sample - loss: 0.0580 - acc: 0.9808 - val_loss: 0.1138 - val_acc: 0.9763\n",
            "Epoch 10/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9861\n",
            "Epoch 00010: val_acc did not improve from 0.97627\n",
            "5684/5684 [==============================] - 2s 385us/sample - loss: 0.0495 - acc: 0.9857 - val_loss: 0.1147 - val_acc: 0.9731\n",
            "0.968009174877899\n",
            "WEED: Training5begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9898\n",
            "Epoch 00001: val_acc improved from -inf to 0.97310, saving model to pruned10-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0380 - acc: 0.9898 - val_loss: 0.1535 - val_acc: 0.9731\n",
            "Epoch 2/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9943\n",
            "Epoch 00002: val_acc improved from 0.97310 to 0.98101, saving model to pruned10-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 697us/sample - loss: 0.0221 - acc: 0.9944 - val_loss: 0.1398 - val_acc: 0.9810\n",
            "Epoch 3/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9896\n",
            "Epoch 00003: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 4s 669us/sample - loss: 0.0369 - acc: 0.9896 - val_loss: 0.1420 - val_acc: 0.9794\n",
            "Epoch 4/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9896\n",
            "Epoch 00004: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 4s 672us/sample - loss: 0.0460 - acc: 0.9898 - val_loss: 0.1425 - val_acc: 0.9794\n",
            "Epoch 5/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9921\n",
            "Epoch 00005: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 4s 670us/sample - loss: 0.0258 - acc: 0.9921 - val_loss: 0.1426 - val_acc: 0.9778\n",
            "Epoch 6/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9913\n",
            "Epoch 00006: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 4s 672us/sample - loss: 0.0297 - acc: 0.9914 - val_loss: 0.1413 - val_acc: 0.9794\n",
            "Epoch 7/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9910\n",
            "Epoch 00007: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 4s 665us/sample - loss: 0.0286 - acc: 0.9910 - val_loss: 0.1419 - val_acc: 0.9794\n",
            "Epoch 8/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9917\n",
            "Epoch 00008: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 4s 655us/sample - loss: 0.0288 - acc: 0.9916 - val_loss: 0.1433 - val_acc: 0.9778\n",
            "Epoch 9/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9925\n",
            "Epoch 00009: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 4s 668us/sample - loss: 0.0252 - acc: 0.9923 - val_loss: 0.1410 - val_acc: 0.9810\n",
            "Epoch 10/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9905\n",
            "Epoch 00010: val_acc did not improve from 0.98101\n",
            "5684/5684 [==============================] - 4s 671us/sample - loss: 0.0342 - acc: 0.9905 - val_loss: 0.1425 - val_acc: 0.9763\n",
            "0.9770368444222607\n",
            "WEED: Training6begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9924\n",
            "Epoch 00001: val_acc improved from -inf to 0.98259, saving model to pruned10-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 978us/sample - loss: 0.0333 - acc: 0.9924 - val_loss: 0.0600 - val_acc: 0.9826\n",
            "Epoch 2/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9933\n",
            "Epoch 00002: val_acc did not improve from 0.98259\n",
            "5684/5684 [==============================] - 4s 630us/sample - loss: 0.0238 - acc: 0.9933 - val_loss: 0.0559 - val_acc: 0.9810\n",
            "Epoch 3/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9928\n",
            "Epoch 00003: val_acc improved from 0.98259 to 0.98418, saving model to pruned10-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 628us/sample - loss: 0.0270 - acc: 0.9928 - val_loss: 0.0578 - val_acc: 0.9842\n",
            "Epoch 4/10\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9918\n",
            "Epoch 00004: val_acc did not improve from 0.98418\n",
            "5684/5684 [==============================] - 4s 654us/sample - loss: 0.0316 - acc: 0.9917 - val_loss: 0.0586 - val_acc: 0.9826\n",
            "Epoch 5/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9923\n",
            "Epoch 00005: val_acc did not improve from 0.98418\n",
            "5684/5684 [==============================] - 4s 619us/sample - loss: 0.0306 - acc: 0.9924 - val_loss: 0.0592 - val_acc: 0.9826\n",
            "Epoch 6/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9923\n",
            "Epoch 00006: val_acc did not improve from 0.98418\n",
            "5684/5684 [==============================] - 3s 615us/sample - loss: 0.0275 - acc: 0.9924 - val_loss: 0.0575 - val_acc: 0.9842\n",
            "Epoch 7/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9907\n",
            "Epoch 00007: val_acc did not improve from 0.98418\n",
            "5684/5684 [==============================] - 4s 639us/sample - loss: 0.0421 - acc: 0.9905 - val_loss: 0.0595 - val_acc: 0.9826\n",
            "Epoch 8/10\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9951\n",
            "Epoch 00008: val_acc did not improve from 0.98418\n",
            "5684/5684 [==============================] - 4s 639us/sample - loss: 0.0200 - acc: 0.9951 - val_loss: 0.0588 - val_acc: 0.9826\n",
            "Epoch 9/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9927\n",
            "Epoch 00009: val_acc did not improve from 0.98418\n",
            "5684/5684 [==============================] - 4s 649us/sample - loss: 0.0266 - acc: 0.9926 - val_loss: 0.0628 - val_acc: 0.9810\n",
            "Epoch 10/10\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9943\n",
            "Epoch 00010: val_acc did not improve from 0.98418\n",
            "5684/5684 [==============================] - 4s 622us/sample - loss: 0.0226 - acc: 0.9940 - val_loss: 0.0584 - val_acc: 0.9810\n",
            "0.9817536349702107\n",
            "WEED: Training7begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9800\n",
            "Epoch 00001: val_acc improved from -inf to 0.98415, saving model to pruned10-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 6s 988us/sample - loss: 0.0636 - acc: 0.9799 - val_loss: 0.0520 - val_acc: 0.9842\n",
            "Epoch 2/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9854\n",
            "Epoch 00002: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 3s 606us/sample - loss: 0.0442 - acc: 0.9856 - val_loss: 0.0429 - val_acc: 0.9842\n",
            "Epoch 3/10\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9847\n",
            "Epoch 00003: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 4s 633us/sample - loss: 0.0503 - acc: 0.9849 - val_loss: 0.0418 - val_acc: 0.9842\n",
            "Epoch 4/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9875\n",
            "Epoch 00004: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 4s 616us/sample - loss: 0.0357 - acc: 0.9877 - val_loss: 0.0412 - val_acc: 0.9842\n",
            "Epoch 5/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9866\n",
            "Epoch 00005: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 3s 612us/sample - loss: 0.0457 - acc: 0.9865 - val_loss: 0.0413 - val_acc: 0.9842\n",
            "Epoch 6/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9882\n",
            "Epoch 00006: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 3s 611us/sample - loss: 0.0394 - acc: 0.9879 - val_loss: 0.0422 - val_acc: 0.9842\n",
            "Epoch 7/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9857\n",
            "Epoch 00007: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 4s 626us/sample - loss: 0.0500 - acc: 0.9856 - val_loss: 0.0422 - val_acc: 0.9842\n",
            "Epoch 8/10\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9837\n",
            "Epoch 00008: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 3s 612us/sample - loss: 0.0542 - acc: 0.9836 - val_loss: 0.0409 - val_acc: 0.9842\n",
            "Epoch 9/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9846\n",
            "Epoch 00009: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 3s 613us/sample - loss: 0.0478 - acc: 0.9847 - val_loss: 0.0418 - val_acc: 0.9842\n",
            "Epoch 10/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9846\n",
            "Epoch 00010: val_acc did not improve from 0.98415\n",
            "5685/5685 [==============================] - 4s 626us/sample - loss: 0.0448 - acc: 0.9845 - val_loss: 0.0412 - val_acc: 0.9826\n",
            "0.9809399092198746\n",
            "WEED: Training8begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9878\n",
            "Epoch 00001: val_acc improved from -inf to 0.97306, saving model to pruned10-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 5s 893us/sample - loss: 0.0561 - acc: 0.9877 - val_loss: 0.1373 - val_acc: 0.9731\n",
            "Epoch 2/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9889\n",
            "Epoch 00002: val_acc improved from 0.97306 to 0.98098, saving model to pruned10-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 569us/sample - loss: 0.0431 - acc: 0.9889 - val_loss: 0.1359 - val_acc: 0.9810\n",
            "Epoch 3/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9883\n",
            "Epoch 00003: val_acc did not improve from 0.98098\n",
            "5685/5685 [==============================] - 3s 561us/sample - loss: 0.0554 - acc: 0.9882 - val_loss: 0.1395 - val_acc: 0.9794\n",
            "Epoch 4/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9907\n",
            "Epoch 00004: val_acc did not improve from 0.98098\n",
            "5685/5685 [==============================] - 3s 540us/sample - loss: 0.0399 - acc: 0.9907 - val_loss: 0.1395 - val_acc: 0.9794\n",
            "Epoch 5/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9909\n",
            "Epoch 00005: val_acc did not improve from 0.98098\n",
            "5685/5685 [==============================] - 3s 542us/sample - loss: 0.0360 - acc: 0.9910 - val_loss: 0.1430 - val_acc: 0.9810\n",
            "Epoch 6/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9889\n",
            "Epoch 00006: val_acc did not improve from 0.98098\n",
            "5685/5685 [==============================] - 3s 542us/sample - loss: 0.0522 - acc: 0.9887 - val_loss: 0.1410 - val_acc: 0.9810\n",
            "Epoch 7/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9869\n",
            "Epoch 00007: val_acc did not improve from 0.98098\n",
            "5685/5685 [==============================] - 3s 595us/sample - loss: 0.0568 - acc: 0.9870 - val_loss: 0.1397 - val_acc: 0.9794\n",
            "Epoch 8/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9895\n",
            "Epoch 00008: val_acc did not improve from 0.98098\n",
            "5685/5685 [==============================] - 3s 587us/sample - loss: 0.0496 - acc: 0.9894 - val_loss: 0.1376 - val_acc: 0.9794\n",
            "Epoch 9/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9895\n",
            "Epoch 00009: val_acc did not improve from 0.98098\n",
            "5685/5685 [==============================] - 3s 588us/sample - loss: 0.0415 - acc: 0.9894 - val_loss: 0.1396 - val_acc: 0.9810\n",
            "Epoch 10/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9905\n",
            "Epoch 00010: val_acc did not improve from 0.98098\n",
            "5685/5685 [==============================] - 3s 566us/sample - loss: 0.0453 - acc: 0.9907 - val_loss: 0.1400 - val_acc: 0.9794\n",
            "0.9795386630709568\n",
            "WEED: Training9begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9825\n",
            "Epoch 00001: val_acc improved from -inf to 0.96513, saving model to pruned10-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 5s 863us/sample - loss: 0.0556 - acc: 0.9826 - val_loss: 0.1484 - val_acc: 0.9651\n",
            "Epoch 2/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9805\n",
            "Epoch 00002: val_acc improved from 0.96513 to 0.96989, saving model to pruned10-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 503us/sample - loss: 0.0623 - acc: 0.9805 - val_loss: 0.1317 - val_acc: 0.9699\n",
            "Epoch 3/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9838\n",
            "Epoch 00003: val_acc improved from 0.96989 to 0.97147, saving model to pruned10-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 520us/sample - loss: 0.0431 - acc: 0.9836 - val_loss: 0.1318 - val_acc: 0.9715\n",
            "Epoch 4/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9845\n",
            "Epoch 00004: val_acc did not improve from 0.97147\n",
            "5685/5685 [==============================] - 3s 496us/sample - loss: 0.0476 - acc: 0.9845 - val_loss: 0.1305 - val_acc: 0.9683\n",
            "Epoch 5/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9855\n",
            "Epoch 00005: val_acc did not improve from 0.97147\n",
            "5685/5685 [==============================] - 3s 496us/sample - loss: 0.0444 - acc: 0.9854 - val_loss: 0.1326 - val_acc: 0.9683\n",
            "Epoch 6/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9802\n",
            "Epoch 00006: val_acc did not improve from 0.97147\n",
            "5685/5685 [==============================] - 3s 541us/sample - loss: 0.0520 - acc: 0.9803 - val_loss: 0.1330 - val_acc: 0.9699\n",
            "Epoch 7/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9822\n",
            "Epoch 00007: val_acc did not improve from 0.97147\n",
            "5685/5685 [==============================] - 3s 588us/sample - loss: 0.0532 - acc: 0.9822 - val_loss: 0.1310 - val_acc: 0.9699\n",
            "Epoch 8/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9838\n",
            "Epoch 00008: val_acc did not improve from 0.97147\n",
            "5685/5685 [==============================] - 3s 550us/sample - loss: 0.0464 - acc: 0.9838 - val_loss: 0.1327 - val_acc: 0.9699\n",
            "Epoch 9/10\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9837\n",
            "Epoch 00009: val_acc did not improve from 0.97147\n",
            "5685/5685 [==============================] - 3s 568us/sample - loss: 0.0484 - acc: 0.9836 - val_loss: 0.1328 - val_acc: 0.9699\n",
            "Epoch 10/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9838\n",
            "Epoch 00010: val_acc did not improve from 0.97147\n",
            "5685/5685 [==============================] - 3s 575us/sample - loss: 0.0475 - acc: 0.9838 - val_loss: 0.1321 - val_acc: 0.9699\n",
            "0.9668639361576946\n",
            "WEED: Training10begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9869\n",
            "Epoch 00001: val_acc improved from -inf to 0.97306, saving model to pruned10-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 4s 783us/sample - loss: 0.0461 - acc: 0.9870 - val_loss: 0.1351 - val_acc: 0.9731\n",
            "Epoch 2/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9921\n",
            "Epoch 00002: val_acc improved from 0.97306 to 0.97781, saving model to pruned10-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 445us/sample - loss: 0.0288 - acc: 0.9921 - val_loss: 0.1240 - val_acc: 0.9778\n",
            "Epoch 3/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9890\n",
            "Epoch 00003: val_acc did not improve from 0.97781\n",
            "5685/5685 [==============================] - 3s 462us/sample - loss: 0.0345 - acc: 0.9891 - val_loss: 0.1309 - val_acc: 0.9746\n",
            "Epoch 4/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9878\n",
            "Epoch 00004: val_acc did not improve from 0.97781\n",
            "5685/5685 [==============================] - 2s 426us/sample - loss: 0.0444 - acc: 0.9879 - val_loss: 0.1262 - val_acc: 0.9746\n",
            "Epoch 5/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9873\n",
            "Epoch 00005: val_acc did not improve from 0.97781\n",
            "5685/5685 [==============================] - 3s 466us/sample - loss: 0.0451 - acc: 0.9872 - val_loss: 0.1346 - val_acc: 0.9731\n",
            "Epoch 6/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9905\n",
            "Epoch 00006: val_acc did not improve from 0.97781\n",
            "5685/5685 [==============================] - 2s 436us/sample - loss: 0.0325 - acc: 0.9905 - val_loss: 0.1274 - val_acc: 0.9746\n",
            "Epoch 7/10\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9864\n",
            "Epoch 00007: val_acc did not improve from 0.97781\n",
            "5685/5685 [==============================] - 2s 436us/sample - loss: 0.0468 - acc: 0.9865 - val_loss: 0.1254 - val_acc: 0.9762\n",
            "Epoch 8/10\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9883\n",
            "Epoch 00008: val_acc did not improve from 0.97781\n",
            "5685/5685 [==============================] - 3s 443us/sample - loss: 0.0423 - acc: 0.9884 - val_loss: 0.1254 - val_acc: 0.9746\n",
            "Epoch 9/10\n",
            "5536/5685 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9910\n",
            "Epoch 00009: val_acc did not improve from 0.97781\n",
            "5685/5685 [==============================] - 2s 430us/sample - loss: 0.0303 - acc: 0.9912 - val_loss: 0.1280 - val_acc: 0.9746\n",
            "Epoch 10/10\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9899\n",
            "Epoch 00010: val_acc did not improve from 0.97781\n",
            "5685/5685 [==============================] - 2s 435us/sample - loss: 0.0359 - acc: 0.9900 - val_loss: 0.1267 - val_acc: 0.9746\n",
            "0.9735461980919329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmz9PSRTvPP",
        "colab_type": "code",
        "outputId": "0d8423c2-c107-4361-de1d-b9596c7ded71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "new_pruned_model.load_weights(\"pruned10-weights-hyper3dnet\" + data + str(3) + \"-best_3layers_4filters.h5\")\n",
        "final_model = sparsity.strip_pruning(new_pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 10)]      0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_8 (Separabl (None, 25, 25, 128)       1658      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 25, 25, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 25, 25, 128)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_9 (Separabl (None, 13, 13, 128)       17664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_10 (Separab (None, 7, 7, 128)         17664     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 38,397\n",
            "Trainable params: 37,885\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8drkv1mK1CUb",
        "colab_type": "code",
        "outputId": "47085d5b-ddd1-4fab-8c0e-402c02a29e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "import numpy as np\n",
        "for i, w in enumerate(final_model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.4f}%\".format(\n",
        "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "separable_conv2d_8/depthwise_kernel:0 -- Total:250, Zeros: 0.0000%\n",
            "separable_conv2d_8/pointwise_kernel:0 -- Total:1280, Zeros: 1.9531%\n",
            "separable_conv2d_8/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "separable_conv2d_9/depthwise_kernel:0 -- Total:1152, Zeros: 0.0000%\n",
            "separable_conv2d_9/pointwise_kernel:0 -- Total:16384, Zeros: 1.9775%\n",
            "separable_conv2d_9/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "separable_conv2d_10/depthwise_kernel:0 -- Total:1152, Zeros: 0.0000%\n",
            "separable_conv2d_10/pointwise_kernel:0 -- Total:16384, Zeros: 1.9775%\n",
            "separable_conv2d_10/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "fc3/kernel:0 -- Total:384, Zeros: 2.0833%\n",
            "fc3/bias:0 -- Total:3, Zeros: 0.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIRfZDn02rV",
        "colab_type": "code",
        "outputId": "0c6aefdd-1b1d-49f0-b9e7-07c38a72794c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Saving pruned model to: ', \"Kochia_hyper3DNet_pruned.h5\")\n",
        "tf.keras.models.save_model(final_model, \"Kochia_hyper3DNet_10bands_pruned.h5\", \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  Kochia_hyper3DNet_pruned.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tkgm44OZDt",
        "colab_type": "text"
      },
      "source": [
        "# Predict and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVT1KBWvEODg",
        "colab_type": "code",
        "outputId": "300bc965-8441-4a73-80fe-0f54708fd693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import truediv\n",
        "import h5py\n",
        "import pickle\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Prepare lists to save metrics\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_m):\n",
        "    list_diag = np.diag(confusion_m)\n",
        "    list_raw_sum = np.sum(confusion_m, axis=1)\n",
        "    each_ac = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_ac)\n",
        "    return each_ac, average_acc\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "PREDICT AND CALCULATE METRICS\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "loaded_model = tf.keras.models.load_model(\"selected10-weights-hyper3dnet\" + 'WEED' + str(1) + \"-best_3layers_4filters.h5\")\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "dataset = 'WEED'\n",
        "# Initialize\n",
        "confmatrices = np.zeros((10, int(classes), int(classes)))\n",
        "\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.30,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.load_weights(\"pruned10-weights-hyper3dnet\" + dataset + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        con_mat = tf.math.confusion_matrix(labels=np.argmax(ytest, axis=-1),\n",
        "                                           predictions=np.argmax(ypred, axis=-1)).numpy()\n",
        "\n",
        "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "    classes_list = list(range(0, int(classes)))\n",
        "    con_mat_df = pd.DataFrame(con_mat_norm, index=classes_list, columns=classes_list)\n",
        "\n",
        "    confmatrices[ntrain - 1, :, :] = con_mat_df.values\n",
        "\n",
        "    # Calculate metrics\n",
        "    oa = accuracy_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    confusion = confusion_matrix(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    prec, rec, f1, support = precision_recall_fscore_support(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1),\n",
        "                                                             average='macro')\n",
        "\n",
        "    # Add metrics to the list\n",
        "    cvoa.append(oa * 100)\n",
        "    cvaa.append(aa * 100)\n",
        "    cvka.append(kappa * 100)\n",
        "    cvpre.append(prec * 100)\n",
        "    cvrec.append(rec * 100)\n",
        "    cvf1.append(f1 * 100)\n",
        "\n",
        "    ntrain += 1\n",
        "\n",
        "file_name = \"classification_report_hyper3dnet_pruned10\" + dataset + \".txt\"\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write(\"Overall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvoa)), float(np.std(cvoa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Average accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvaa)), float(np.std(cvaa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Kappa accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvka)), float(np.std(cvka))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Precision accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvpre)), float(np.std(cvpre))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Recall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvrec)), float(np.std(cvrec))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"F1 accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvf1)), float(np.std(cvf1))))\n",
        "\n",
        "# Calculate mean and std\n",
        "means = np.mean(confmatrices * 100, axis=0)\n",
        "stds = np.std(confmatrices * 100, axis=0)\n",
        "\n",
        "def plot_confusion_matrix(cm, cms, classescf,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classescf))\n",
        "    plt.xticks(tick_marks, classescf, rotation=45)\n",
        "    plt.yticks(tick_marks, classescf)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "\n",
        "            if (cm[i, j] == 100 or cm[i, j] == 0) and cms[i, j] == 0:\n",
        "                plt.text(j, i, '{0:.0f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.0f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "            else:\n",
        "                plt.text(j, i, '{0:.2f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.2f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "with open('meanshyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(means, f)\n",
        "with open('stdshyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(stds, f)\n",
        "with open('cvf1hyper3dnet10p', 'wb') as f:\n",
        "    pickle.dump(cvf1, f)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "classes_list = list(range(0, int(3)))\n",
        "plt.figure()\n",
        "plot_confusion_matrix(means, stds, classescf=classes_list)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAEmCAYAAAA5jbhCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}