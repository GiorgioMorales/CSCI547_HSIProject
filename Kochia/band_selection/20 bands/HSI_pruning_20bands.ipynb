{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HSI_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyQTRzMcUCB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare tensorflow-model-optimization for pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNPotwubYRu",
        "colab_type": "code",
        "outputId": "1550bdf7-4dc3-4caf-e91b-fbf7eb07c290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tensorflow-gpu==1.14.0\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 44.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c4/4c3d011e432bd9c19f0323f7da7d3f783402615e4c3b5a98416c7da9cb05/tensorflow_model_optimization-0.2.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.17.5)\n",
            "Collecting enum34~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/df/6e/46a0304561a07bda942c4283b67228d5b87d1ebd1189c866ede4b4cd6a0a/enum34-1.1.9-py3-none-any.whl\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.9 tensorflow-model-optimization-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfBLXz1mCNV",
        "colab_type": "code",
        "outputId": "ec8a61f7-ba8f-4c9e-fd2e-3083388f68ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk1dYHpbicZ",
        "colab_type": "text"
      },
      "source": [
        "#Download dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu64LC4Wm1oc",
        "colab_type": "code",
        "outputId": "58e279dd-b3d3-432c-da00-7a5d0e31d58d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWT-iIqSb_46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8/view?usp=sharing\n",
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8\n",
        "download = drive.CreateFile({'id': '1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8'})\n",
        "download.GetContentFile('weed_dataset_w25.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDlBcilciuY",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRqt2u-MVfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from operator import truediv\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syCzlwNcr-di",
        "colab_type": "code",
        "outputId": "1b2008a7-ce87-4500-ce3b-3d39efe358ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "LOAD HDF5 FILE\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "hdf5_file = h5py.File('weed_dataset_w25.hdf5', \"r\")\n",
        "train_x = np.array(hdf5_file[\"train_img\"][...])\n",
        "# train_x = train_x / np.max(train_x)\n",
        "# train_x = np.clip(train_x, 0, 1)\n",
        "#train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))\n",
        "train_y = np.array(hdf5_file[\"train_labels\"][...])\n",
        "\n",
        "# Average consecutive bands\n",
        "img2 = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], int(train_x.shape[3]/2)))\n",
        "for n in range(0, train_x.shape[0]):\n",
        "    # Average consecutive bands\n",
        "    for i in range(0, train_x.shape[3], 2):\n",
        "        img2[n, :, :, int(i/2)] = (train_x[n, :, :, i] + train_x[n, :, :, i + 1]) / 2.\n",
        "\n",
        "train_x = img2\n",
        "\n",
        "print(train_x.shape)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3wrK8dNe-T7",
        "colab_type": "code",
        "outputId": "73677d37-4893-4527-fe1e-8404431b2df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3]))\n",
        "\n",
        "temp = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], 20))\n",
        "\n",
        "temp[:, :, :, 0] = train_x[:, :, :, 10]\n",
        "temp[:, :, :, 1] = train_x[:, :, :, 15] \n",
        "temp[:, :, :, 2] = train_x[:, :, :,20] \n",
        "temp[:, :, :, 3] = train_x[:, :, :,22] \n",
        "temp[:, :, :, 4] = train_x[:, :, :,24] \n",
        "temp[:, :, :, 5] = train_x[:, :, :, 27] \n",
        "temp[:, :, :, 6] = train_x[:, :, :, 30]\n",
        "temp[:, :, :, 7] = train_x[:, :, :, 40]\n",
        "temp[:, :, :, 8] = train_x[:, :, :,55]\n",
        "temp[:, :, :, 9] = train_x[:, :, :, 36]  \n",
        "temp[:, :, :, 10] = train_x[:, :, :, 59] \n",
        "temp[:, :, :, 11] = train_x[:, :, :, 61]    \n",
        "temp[:, :, :, 12] = train_x[:, :, :, 66] \n",
        "temp[:, :, :, 13] = train_x[:, :, :, 73]\n",
        "temp[:, :, :, 14] = train_x[:, :, :, 114]\n",
        "temp[:, :, :, 15] = train_x[:, :, :, 128]\n",
        "temp[:, :, :, 16] = train_x[:, :, :, 131]\n",
        "temp[:, :, :, 17] = train_x[:, :, :, 136] \n",
        "temp[:, :, :, 18] = train_x[:, :, :, 141]\n",
        "temp[:, :, :, 19] = train_x[:, :, :, 144]\n",
        "train_x = temp\n",
        "\n",
        "print(train_x.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvx55MBgcrZJ",
        "colab_type": "text"
      },
      "source": [
        "# Trained pruned network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2xU6uWWAuz",
        "colab_type": "code",
        "outputId": "87267328-d1cd-4909-dadb-968f883bb718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data = 'WEED'\n",
        "loaded_model = tf.keras.models.load_model(\"selected20-weights-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE2WVq7DItg",
        "colab_type": "code",
        "outputId": "bea34409-b858-4681-93a0-0407e6b835af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.5,\n",
        "                                                   final_sparsity=0.9,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adadelta',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2970\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.debugging.assert_greater_equal is deprecated. Please use tf.compat.v1.debugging.assert_greater_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py:59: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 20)]      0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 25, 25, 128)       5750      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 128)       513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 128)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 13, 13, 128)       34050     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 13, 13, 128)       513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 13, 13, 128)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 7, 7, 128)         34050     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_global_a (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_fc3 (Pru (None, 3)                 773       \n",
            "=================================================================\n",
            "Total params: 75,653\n",
            "Trainable params: 39,415\n",
            "Non-trainable params: 36,238\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYBXiSTtaDt",
        "colab_type": "code",
        "outputId": "705ea5fa-b3c8-4799-9b37-081aee03c6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "cva1 = []\n",
        "cva2 = []\n",
        "cva3 = []\n",
        "\n",
        "\n",
        "data = 'WEED'\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "\n",
        "    ytrain = tf.keras.utils.to_categorical(train_y[train]).astype(np.int32)\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.01,\n",
        "                                                      final_sparsity=0.05,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    loaded_model.load_weights(\"selected20-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "    #new_pruned_model.summary()\n",
        "\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.categorical_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    epochs = 15\n",
        "    # checkpoint\n",
        "    filepath = \"pruned20-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint, sparsity.UpdatePruningStep()]\n",
        "\n",
        "    # Train model on dataset\n",
        "    print(data + \": Training\" + str(ntrain) + \"begins...\")\n",
        "    history = new_pruned_model.fit(x=train_x[train], y=ytrain, validation_data=(train_x[test], ytest),\n",
        "                        batch_size=32, epochs=epochs, callbacks=callbacks_list)\n",
        "    \n",
        "    new_pruned_model.load_weights(\"pruned20-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "    print(np.sum(ypred*ytest)/len(ytest))\n",
        "    \n",
        "    ntrain += 1\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WEED: Training1begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9979\n",
            "Epoch 00001: val_acc improved from -inf to 0.96994, saving model to pruned20-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0059 - acc: 0.9979 - val_loss: 0.1117 - val_acc: 0.9699\n",
            "Epoch 2/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9973\n",
            "Epoch 00002: val_acc improved from 0.96994 to 0.98734, saving model to pruned20-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 556us/sample - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0243 - val_acc: 0.9873\n",
            "Epoch 3/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9977\n",
            "Epoch 00003: val_acc improved from 0.98734 to 0.99209, saving model to pruned20-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 567us/sample - loss: 0.0070 - acc: 0.9975 - val_loss: 0.0204 - val_acc: 0.9921\n",
            "Epoch 4/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9966\n",
            "Epoch 00004: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 549us/sample - loss: 0.0063 - acc: 0.9967 - val_loss: 0.0198 - val_acc: 0.9921\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9972\n",
            "Epoch 00005: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 560us/sample - loss: 0.0085 - acc: 0.9972 - val_loss: 0.0188 - val_acc: 0.9921\n",
            "Epoch 6/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9980\n",
            "Epoch 00006: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 547us/sample - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0198 - val_acc: 0.9921\n",
            "Epoch 7/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9982\n",
            "Epoch 00007: val_acc improved from 0.99209 to 0.99367, saving model to pruned20-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 575us/sample - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0189 - val_acc: 0.9937\n",
            "Epoch 8/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9979\n",
            "Epoch 00008: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 535us/sample - loss: 0.0057 - acc: 0.9977 - val_loss: 0.0287 - val_acc: 0.9889\n",
            "Epoch 9/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9979\n",
            "Epoch 00009: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 561us/sample - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0213 - val_acc: 0.9921\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9977\n",
            "Epoch 00010: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 560us/sample - loss: 0.0079 - acc: 0.9975 - val_loss: 0.0192 - val_acc: 0.9921\n",
            "Epoch 11/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9984\n",
            "Epoch 00011: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 538us/sample - loss: 0.0038 - acc: 0.9984 - val_loss: 0.0195 - val_acc: 0.9905\n",
            "Epoch 12/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9973\n",
            "Epoch 00012: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 551us/sample - loss: 0.0075 - acc: 0.9974 - val_loss: 0.0193 - val_acc: 0.9889\n",
            "Epoch 13/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9978\n",
            "Epoch 00013: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 553us/sample - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0201 - val_acc: 0.9905\n",
            "Epoch 14/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9979\n",
            "Epoch 00014: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 558us/sample - loss: 0.0081 - acc: 0.9979 - val_loss: 0.0189 - val_acc: 0.9921\n",
            "Epoch 15/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9984\n",
            "Epoch 00015: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 564us/sample - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0201 - val_acc: 0.9905\n",
            "0.9900487470381623\n",
            "WEED: Training2begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9988\n",
            "Epoch 00001: val_acc improved from -inf to 0.98892, saving model to pruned20-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 985us/sample - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0565 - val_acc: 0.9889\n",
            "Epoch 2/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
            "Epoch 00002: val_acc improved from 0.98892 to 0.99367, saving model to pruned20-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 548us/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0416 - val_acc: 0.9937\n",
            "Epoch 3/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9984\n",
            "Epoch 00003: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 535us/sample - loss: 0.0035 - acc: 0.9984 - val_loss: 0.0442 - val_acc: 0.9921\n",
            "Epoch 4/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9975\n",
            "Epoch 00004: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 528us/sample - loss: 0.0059 - acc: 0.9975 - val_loss: 0.0451 - val_acc: 0.9937\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9984\n",
            "Epoch 00005: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 523us/sample - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0416 - val_acc: 0.9921\n",
            "Epoch 6/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9982\n",
            "Epoch 00006: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 539us/sample - loss: 0.0047 - acc: 0.9981 - val_loss: 0.0405 - val_acc: 0.9921\n",
            "Epoch 7/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9991\n",
            "Epoch 00007: val_acc improved from 0.99367 to 0.99525, saving model to pruned20-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 564us/sample - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0400 - val_acc: 0.9953\n",
            "Epoch 8/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
            "Epoch 00008: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 529us/sample - loss: 0.0025 - acc: 0.9989 - val_loss: 0.0423 - val_acc: 0.9921\n",
            "Epoch 9/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9984\n",
            "Epoch 00009: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 540us/sample - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0424 - val_acc: 0.9921\n",
            "Epoch 10/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9984\n",
            "Epoch 00010: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 527us/sample - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0427 - val_acc: 0.9921\n",
            "Epoch 11/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9991\n",
            "Epoch 00011: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 542us/sample - loss: 0.0032 - acc: 0.9991 - val_loss: 0.0428 - val_acc: 0.9921\n",
            "Epoch 12/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9988\n",
            "Epoch 00012: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 542us/sample - loss: 0.0058 - acc: 0.9988 - val_loss: 0.0428 - val_acc: 0.9921\n",
            "Epoch 13/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9996\n",
            "Epoch 00013: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 533us/sample - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0424 - val_acc: 0.9921\n",
            "Epoch 14/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9975\n",
            "Epoch 00014: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 546us/sample - loss: 0.0063 - acc: 0.9975 - val_loss: 0.0424 - val_acc: 0.9921\n",
            "Epoch 15/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
            "Epoch 00015: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 548us/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0423 - val_acc: 0.9921\n",
            "0.9937173005534915\n",
            "WEED: Training3begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9956\n",
            "Epoch 00001: val_acc improved from -inf to 0.99525, saving model to pruned20-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 6s 1ms/sample - loss: 0.0119 - acc: 0.9956 - val_loss: 0.0206 - val_acc: 0.9953\n",
            "Epoch 2/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9954\n",
            "Epoch 00002: val_acc improved from 0.99525 to 0.99842, saving model to pruned20-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 547us/sample - loss: 0.0130 - acc: 0.9954 - val_loss: 0.0093 - val_acc: 0.9984\n",
            "Epoch 3/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9975\n",
            "Epoch 00003: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 553us/sample - loss: 0.0109 - acc: 0.9975 - val_loss: 0.0107 - val_acc: 0.9953\n",
            "Epoch 4/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9956\n",
            "Epoch 00004: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 540us/sample - loss: 0.0098 - acc: 0.9956 - val_loss: 0.0101 - val_acc: 0.9984\n",
            "Epoch 5/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9962\n",
            "Epoch 00005: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 541us/sample - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0097 - val_acc: 0.9984\n",
            "Epoch 6/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9971\n",
            "Epoch 00006: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 540us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.0094 - val_acc: 0.9968\n",
            "Epoch 7/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9938\n",
            "Epoch 00007: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 538us/sample - loss: 0.0197 - acc: 0.9937 - val_loss: 0.0122 - val_acc: 0.9953\n",
            "Epoch 8/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9957\n",
            "Epoch 00008: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 530us/sample - loss: 0.0118 - acc: 0.9958 - val_loss: 0.0113 - val_acc: 0.9953\n",
            "Epoch 9/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9966\n",
            "Epoch 00009: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 564us/sample - loss: 0.0114 - acc: 0.9967 - val_loss: 0.0117 - val_acc: 0.9953\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9961\n",
            "Epoch 00010: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 545us/sample - loss: 0.0117 - acc: 0.9960 - val_loss: 0.0111 - val_acc: 0.9968\n",
            "Epoch 11/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9970\n",
            "Epoch 00011: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 554us/sample - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0113 - val_acc: 0.9968\n",
            "Epoch 12/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9959\n",
            "Epoch 00012: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 545us/sample - loss: 0.0118 - acc: 0.9960 - val_loss: 0.0108 - val_acc: 0.9968\n",
            "Epoch 13/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9964\n",
            "Epoch 00013: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 534us/sample - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0097 - val_acc: 0.9968\n",
            "Epoch 14/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9964\n",
            "Epoch 00014: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 548us/sample - loss: 0.0097 - acc: 0.9965 - val_loss: 0.0110 - val_acc: 0.9968\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9968\n",
            "Epoch 00015: val_acc did not improve from 0.99842\n",
            "5684/5684 [==============================] - 3s 550us/sample - loss: 0.0096 - acc: 0.9968 - val_loss: 0.0112 - val_acc: 0.9984\n",
            "0.9949535371417392\n",
            "WEED: Training4begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9977\n",
            "Epoch 00001: val_acc improved from -inf to 0.98418, saving model to pruned20-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 5s 933us/sample - loss: 0.0081 - acc: 0.9975 - val_loss: 0.0696 - val_acc: 0.9842\n",
            "Epoch 2/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9979\n",
            "Epoch 00002: val_acc improved from 0.98418 to 0.98892, saving model to pruned20-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 567us/sample - loss: 0.0054 - acc: 0.9979 - val_loss: 0.0611 - val_acc: 0.9889\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9988\n",
            "Epoch 00003: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 3s 533us/sample - loss: 0.0045 - acc: 0.9988 - val_loss: 0.0650 - val_acc: 0.9889\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9956\n",
            "Epoch 00004: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 3s 545us/sample - loss: 0.0126 - acc: 0.9956 - val_loss: 0.0639 - val_acc: 0.9889\n",
            "Epoch 5/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9979\n",
            "Epoch 00005: val_acc improved from 0.98892 to 0.99209, saving model to pruned20-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 564us/sample - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0634 - val_acc: 0.9921\n",
            "Epoch 6/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9978\n",
            "Epoch 00006: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 531us/sample - loss: 0.0045 - acc: 0.9979 - val_loss: 0.0640 - val_acc: 0.9905\n",
            "Epoch 7/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9984\n",
            "Epoch 00007: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 535us/sample - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0621 - val_acc: 0.9905\n",
            "Epoch 8/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9977\n",
            "Epoch 00008: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 542us/sample - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0622 - val_acc: 0.9905\n",
            "Epoch 9/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9973\n",
            "Epoch 00009: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 533us/sample - loss: 0.0103 - acc: 0.9974 - val_loss: 0.0670 - val_acc: 0.9858\n",
            "Epoch 10/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9977\n",
            "Epoch 00010: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 541us/sample - loss: 0.0063 - acc: 0.9977 - val_loss: 0.0612 - val_acc: 0.9889\n",
            "Epoch 11/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9984\n",
            "Epoch 00011: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 554us/sample - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0605 - val_acc: 0.9905\n",
            "Epoch 12/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9986\n",
            "Epoch 00012: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 540us/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.0587 - val_acc: 0.9905\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9979\n",
            "Epoch 00013: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 543us/sample - loss: 0.0073 - acc: 0.9979 - val_loss: 0.0580 - val_acc: 0.9905\n",
            "Epoch 14/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9968\n",
            "Epoch 00014: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 536us/sample - loss: 0.0115 - acc: 0.9968 - val_loss: 0.0607 - val_acc: 0.9889\n",
            "Epoch 15/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9973\n",
            "Epoch 00015: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 551us/sample - loss: 0.0069 - acc: 0.9974 - val_loss: 0.0604 - val_acc: 0.9889\n",
            "0.9882499497650712\n",
            "WEED: Training5begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9982\n",
            "Epoch 00001: val_acc improved from -inf to 0.98259, saving model to pruned20-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 5s 966us/sample - loss: 0.0063 - acc: 0.9982 - val_loss: 0.1048 - val_acc: 0.9826\n",
            "Epoch 2/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9973\n",
            "Epoch 00002: val_acc improved from 0.98259 to 0.98734, saving model to pruned20-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 559us/sample - loss: 0.0088 - acc: 0.9974 - val_loss: 0.0839 - val_acc: 0.9873\n",
            "Epoch 3/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9974\n",
            "Epoch 00003: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 559us/sample - loss: 0.0074 - acc: 0.9974 - val_loss: 0.0858 - val_acc: 0.9842\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9968\n",
            "Epoch 00004: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 554us/sample - loss: 0.0102 - acc: 0.9967 - val_loss: 0.1005 - val_acc: 0.9794\n",
            "Epoch 5/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9982\n",
            "Epoch 00005: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 539us/sample - loss: 0.0066 - acc: 0.9982 - val_loss: 0.0871 - val_acc: 0.9858\n",
            "Epoch 6/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9979\n",
            "Epoch 00006: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 552us/sample - loss: 0.0078 - acc: 0.9979 - val_loss: 0.0814 - val_acc: 0.9873\n",
            "Epoch 7/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9975\n",
            "Epoch 00007: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 547us/sample - loss: 0.0072 - acc: 0.9975 - val_loss: 0.0826 - val_acc: 0.9873\n",
            "Epoch 8/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9979\n",
            "Epoch 00008: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 548us/sample - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0948 - val_acc: 0.9826\n",
            "Epoch 9/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9973\n",
            "Epoch 00009: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 560us/sample - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0846 - val_acc: 0.9858\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9970\n",
            "Epoch 00010: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 569us/sample - loss: 0.0065 - acc: 0.9970 - val_loss: 0.0838 - val_acc: 0.9873\n",
            "Epoch 11/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9975\n",
            "Epoch 00011: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 568us/sample - loss: 0.0068 - acc: 0.9975 - val_loss: 0.0856 - val_acc: 0.9842\n",
            "Epoch 12/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9980\n",
            "Epoch 00012: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 547us/sample - loss: 0.0062 - acc: 0.9981 - val_loss: 0.0829 - val_acc: 0.9858\n",
            "Epoch 13/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9979\n",
            "Epoch 00013: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 564us/sample - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0790 - val_acc: 0.9842\n",
            "Epoch 14/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9982\n",
            "Epoch 00014: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 556us/sample - loss: 0.0044 - acc: 0.9982 - val_loss: 0.0786 - val_acc: 0.9858\n",
            "Epoch 15/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9966\n",
            "Epoch 00015: val_acc did not improve from 0.98734\n",
            "5684/5684 [==============================] - 3s 545us/sample - loss: 0.0091 - acc: 0.9967 - val_loss: 0.0804 - val_acc: 0.9873\n",
            "0.9845170118559479\n",
            "WEED: Training6begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9957\n",
            "Epoch 00001: val_acc improved from -inf to 0.97310, saving model to pruned20-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 5s 960us/sample - loss: 0.0151 - acc: 0.9954 - val_loss: 0.0660 - val_acc: 0.9731\n",
            "Epoch 2/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9943\n",
            "Epoch 00002: val_acc improved from 0.97310 to 0.98734, saving model to pruned20-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 553us/sample - loss: 0.0159 - acc: 0.9942 - val_loss: 0.0391 - val_acc: 0.9873\n",
            "Epoch 3/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9936\n",
            "Epoch 00003: val_acc improved from 0.98734 to 0.98892, saving model to pruned20-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 552us/sample - loss: 0.0198 - acc: 0.9937 - val_loss: 0.0368 - val_acc: 0.9889\n",
            "Epoch 4/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9940\n",
            "Epoch 00004: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 3s 541us/sample - loss: 0.0156 - acc: 0.9940 - val_loss: 0.0342 - val_acc: 0.9889\n",
            "Epoch 5/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9950\n",
            "Epoch 00005: val_acc did not improve from 0.98892\n",
            "5684/5684 [==============================] - 3s 562us/sample - loss: 0.0142 - acc: 0.9951 - val_loss: 0.0353 - val_acc: 0.9889\n",
            "Epoch 6/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9941\n",
            "Epoch 00006: val_acc improved from 0.98892 to 0.99051, saving model to pruned20-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 557us/sample - loss: 0.0155 - acc: 0.9940 - val_loss: 0.0340 - val_acc: 0.9905\n",
            "Epoch 7/15\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9959\n",
            "Epoch 00007: val_acc did not improve from 0.99051\n",
            "5684/5684 [==============================] - 3s 535us/sample - loss: 0.0110 - acc: 0.9958 - val_loss: 0.0328 - val_acc: 0.9905\n",
            "Epoch 8/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9950\n",
            "Epoch 00008: val_acc did not improve from 0.99051\n",
            "5684/5684 [==============================] - 3s 541us/sample - loss: 0.0143 - acc: 0.9949 - val_loss: 0.0365 - val_acc: 0.9889\n",
            "Epoch 9/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9961\n",
            "Epoch 00009: val_acc improved from 0.99051 to 0.99209, saving model to pruned20-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 558us/sample - loss: 0.0137 - acc: 0.9961 - val_loss: 0.0308 - val_acc: 0.9921\n",
            "Epoch 10/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9954\n",
            "Epoch 00010: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 532us/sample - loss: 0.0143 - acc: 0.9952 - val_loss: 0.0327 - val_acc: 0.9905\n",
            "Epoch 11/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9957\n",
            "Epoch 00011: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 544us/sample - loss: 0.0159 - acc: 0.9958 - val_loss: 0.0309 - val_acc: 0.9921\n",
            "Epoch 12/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9968\n",
            "Epoch 00012: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 538us/sample - loss: 0.0127 - acc: 0.9967 - val_loss: 0.0295 - val_acc: 0.9905\n",
            "Epoch 13/15\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9961\n",
            "Epoch 00013: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 544us/sample - loss: 0.0131 - acc: 0.9961 - val_loss: 0.0312 - val_acc: 0.9921\n",
            "Epoch 14/15\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9957\n",
            "Epoch 00014: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 547us/sample - loss: 0.0153 - acc: 0.9958 - val_loss: 0.0307 - val_acc: 0.9905\n",
            "Epoch 15/15\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9958\n",
            "Epoch 00015: val_acc did not improve from 0.99209\n",
            "5684/5684 [==============================] - 3s 546us/sample - loss: 0.0141 - acc: 0.9958 - val_loss: 0.0315 - val_acc: 0.9905\n",
            "0.9865612789430019\n",
            "WEED: Training7begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9998\n",
            "Epoch 00001: val_acc improved from -inf to 0.99049, saving model to pruned20-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0010 - acc: 0.9998 - val_loss: 0.0479 - val_acc: 0.9905\n",
            "Epoch 2/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9989\n",
            "Epoch 00002: val_acc improved from 0.99049 to 0.99208, saving model to pruned20-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 551us/sample - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0350 - val_acc: 0.9921\n",
            "Epoch 3/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
            "Epoch 00003: val_acc improved from 0.99208 to 0.99366, saving model to pruned20-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 547us/sample - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0321 - val_acc: 0.9937\n",
            "Epoch 4/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9989\n",
            "Epoch 00004: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 542us/sample - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0295 - val_acc: 0.9937\n",
            "Epoch 5/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9993\n",
            "Epoch 00005: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 531us/sample - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0307 - val_acc: 0.9921\n",
            "Epoch 6/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9988\n",
            "Epoch 00006: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 547us/sample - loss: 0.0023 - acc: 0.9988 - val_loss: 0.0327 - val_acc: 0.9921\n",
            "Epoch 7/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9991\n",
            "Epoch 00007: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 554us/sample - loss: 0.0046 - acc: 0.9991 - val_loss: 0.0402 - val_acc: 0.9905\n",
            "Epoch 8/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
            "Epoch 00008: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 540us/sample - loss: 0.0036 - acc: 0.9991 - val_loss: 0.0316 - val_acc: 0.9937\n",
            "Epoch 9/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9988\n",
            "Epoch 00009: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 534us/sample - loss: 0.0020 - acc: 0.9988 - val_loss: 0.0332 - val_acc: 0.9921\n",
            "Epoch 10/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9991\n",
            "Epoch 00010: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 551us/sample - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0306 - val_acc: 0.9937\n",
            "Epoch 11/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
            "Epoch 00011: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 547us/sample - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0340 - val_acc: 0.9921\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9991\n",
            "Epoch 00012: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 549us/sample - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0331 - val_acc: 0.9921\n",
            "Epoch 13/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9996\n",
            "Epoch 00013: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 547us/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0343 - val_acc: 0.9921\n",
            "Epoch 14/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
            "Epoch 00014: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 537us/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0315 - val_acc: 0.9937\n",
            "Epoch 15/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9993\n",
            "Epoch 00015: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 540us/sample - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0317 - val_acc: 0.9937\n",
            "0.9926711926321615\n",
            "WEED: Training8begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9961\n",
            "Epoch 00001: val_acc improved from -inf to 0.98732, saving model to pruned20-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 5s 950us/sample - loss: 0.0108 - acc: 0.9961 - val_loss: 0.0368 - val_acc: 0.9873\n",
            "Epoch 2/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9966\n",
            "Epoch 00002: val_acc improved from 0.98732 to 0.99208, saving model to pruned20-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 545us/sample - loss: 0.0098 - acc: 0.9967 - val_loss: 0.0238 - val_acc: 0.9921\n",
            "Epoch 3/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9979\n",
            "Epoch 00003: val_acc did not improve from 0.99208\n",
            "5685/5685 [==============================] - 3s 537us/sample - loss: 0.0086 - acc: 0.9975 - val_loss: 0.0228 - val_acc: 0.9921\n",
            "Epoch 4/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9964\n",
            "Epoch 00004: val_acc did not improve from 0.99208\n",
            "5685/5685 [==============================] - 3s 521us/sample - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0223 - val_acc: 0.9921\n",
            "Epoch 5/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9974\n",
            "Epoch 00005: val_acc improved from 0.99208 to 0.99366, saving model to pruned20-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 535us/sample - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0207 - val_acc: 0.9937\n",
            "Epoch 6/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9984\n",
            "Epoch 00006: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 536us/sample - loss: 0.0062 - acc: 0.9984 - val_loss: 0.0200 - val_acc: 0.9921\n",
            "Epoch 7/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9966\n",
            "Epoch 00007: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 535us/sample - loss: 0.0088 - acc: 0.9967 - val_loss: 0.0207 - val_acc: 0.9921\n",
            "Epoch 8/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9986\n",
            "Epoch 00008: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 544us/sample - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0202 - val_acc: 0.9921\n",
            "Epoch 9/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9971\n",
            "Epoch 00009: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 530us/sample - loss: 0.0071 - acc: 0.9972 - val_loss: 0.0209 - val_acc: 0.9921\n",
            "Epoch 10/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9968\n",
            "Epoch 00010: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 530us/sample - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0196 - val_acc: 0.9921\n",
            "Epoch 11/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9986\n",
            "Epoch 00011: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 531us/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.0202 - val_acc: 0.9921\n",
            "Epoch 12/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9970\n",
            "Epoch 00012: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 543us/sample - loss: 0.0088 - acc: 0.9968 - val_loss: 0.0204 - val_acc: 0.9921\n",
            "Epoch 13/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9979\n",
            "Epoch 00013: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 535us/sample - loss: 0.0071 - acc: 0.9979 - val_loss: 0.0239 - val_acc: 0.9905\n",
            "Epoch 14/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9964\n",
            "Epoch 00014: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 529us/sample - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0220 - val_acc: 0.9921\n",
            "Epoch 15/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9970\n",
            "Epoch 00015: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 524us/sample - loss: 0.0072 - acc: 0.9967 - val_loss: 0.0211 - val_acc: 0.9921\n",
            "0.9915080975018672\n",
            "WEED: Training9begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9987\n",
            "Epoch 00001: val_acc improved from -inf to 0.98098, saving model to pruned20-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 6s 1ms/sample - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0995 - val_acc: 0.9810\n",
            "Epoch 2/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9968\n",
            "Epoch 00002: val_acc improved from 0.98098 to 0.98891, saving model to pruned20-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 556us/sample - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0780 - val_acc: 0.9889\n",
            "Epoch 3/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9973\n",
            "Epoch 00003: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 543us/sample - loss: 0.0076 - acc: 0.9974 - val_loss: 0.0769 - val_acc: 0.9889\n",
            "Epoch 4/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9975\n",
            "Epoch 00004: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 551us/sample - loss: 0.0070 - acc: 0.9975 - val_loss: 0.0806 - val_acc: 0.9873\n",
            "Epoch 5/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9986\n",
            "Epoch 00005: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 535us/sample - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0793 - val_acc: 0.9889\n",
            "Epoch 6/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9959\n",
            "Epoch 00006: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 542us/sample - loss: 0.0100 - acc: 0.9960 - val_loss: 0.0831 - val_acc: 0.9857\n",
            "Epoch 7/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9981\n",
            "Epoch 00007: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 536us/sample - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0817 - val_acc: 0.9873\n",
            "Epoch 8/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9975\n",
            "Epoch 00008: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 543us/sample - loss: 0.0067 - acc: 0.9975 - val_loss: 0.0797 - val_acc: 0.9889\n",
            "Epoch 9/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9979\n",
            "Epoch 00009: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 543us/sample - loss: 0.0086 - acc: 0.9979 - val_loss: 0.0825 - val_acc: 0.9889\n",
            "Epoch 10/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9979\n",
            "Epoch 00010: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 550us/sample - loss: 0.0072 - acc: 0.9977 - val_loss: 0.0784 - val_acc: 0.9889\n",
            "Epoch 11/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9975\n",
            "Epoch 00011: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 546us/sample - loss: 0.0065 - acc: 0.9975 - val_loss: 0.0798 - val_acc: 0.9889\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9981\n",
            "Epoch 00012: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 537us/sample - loss: 0.0076 - acc: 0.9981 - val_loss: 0.0787 - val_acc: 0.9889\n",
            "Epoch 13/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9984\n",
            "Epoch 00013: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 533us/sample - loss: 0.0066 - acc: 0.9984 - val_loss: 0.0759 - val_acc: 0.9889\n",
            "Epoch 14/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9984\n",
            "Epoch 00014: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 538us/sample - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0763 - val_acc: 0.9889\n",
            "Epoch 15/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9979\n",
            "Epoch 00015: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 536us/sample - loss: 0.0051 - acc: 0.9977 - val_loss: 0.0764 - val_acc: 0.9889\n",
            "0.9857485777192148\n",
            "WEED: Training10begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9974\n",
            "Epoch 00001: val_acc improved from -inf to 0.98257, saving model to pruned20-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 5s 957us/sample - loss: 0.0076 - acc: 0.9974 - val_loss: 0.0588 - val_acc: 0.9826\n",
            "Epoch 2/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9986\n",
            "Epoch 00002: val_acc improved from 0.98257 to 0.98574, saving model to pruned20-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 545us/sample - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0512 - val_acc: 0.9857\n",
            "Epoch 3/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9973\n",
            "Epoch 00003: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 543us/sample - loss: 0.0070 - acc: 0.9974 - val_loss: 0.0501 - val_acc: 0.9857\n",
            "Epoch 4/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9970\n",
            "Epoch 00004: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 521us/sample - loss: 0.0077 - acc: 0.9970 - val_loss: 0.0560 - val_acc: 0.9842\n",
            "Epoch 5/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9977\n",
            "Epoch 00005: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 526us/sample - loss: 0.0062 - acc: 0.9977 - val_loss: 0.0550 - val_acc: 0.9810\n",
            "Epoch 6/15\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9980\n",
            "Epoch 00006: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 524us/sample - loss: 0.0053 - acc: 0.9981 - val_loss: 0.0517 - val_acc: 0.9810\n",
            "Epoch 7/15\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9984\n",
            "Epoch 00007: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 540us/sample - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0535 - val_acc: 0.9810\n",
            "Epoch 8/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9988\n",
            "Epoch 00008: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 566us/sample - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0549 - val_acc: 0.9810\n",
            "Epoch 9/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9972\n",
            "Epoch 00009: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 544us/sample - loss: 0.0070 - acc: 0.9972 - val_loss: 0.0533 - val_acc: 0.9810\n",
            "Epoch 10/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9982\n",
            "Epoch 00010: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 535us/sample - loss: 0.0041 - acc: 0.9982 - val_loss: 0.0535 - val_acc: 0.9842\n",
            "Epoch 11/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9989\n",
            "Epoch 00011: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 540us/sample - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0570 - val_acc: 0.9826\n",
            "Epoch 12/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9984\n",
            "Epoch 00012: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 528us/sample - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0533 - val_acc: 0.9826\n",
            "Epoch 13/15\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9989\n",
            "Epoch 00013: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 532us/sample - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0526 - val_acc: 0.9842\n",
            "Epoch 14/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9991\n",
            "Epoch 00014: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 528us/sample - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0521 - val_acc: 0.9857\n",
            "Epoch 15/15\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9989\n",
            "Epoch 00015: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 3s 530us/sample - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0517 - val_acc: 0.9857\n",
            "0.9852747021127968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmz9PSRTvPP",
        "colab_type": "code",
        "outputId": "5851a0f8-e14a-444b-dc77-8b4e946f7be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "new_pruned_model.load_weights(\"pruned20-weights-hyper3dnet\" + data + str(3) + \"-best_3layers_4filters.h5\")\n",
        "final_model = sparsity.strip_pruning(new_pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 20)]      0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_8 (Separabl (None, 25, 25, 128)       3188      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 25, 25, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 25, 25, 128)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_9 (Separabl (None, 13, 13, 128)       17664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_10 (Separab (None, 7, 7, 128)         17664     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 39,927\n",
            "Trainable params: 39,415\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8drkv1mK1CUb",
        "colab_type": "code",
        "outputId": "78bcd642-f624-4a42-bd9c-d651b5d3553c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "import numpy as np\n",
        "for i, w in enumerate(final_model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.4f}%\".format(\n",
        "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "separable_conv2d_8/depthwise_kernel:0 -- Total:500, Zeros: 0.0000%\n",
            "separable_conv2d_8/pointwise_kernel:0 -- Total:2560, Zeros: 2.1094%\n",
            "separable_conv2d_8/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "separable_conv2d_9/depthwise_kernel:0 -- Total:1152, Zeros: 0.0000%\n",
            "separable_conv2d_9/pointwise_kernel:0 -- Total:16384, Zeros: 2.0935%\n",
            "separable_conv2d_9/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "separable_conv2d_10/depthwise_kernel:0 -- Total:1152, Zeros: 0.0000%\n",
            "separable_conv2d_10/pointwise_kernel:0 -- Total:16384, Zeros: 2.0935%\n",
            "separable_conv2d_10/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "fc3/kernel:0 -- Total:384, Zeros: 2.0833%\n",
            "fc3/bias:0 -- Total:3, Zeros: 0.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIRfZDn02rV",
        "colab_type": "code",
        "outputId": "069a2f61-ff70-420b-befd-8e6fb99cd614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Saving pruned model to: ', \"Kochia_hyper3DNet_pruned.h5\")\n",
        "tf.keras.models.save_model(final_model, \"Kochia_hyper3DNet_20bands_pruned.h5\", \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  Kochia_hyper3DNet_pruned.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tkgm44OZDt",
        "colab_type": "text"
      },
      "source": [
        "# Predict and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVT1KBWvEODg",
        "colab_type": "code",
        "outputId": "67485837-a575-47f8-bb1a-f443585678ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import truediv\n",
        "import h5py\n",
        "import pickle\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Prepare lists to save metrics\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_m):\n",
        "    list_diag = np.diag(confusion_m)\n",
        "    list_raw_sum = np.sum(confusion_m, axis=1)\n",
        "    each_ac = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_ac)\n",
        "    return each_ac, average_acc\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "PREDICT AND CALCULATE METRICS\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "loaded_model = tf.keras.models.load_model(\"selected20-weights-hyper3dnet\" + 'WEED' + str(1) + \"-best_3layers_4filters.h5\")\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "dataset = 'WEED'\n",
        "# Initialize\n",
        "confmatrices = np.zeros((10, int(classes), int(classes)))\n",
        "\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.30,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.load_weights(\"pruned20-weights-hyper3dnet\" + dataset + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        con_mat = tf.math.confusion_matrix(labels=np.argmax(ytest, axis=-1),\n",
        "                                           predictions=np.argmax(ypred, axis=-1)).numpy()\n",
        "\n",
        "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "    classes_list = list(range(0, int(classes)))\n",
        "    con_mat_df = pd.DataFrame(con_mat_norm, index=classes_list, columns=classes_list)\n",
        "\n",
        "    confmatrices[ntrain - 1, :, :] = con_mat_df.values\n",
        "\n",
        "    # Calculate metrics\n",
        "    oa = accuracy_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    confusion = confusion_matrix(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    prec, rec, f1, support = precision_recall_fscore_support(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1),\n",
        "                                                             average='macro')\n",
        "\n",
        "    # Add metrics to the list\n",
        "    cvoa.append(oa * 100)\n",
        "    cvaa.append(aa * 100)\n",
        "    cvka.append(kappa * 100)\n",
        "    cvpre.append(prec * 100)\n",
        "    cvrec.append(rec * 100)\n",
        "    cvf1.append(f1 * 100)\n",
        "\n",
        "    ntrain += 1\n",
        "\n",
        "file_name = \"classification_report_hyper3dnet_pruned20\" + dataset + \".txt\"\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write(\"Overall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvoa)), float(np.std(cvoa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Average accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvaa)), float(np.std(cvaa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Kappa accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvka)), float(np.std(cvka))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Precision accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvpre)), float(np.std(cvpre))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Recall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvrec)), float(np.std(cvrec))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"F1 accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvf1)), float(np.std(cvf1))))\n",
        "\n",
        "# Calculate mean and std\n",
        "means = np.mean(confmatrices * 100, axis=0)\n",
        "stds = np.std(confmatrices * 100, axis=0)\n",
        "\n",
        "def plot_confusion_matrix(cm, cms, classescf,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classescf))\n",
        "    plt.xticks(tick_marks, classescf, rotation=45)\n",
        "    plt.yticks(tick_marks, classescf)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "\n",
        "            if (cm[i, j] == 100 or cm[i, j] == 0) and cms[i, j] == 0:\n",
        "                plt.text(j, i, '{0:.0f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.0f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "            else:\n",
        "                plt.text(j, i, '{0:.2f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.2f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "with open('meanshyper3dnet20p', 'wb') as f:\n",
        "    pickle.dump(means, f)\n",
        "with open('stdshyper3dnet20p', 'wb') as f:\n",
        "    pickle.dump(stds, f)\n",
        "with open('cvf1hyper3dnet20p', 'wb') as f:\n",
        "    pickle.dump(cvf1, f)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "classes_list = list(range(0, int(3)))\n",
        "plt.figure()\n",
        "plot_confusion_matrix(means, stds, classescf=classes_list)\n",
        "\n",
        "plt.savefig('MatrixConfusion_WEEDhyper3dnet_pruned_20p.png', dpi=1200)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAEmCAYAAAA5jbhCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}