{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HSI_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyQTRzMcUCB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare tensorflow-model-optimization for pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNPotwubYRu",
        "colab_type": "code",
        "outputId": "669eb024-c7c5-45a0-8aa7-aa242b94caae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tensorflow-gpu==1.14.0\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 42kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 66.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c4/4c3d011e432bd9c19f0323f7da7d3f783402615e4c3b5a98416c7da9cb05/tensorflow_model_optimization-0.2.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.3MB/s \n",
            "\u001b[?25hCollecting enum34~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/af/42/cb9355df32c69b553e72a2e28daee25d1611d2c0d9c272aa1d34204205b2/enum34-1.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.17.5)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.6 tensorflow-model-optimization-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfBLXz1mCNV",
        "colab_type": "code",
        "outputId": "b06a3fa9-b767-482d-97e3-eb1d8ada4a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk1dYHpbicZ",
        "colab_type": "text"
      },
      "source": [
        "#Download dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu64LC4Wm1oc",
        "colab_type": "code",
        "outputId": "900f744d-3da1-4da4-f28e-7de0d4537443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.1.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWT-iIqSb_46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8/view?usp=sharing\n",
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8\n",
        "download = drive.CreateFile({'id': '1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8'})\n",
        "download.GetContentFile('weed_dataset_w25.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDlBcilciuY",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRqt2u-MVfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from operator import truediv\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syCzlwNcr-di",
        "colab_type": "code",
        "outputId": "ce5f141e-a043-45cd-fca3-c2d4a31587c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "LOAD HDF5 FILE\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "hdf5_file = h5py.File('weed_dataset_w25.hdf5', \"r\")\n",
        "train_x = np.array(hdf5_file[\"train_img\"][...])\n",
        "# train_x = train_x / np.max(train_x)\n",
        "# train_x = np.clip(train_x, 0, 1)\n",
        "#train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))\n",
        "train_y = np.array(hdf5_file[\"train_labels\"][...])\n",
        "\n",
        "# Average consecutive bands\n",
        "img2 = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], int(train_x.shape[3]/2)))\n",
        "for n in range(0, train_x.shape[0]):\n",
        "    # Average consecutive bands\n",
        "    for i in range(0, train_x.shape[3], 2):\n",
        "        img2[n, :, :, int(i/2)] = (train_x[n, :, :, i] + train_x[n, :, :, i + 1]) / 2.\n",
        "\n",
        "train_x = img2\n",
        "\n",
        "print(train_x.shape)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3wrK8dNe-T7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3]))\n",
        "\n",
        "temp = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], 20))\n",
        "\n",
        "temp[:, :, :, 0] = train_x[:, :, :, 2]\n",
        "temp[:, :, :, 1] = train_x[:, :, :, 9]\n",
        "temp[:, :, :, 2] = train_x[:, :, :, 10]\n",
        "temp[:, :, :, 3] = train_x[:, :, :, 36]\n",
        "temp[:, :, :, 4] = train_x[:, :, :, 37]\n",
        "temp[:, :, :,5] = train_x[:, :, :, 43]\n",
        "temp[:, :, :,6] = train_x[:, :, :,55]\n",
        "temp[:, :, :,7] = train_x[:, :, :, 66]\n",
        "temp[:, :, :,8] = train_x[:, :, :, 101]\n",
        "temp[:, :, :, 9] = train_x[:, :, :, 105]\n",
        "temp[:, :, :, 10] = train_x[:, :, :, 106]\n",
        "temp[:, :, :, 11] = train_x[:, :, :, 107]\n",
        "temp[:, :, :, 12] = train_x[:, :, :, 111]\n",
        "temp[:, :, :, 13] = train_x[:, :, :, 113]\n",
        "temp[:, :, :, 14] = train_x[:, :, :, 114]\n",
        "temp[:, :, :, 15] = train_x[:, :, :, 121]\n",
        "temp[:, :, :, 16] = train_x[:, :, :, 122]\n",
        "temp[:, :, :, 17] = train_x[:, :, :, 131]\n",
        "temp[:, :, :, 18] = train_x[:, :, :, 132]\n",
        "temp[:, :, :, 19] = train_x[:, :, :, 144]\n",
        "\n",
        "\n",
        "train_x = temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvx55MBgcrZJ",
        "colab_type": "text"
      },
      "source": [
        "# Trained pruned network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2xU6uWWAuz",
        "colab_type": "code",
        "outputId": "6df344d9-d15f-47cf-c94e-aef476b931a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "data = 'WEED'\n",
        "loaded_model = tf.keras.models.load_model(\"selected-weights-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE2WVq7DItg",
        "colab_type": "code",
        "outputId": "482e1509-f38b-4298-d36f-3e93208710db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                   final_sparsity=0.90,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adadelta',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1584\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.debugging.assert_greater_equal is deprecated. Please use tf.compat.v1.debugging.assert_greater_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py:59: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 20)]      0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 25, 25, 128)       5750      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 25, 25, 128)       513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 25, 25, 128)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 13, 13, 128)       34050     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 13, 13, 128)       513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 13, 13, 128)       1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_separabl (None, 7, 7, 128)         34050     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_global_a (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_fc3 (Pru (None, 3)                 773       \n",
            "=================================================================\n",
            "Total params: 75,653\n",
            "Trainable params: 39,415\n",
            "Non-trainable params: 36,238\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYBXiSTtaDt",
        "colab_type": "code",
        "outputId": "1cd24d43-3843-4472-e840-fda370bced85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "cva1 = []\n",
        "cva2 = []\n",
        "cva3 = []\n",
        "\n",
        "epochs = 8\n",
        "\n",
        "data = 'WEED'\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "\n",
        "    ytrain = tf.keras.utils.to_categorical(train_y[train]).astype(np.int32)\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.05,\n",
        "                                                      final_sparsity=0.10,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    loaded_model.load_weights(\"selected-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "    #new_pruned_model.summary()\n",
        "\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.categorical_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # checkpoint\n",
        "    filepath = \"pruned-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint, sparsity.UpdatePruningStep()]\n",
        "\n",
        "    # Train model on dataset\n",
        "    print(data + \": Training\" + str(ntrain) + \"begins...\")\n",
        "    history = new_pruned_model.fit(x=train_x[train], y=ytrain, validation_data=(train_x[test], ytest),\n",
        "                        batch_size=32, epochs=epochs, callbacks=callbacks_list)\n",
        "    \n",
        "    new_pruned_model.load_weights(\"pruned-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "    print(np.sum(ypred*ytest)/len(ytest))\n",
        "    \n",
        "    ntrain += 1\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WEED: Training1begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/8\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9949\n",
            "Epoch 00001: val_acc improved from -inf to 0.98576, saving model to pruned-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 10s 2ms/sample - loss: 0.0146 - acc: 0.9947 - val_loss: 0.0607 - val_acc: 0.9858\n",
            "Epoch 2/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9935\n",
            "Epoch 00002: val_acc improved from 0.98576 to 0.98892, saving model to pruned-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 443us/sample - loss: 0.0194 - acc: 0.9935 - val_loss: 0.0506 - val_acc: 0.9889\n",
            "Epoch 3/8\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9941\n",
            "Epoch 00003: val_acc improved from 0.98892 to 0.99525, saving model to pruned-weights-hyper3dnetWEED1-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 2s 439us/sample - loss: 0.0214 - acc: 0.9940 - val_loss: 0.0447 - val_acc: 0.9953\n",
            "Epoch 4/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9941\n",
            "Epoch 00004: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 2s 433us/sample - loss: 0.0206 - acc: 0.9938 - val_loss: 0.1154 - val_acc: 0.9715\n",
            "Epoch 5/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9951\n",
            "Epoch 00005: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 457us/sample - loss: 0.0258 - acc: 0.9951 - val_loss: 0.0429 - val_acc: 0.9905\n",
            "Epoch 6/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9940\n",
            "Epoch 00006: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 449us/sample - loss: 0.0223 - acc: 0.9938 - val_loss: 0.0426 - val_acc: 0.9905\n",
            "Epoch 7/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9939\n",
            "Epoch 00007: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 444us/sample - loss: 0.0221 - acc: 0.9938 - val_loss: 0.0419 - val_acc: 0.9921\n",
            "Epoch 8/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9921\n",
            "Epoch 00008: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 446us/sample - loss: 0.0245 - acc: 0.9921 - val_loss: 0.0498 - val_acc: 0.9937\n",
            "0.9920331491522997\n",
            "WEED: Training2begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9949\n",
            "Epoch 00001: val_acc improved from -inf to 0.99209, saving model to pruned-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 5s 809us/sample - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0268 - val_acc: 0.9921\n",
            "Epoch 2/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9943\n",
            "Epoch 00002: val_acc improved from 0.99209 to 0.99367, saving model to pruned-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 450us/sample - loss: 0.0190 - acc: 0.9944 - val_loss: 0.0286 - val_acc: 0.9937\n",
            "Epoch 3/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9942\n",
            "Epoch 00003: val_acc improved from 0.99367 to 0.99525, saving model to pruned-weights-hyper3dnetWEED2-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 451us/sample - loss: 0.0182 - acc: 0.9942 - val_loss: 0.0197 - val_acc: 0.9953\n",
            "Epoch 4/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9954\n",
            "Epoch 00004: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 2s 433us/sample - loss: 0.0180 - acc: 0.9952 - val_loss: 0.0865 - val_acc: 0.9873\n",
            "Epoch 5/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9968\n",
            "Epoch 00005: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 2s 427us/sample - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0295 - val_acc: 0.9873\n",
            "Epoch 6/8\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9957\n",
            "Epoch 00006: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 450us/sample - loss: 0.0191 - acc: 0.9958 - val_loss: 0.0218 - val_acc: 0.9953\n",
            "Epoch 7/8\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9948\n",
            "Epoch 00007: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 445us/sample - loss: 0.0209 - acc: 0.9945 - val_loss: 0.0231 - val_acc: 0.9953\n",
            "Epoch 8/8\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9950\n",
            "Epoch 00008: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 2s 430us/sample - loss: 0.0179 - acc: 0.9951 - val_loss: 0.0240 - val_acc: 0.9953\n",
            "0.9934341839667191\n",
            "WEED: Training3begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9905\n",
            "Epoch 00001: val_acc improved from -inf to 0.99367, saving model to pruned-weights-hyper3dnetWEED3-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 768us/sample - loss: 0.0292 - acc: 0.9901 - val_loss: 0.0402 - val_acc: 0.9937\n",
            "Epoch 2/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9898\n",
            "Epoch 00002: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 2s 428us/sample - loss: 0.0332 - acc: 0.9898 - val_loss: 0.0399 - val_acc: 0.9921\n",
            "Epoch 3/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9923\n",
            "Epoch 00003: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 2s 439us/sample - loss: 0.0240 - acc: 0.9921 - val_loss: 0.0355 - val_acc: 0.9905\n",
            "Epoch 4/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9895\n",
            "Epoch 00004: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 2s 425us/sample - loss: 0.0347 - acc: 0.9891 - val_loss: 0.0501 - val_acc: 0.9873\n",
            "Epoch 5/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9915\n",
            "Epoch 00005: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 2s 437us/sample - loss: 0.0307 - acc: 0.9912 - val_loss: 0.0372 - val_acc: 0.9873\n",
            "Epoch 6/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9887\n",
            "Epoch 00006: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 2s 432us/sample - loss: 0.0320 - acc: 0.9884 - val_loss: 0.0365 - val_acc: 0.9873\n",
            "Epoch 7/8\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9905\n",
            "Epoch 00007: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 2s 427us/sample - loss: 0.0305 - acc: 0.9903 - val_loss: 0.0454 - val_acc: 0.9873\n",
            "Epoch 8/8\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9897\n",
            "Epoch 00008: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 2s 426us/sample - loss: 0.0324 - acc: 0.9898 - val_loss: 0.0376 - val_acc: 0.9921\n",
            "0.9901959070702591\n",
            "WEED: Training4begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9945\n",
            "Epoch 00001: val_acc improved from -inf to 0.99525, saving model to pruned-weights-hyper3dnetWEED4-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 783us/sample - loss: 0.0214 - acc: 0.9944 - val_loss: 0.0387 - val_acc: 0.9953\n",
            "Epoch 2/8\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9932\n",
            "Epoch 00002: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 442us/sample - loss: 0.0213 - acc: 0.9933 - val_loss: 0.0509 - val_acc: 0.9889\n",
            "Epoch 3/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9930\n",
            "Epoch 00003: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 2s 429us/sample - loss: 0.0234 - acc: 0.9931 - val_loss: 0.0367 - val_acc: 0.9953\n",
            "Epoch 4/8\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9955\n",
            "Epoch 00004: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 2s 428us/sample - loss: 0.0140 - acc: 0.9954 - val_loss: 0.0447 - val_acc: 0.9905\n",
            "Epoch 5/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9933\n",
            "Epoch 00005: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 2s 437us/sample - loss: 0.0239 - acc: 0.9933 - val_loss: 0.0390 - val_acc: 0.9905\n",
            "Epoch 6/8\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9952\n",
            "Epoch 00006: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 2s 423us/sample - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0356 - val_acc: 0.9937\n",
            "Epoch 7/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9945\n",
            "Epoch 00007: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 2s 431us/sample - loss: 0.0137 - acc: 0.9945 - val_loss: 0.0331 - val_acc: 0.9937\n",
            "Epoch 8/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9941\n",
            "Epoch 00008: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 2s 422us/sample - loss: 0.0198 - acc: 0.9937 - val_loss: 0.0322 - val_acc: 0.9937\n",
            "0.9928413891056693\n",
            "WEED: Training5begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/8\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9963\n",
            "Epoch 00001: val_acc improved from -inf to 0.97785, saving model to pruned-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 4s 787us/sample - loss: 0.0158 - acc: 0.9961 - val_loss: 0.1110 - val_acc: 0.9778\n",
            "Epoch 2/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9971\n",
            "Epoch 00002: val_acc improved from 0.97785 to 0.98576, saving model to pruned-weights-hyper3dnetWEED5-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 526us/sample - loss: 0.0094 - acc: 0.9972 - val_loss: 0.0876 - val_acc: 0.9858\n",
            "Epoch 3/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9970\n",
            "Epoch 00003: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 3s 519us/sample - loss: 0.0111 - acc: 0.9968 - val_loss: 0.0992 - val_acc: 0.9858\n",
            "Epoch 4/8\n",
            "5632/5684 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9959\n",
            "Epoch 00004: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 3s 540us/sample - loss: 0.0113 - acc: 0.9958 - val_loss: 0.1081 - val_acc: 0.9810\n",
            "Epoch 5/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9940\n",
            "Epoch 00005: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 3s 542us/sample - loss: 0.0218 - acc: 0.9940 - val_loss: 0.0975 - val_acc: 0.9794\n",
            "Epoch 6/8\n",
            "5600/5684 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9936\n",
            "Epoch 00006: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 3s 543us/sample - loss: 0.0219 - acc: 0.9937 - val_loss: 0.1004 - val_acc: 0.9794\n",
            "Epoch 7/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9966\n",
            "Epoch 00007: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 3s 537us/sample - loss: 0.0105 - acc: 0.9967 - val_loss: 0.1014 - val_acc: 0.9794\n",
            "Epoch 8/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9961\n",
            "Epoch 00008: val_acc did not improve from 0.98576\n",
            "5684/5684 [==============================] - 3s 518us/sample - loss: 0.0164 - acc: 0.9961 - val_loss: 0.0995 - val_acc: 0.9778\n",
            "0.9836845553396355\n",
            "WEED: Training6begins...\n",
            "Train on 5684 samples, validate on 632 samples\n",
            "Epoch 1/8\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9934\n",
            "Epoch 00001: val_acc improved from -inf to 0.98892, saving model to pruned-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 5s 815us/sample - loss: 0.0234 - acc: 0.9935 - val_loss: 0.0681 - val_acc: 0.9889\n",
            "Epoch 2/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9898\n",
            "Epoch 00002: val_acc improved from 0.98892 to 0.99367, saving model to pruned-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 462us/sample - loss: 0.0387 - acc: 0.9898 - val_loss: 0.0436 - val_acc: 0.9937\n",
            "Epoch 3/8\n",
            "5568/5684 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9908\n",
            "Epoch 00003: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 453us/sample - loss: 0.0298 - acc: 0.9907 - val_loss: 0.0462 - val_acc: 0.9905\n",
            "Epoch 4/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9910\n",
            "Epoch 00004: val_acc did not improve from 0.99367\n",
            "5684/5684 [==============================] - 3s 472us/sample - loss: 0.0272 - acc: 0.9909 - val_loss: 0.0424 - val_acc: 0.9937\n",
            "Epoch 5/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9921\n",
            "Epoch 00005: val_acc improved from 0.99367 to 0.99525, saving model to pruned-weights-hyper3dnetWEED6-best_3layers_4filters.h5\n",
            "5684/5684 [==============================] - 3s 465us/sample - loss: 0.0244 - acc: 0.9921 - val_loss: 0.0448 - val_acc: 0.9953\n",
            "Epoch 6/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9947\n",
            "Epoch 00006: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 465us/sample - loss: 0.0210 - acc: 0.9944 - val_loss: 0.0480 - val_acc: 0.9937\n",
            "Epoch 7/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9944\n",
            "Epoch 00007: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 464us/sample - loss: 0.0222 - acc: 0.9944 - val_loss: 0.0457 - val_acc: 0.9953\n",
            "Epoch 8/8\n",
            "5664/5684 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9899\n",
            "Epoch 00008: val_acc did not improve from 0.99525\n",
            "5684/5684 [==============================] - 3s 458us/sample - loss: 0.0300 - acc: 0.9898 - val_loss: 0.0458 - val_acc: 0.9937\n",
            "0.9922688829012951\n",
            "WEED: Training7begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9958\n",
            "Epoch 00001: val_acc improved from -inf to 0.99208, saving model to pruned-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 5s 848us/sample - loss: 0.0157 - acc: 0.9958 - val_loss: 0.0245 - val_acc: 0.9921\n",
            "Epoch 2/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9950\n",
            "Epoch 00002: val_acc improved from 0.99208 to 0.99366, saving model to pruned-weights-hyper3dnetWEED7-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 464us/sample - loss: 0.0148 - acc: 0.9949 - val_loss: 0.0268 - val_acc: 0.9937\n",
            "Epoch 3/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9951\n",
            "Epoch 00003: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 457us/sample - loss: 0.0211 - acc: 0.9951 - val_loss: 0.0237 - val_acc: 0.9921\n",
            "Epoch 4/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9943\n",
            "Epoch 00004: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 447us/sample - loss: 0.0189 - acc: 0.9944 - val_loss: 0.0246 - val_acc: 0.9905\n",
            "Epoch 5/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9958\n",
            "Epoch 00005: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 444us/sample - loss: 0.0140 - acc: 0.9958 - val_loss: 0.0247 - val_acc: 0.9905\n",
            "Epoch 6/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9959\n",
            "Epoch 00006: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 440us/sample - loss: 0.0185 - acc: 0.9960 - val_loss: 0.0241 - val_acc: 0.9905\n",
            "Epoch 7/8\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9950\n",
            "Epoch 00007: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 2s 439us/sample - loss: 0.0172 - acc: 0.9951 - val_loss: 0.0236 - val_acc: 0.9921\n",
            "Epoch 8/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9940\n",
            "Epoch 00008: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 443us/sample - loss: 0.0197 - acc: 0.9940 - val_loss: 0.0240 - val_acc: 0.9921\n",
            "0.9922834340432768\n",
            "WEED: Training8begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/8\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9938\n",
            "Epoch 00001: val_acc improved from -inf to 0.98415, saving model to pruned-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 4s 775us/sample - loss: 0.0193 - acc: 0.9938 - val_loss: 0.0586 - val_acc: 0.9842\n",
            "Epoch 2/8\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9928\n",
            "Epoch 00002: val_acc improved from 0.98415 to 0.98732, saving model to pruned-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 455us/sample - loss: 0.0196 - acc: 0.9930 - val_loss: 0.0336 - val_acc: 0.9873\n",
            "Epoch 3/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9922\n",
            "Epoch 00003: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 2s 432us/sample - loss: 0.0244 - acc: 0.9923 - val_loss: 0.0269 - val_acc: 0.9873\n",
            "Epoch 4/8\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9945\n",
            "Epoch 00004: val_acc improved from 0.98732 to 0.98891, saving model to pruned-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 442us/sample - loss: 0.0184 - acc: 0.9945 - val_loss: 0.0300 - val_acc: 0.9889\n",
            "Epoch 5/8\n",
            "5568/5685 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9917\n",
            "Epoch 00005: val_acc improved from 0.98891 to 0.99049, saving model to pruned-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 450us/sample - loss: 0.0237 - acc: 0.9919 - val_loss: 0.0217 - val_acc: 0.9905\n",
            "Epoch 6/8\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9938\n",
            "Epoch 00006: val_acc did not improve from 0.99049\n",
            "5685/5685 [==============================] - 2s 439us/sample - loss: 0.0239 - acc: 0.9938 - val_loss: 0.0281 - val_acc: 0.9905\n",
            "Epoch 7/8\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9906\n",
            "Epoch 00007: val_acc did not improve from 0.99049\n",
            "5685/5685 [==============================] - 2s 425us/sample - loss: 0.0317 - acc: 0.9907 - val_loss: 0.0331 - val_acc: 0.9889\n",
            "Epoch 8/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9927\n",
            "Epoch 00008: val_acc improved from 0.99049 to 0.99208, saving model to pruned-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 445us/sample - loss: 0.0218 - acc: 0.9926 - val_loss: 0.0220 - val_acc: 0.9921\n",
            "0.9898314264731366\n",
            "WEED: Training9begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9950\n",
            "Epoch 00001: val_acc improved from -inf to 0.98732, saving model to pruned-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 5s 938us/sample - loss: 0.0205 - acc: 0.9949 - val_loss: 0.0509 - val_acc: 0.9873\n",
            "Epoch 2/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9936\n",
            "Epoch 00002: val_acc improved from 0.98732 to 0.99049, saving model to pruned-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 569us/sample - loss: 0.0284 - acc: 0.9937 - val_loss: 0.0391 - val_acc: 0.9905\n",
            "Epoch 3/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9932\n",
            "Epoch 00003: val_acc did not improve from 0.99049\n",
            "5685/5685 [==============================] - 3s 546us/sample - loss: 0.0206 - acc: 0.9933 - val_loss: 0.0371 - val_acc: 0.9905\n",
            "Epoch 4/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9925\n",
            "Epoch 00004: val_acc did not improve from 0.99049\n",
            "5685/5685 [==============================] - 3s 544us/sample - loss: 0.0204 - acc: 0.9924 - val_loss: 0.0347 - val_acc: 0.9905\n",
            "Epoch 5/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9917\n",
            "Epoch 00005: val_acc did not improve from 0.99049\n",
            "5685/5685 [==============================] - 3s 537us/sample - loss: 0.0244 - acc: 0.9917 - val_loss: 0.0366 - val_acc: 0.9905\n",
            "Epoch 6/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9948\n",
            "Epoch 00006: val_acc improved from 0.99049 to 0.99208, saving model to pruned-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 542us/sample - loss: 0.0157 - acc: 0.9949 - val_loss: 0.0355 - val_acc: 0.9921\n",
            "Epoch 7/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9934\n",
            "Epoch 00007: val_acc did not improve from 0.99208\n",
            "5685/5685 [==============================] - 3s 539us/sample - loss: 0.0228 - acc: 0.9933 - val_loss: 0.0368 - val_acc: 0.9905\n",
            "Epoch 8/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9943\n",
            "Epoch 00008: val_acc did not improve from 0.99208\n",
            "5685/5685 [==============================] - 3s 550us/sample - loss: 0.0158 - acc: 0.9944 - val_loss: 0.0388 - val_acc: 0.9905\n",
            "0.9913445516072168\n",
            "WEED: Training10begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/8\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9945\n",
            "Epoch 00001: val_acc improved from -inf to 0.98415, saving model to pruned-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 5s 835us/sample - loss: 0.0187 - acc: 0.9945 - val_loss: 0.0645 - val_acc: 0.9842\n",
            "Epoch 2/8\n",
            "5632/5685 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9943\n",
            "Epoch 00002: val_acc improved from 0.98415 to 0.98732, saving model to pruned-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 489us/sample - loss: 0.0187 - acc: 0.9944 - val_loss: 0.0588 - val_acc: 0.9873\n",
            "Epoch 3/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9936\n",
            "Epoch 00003: val_acc improved from 0.98732 to 0.98891, saving model to pruned-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 481us/sample - loss: 0.0201 - acc: 0.9935 - val_loss: 0.0664 - val_acc: 0.9889\n",
            "Epoch 4/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9959\n",
            "Epoch 00004: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 3s 469us/sample - loss: 0.0151 - acc: 0.9960 - val_loss: 0.0659 - val_acc: 0.9857\n",
            "Epoch 5/8\n",
            "5600/5685 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9939\n",
            "Epoch 00005: val_acc improved from 0.98891 to 0.99208, saving model to pruned-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 488us/sample - loss: 0.0170 - acc: 0.9938 - val_loss: 0.0478 - val_acc: 0.9921\n",
            "Epoch 6/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9922\n",
            "Epoch 00006: val_acc improved from 0.99208 to 0.99366, saving model to pruned-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 3s 474us/sample - loss: 0.0236 - acc: 0.9923 - val_loss: 0.0453 - val_acc: 0.9937\n",
            "Epoch 7/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9935\n",
            "Epoch 00007: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 468us/sample - loss: 0.0196 - acc: 0.9935 - val_loss: 0.0482 - val_acc: 0.9921\n",
            "Epoch 8/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9919\n",
            "Epoch 00008: val_acc did not improve from 0.99366\n",
            "5685/5685 [==============================] - 3s 468us/sample - loss: 0.0221 - acc: 0.9919 - val_loss: 0.0476 - val_acc: 0.9937\n",
            "0.9897223210075817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmz9PSRTvPP",
        "colab_type": "code",
        "outputId": "4408b654-5c5c-4517-ef0f-2b74c7422047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "new_pruned_model.load_weights(\"pruned-weights-hyper3dnet\" + data + str(2) + \"-best_3layers_4filters.h5\")\n",
        "final_model = sparsity.strip_pruning(new_pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 25, 25, 20)]      0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_8 (Separabl (None, 25, 25, 128)       3188      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 25, 25, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 25, 25, 128)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_9 (Separabl (None, 13, 13, 128)       17664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_10 (Separab (None, 7, 7, 128)         17664     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 39,927\n",
            "Trainable params: 39,415\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8drkv1mK1CUb",
        "colab_type": "code",
        "outputId": "e3e6fc26-01e9-4080-adb8-bde51dde047f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "import numpy as np\n",
        "for i, w in enumerate(final_model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.4f}%\".format(\n",
        "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "separable_conv2d_8/depthwise_kernel:0 -- Total:500, Zeros: 0.0000%\n",
            "separable_conv2d_8/pointwise_kernel:0 -- Total:2560, Zeros: 8.3984%\n",
            "separable_conv2d_8/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_10/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "separable_conv2d_9/depthwise_kernel:0 -- Total:1152, Zeros: 0.0000%\n",
            "separable_conv2d_9/pointwise_kernel:0 -- Total:16384, Zeros: 8.3984%\n",
            "separable_conv2d_9/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_11/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "separable_conv2d_10/depthwise_kernel:0 -- Total:1152, Zeros: 0.0000%\n",
            "separable_conv2d_10/pointwise_kernel:0 -- Total:16384, Zeros: 8.3984%\n",
            "separable_conv2d_10/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "fc3/kernel:0 -- Total:384, Zeros: 8.3333%\n",
            "fc3/bias:0 -- Total:3, Zeros: 0.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIRfZDn02rV",
        "colab_type": "code",
        "outputId": "cbfbef74-5519-405c-8705-5ff48ae5a33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Saving pruned model to: ', \"Kochia_hyper3DNet_pruned.h5\")\n",
        "tf.keras.models.save_model(final_model, \"Kochia_hyper3DNet_20bands_pruned.h5\", \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  Kochia_hyper3DNet_pruned.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tkgm44OZDt",
        "colab_type": "text"
      },
      "source": [
        "# Predict and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVT1KBWvEODg",
        "colab_type": "code",
        "outputId": "029feea5-1a4b-45cd-e5b9-9499f7d17f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import truediv\n",
        "import h5py\n",
        "import pickle\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Prepare lists to save metrics\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_m):\n",
        "    list_diag = np.diag(confusion_m)\n",
        "    list_raw_sum = np.sum(confusion_m, axis=1)\n",
        "    each_ac = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_ac)\n",
        "    return each_ac, average_acc\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "PREDICT AND CALCULATE METRICS\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "loaded_model = tf.keras.models.load_model(\"selected-weights-hyper3dnet\" + 'WEED' + str(1) + \"-best_3layers_4filters.h5\")\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "dataset = 'WEED'\n",
        "# Initialize\n",
        "confmatrices = np.zeros((10, int(classes), int(classes)))\n",
        "\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.30,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "\n",
        "    new_pruned_model.load_weights(\"pruned-weights-hyper3dnet\" + dataset + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "    ypred = new_pruned_model.predict(train_x[test])\n",
        "\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        con_mat = tf.math.confusion_matrix(labels=np.argmax(ytest, axis=-1),\n",
        "                                           predictions=np.argmax(ypred, axis=-1)).numpy()\n",
        "\n",
        "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "    classes_list = list(range(0, int(classes)))\n",
        "    con_mat_df = pd.DataFrame(con_mat_norm, index=classes_list, columns=classes_list)\n",
        "\n",
        "    confmatrices[ntrain - 1, :, :] = con_mat_df.values\n",
        "\n",
        "    # Calculate metrics\n",
        "    oa = accuracy_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    confusion = confusion_matrix(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1))\n",
        "    prec, rec, f1, support = precision_recall_fscore_support(np.argmax(ytest, axis=1), np.argmax(ypred, axis=-1),\n",
        "                                                             average='macro')\n",
        "\n",
        "    # Add metrics to the list\n",
        "    cvoa.append(oa * 100)\n",
        "    cvaa.append(aa * 100)\n",
        "    cvka.append(kappa * 100)\n",
        "    cvpre.append(prec * 100)\n",
        "    cvrec.append(rec * 100)\n",
        "    cvf1.append(f1 * 100)\n",
        "\n",
        "    ntrain += 1\n",
        "\n",
        "file_name = \"classification_report_hyper3dnet_pruned\" + dataset + \".txt\"\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write(\"Overall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvoa)), float(np.std(cvoa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Average accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvaa)), float(np.std(cvaa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Kappa accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvka)), float(np.std(cvka))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Precision accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvpre)), float(np.std(cvpre))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"Recall accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvrec)), float(np.std(cvrec))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write(\"F1 accuracy%.3f%% (+/- %.3f%%)\" % (float(np.mean(cvf1)), float(np.std(cvf1))))\n",
        "\n",
        "# Calculate mean and std\n",
        "means = np.mean(confmatrices * 100, axis=0)\n",
        "stds = np.std(confmatrices * 100, axis=0)\n",
        "\n",
        "def plot_confusion_matrix(cm, cms, classescf,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classescf))\n",
        "    plt.xticks(tick_marks, classescf, rotation=45)\n",
        "    plt.yticks(tick_marks, classescf)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "\n",
        "            if (cm[i, j] == 100 or cm[i, j] == 0) and cms[i, j] == 0:\n",
        "                plt.text(j, i, '{0:.0f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.0f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "            else:\n",
        "                plt.text(j, i, '{0:.2f}'.format(cm[i, j]) + '\\n$\\pm$' + '{0:.2f}'.format(cms[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         verticalalignment=\"center\", fontsize=15,\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "with open('meanshyper3dnet20p', 'wb') as f:\n",
        "    pickle.dump(means, f)\n",
        "with open('stdshyper3dnet20p', 'wb') as f:\n",
        "    pickle.dump(stds, f)\n",
        "with open('cvf1hyper3dnet20p', 'wb') as f:\n",
        "    pickle.dump(cvf1, f)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "classes_list = list(range(0, int(3)))\n",
        "plt.figure()\n",
        "plot_confusion_matrix(means, stds, classescf=classes_list)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAEmCAYAAAA5jbhCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}