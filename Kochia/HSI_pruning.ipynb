{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HSI_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyQTRzMcUCB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare tensorflow-model-optimization for pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNPotwubYRu",
        "colab_type": "code",
        "outputId": "3a30d8de-9a4c-4376-9998-74ad21078014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tensorflow-gpu==1.14.0\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 40kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting tensorflow-model-optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c4/4c3d011e432bd9c19f0323f7da7d3f783402615e4c3b5a98416c7da9cb05/tensorflow_model_optimization-0.2.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n",
            "Collecting enum34~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/af/42/cb9355df32c69b553e72a2e28daee25d1611d2c0d9c272aa1d34204205b2/enum34-1.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.17.5)\n",
            "Installing collected packages: enum34, tensorflow-model-optimization\n",
            "Successfully installed enum34-1.1.6 tensorflow-model-optimization-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfBLXz1mCNV",
        "colab_type": "code",
        "outputId": "944db9b6-f4d2-4496-d3d7-9fc9d2ab447c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk1dYHpbicZ",
        "colab_type": "text"
      },
      "source": [
        "#Download dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu64LC4Wm1oc",
        "colab_type": "code",
        "outputId": "96dc0eb5-d510-4749-be3a-761785038f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWT-iIqSb_46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8/view?usp=sharing\n",
        "#https://drive.google.com/open?id=1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8\n",
        "download = drive.CreateFile({'id': '1swPiOWtQ80zWHOK0pyWDuhilhRDkFux8'})\n",
        "download.GetContentFile('weed_dataset_w25.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDlBcilciuY",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRqt2u-MVfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from operator import truediv\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syCzlwNcr-di",
        "colab_type": "code",
        "outputId": "5abd3e7c-c64d-497d-fb22-844a72d52f94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "LOAD HDF5 FILE\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "hdf5_file = h5py.File('weed_dataset_w25.hdf5', \"r\")\n",
        "train_x = np.array(hdf5_file[\"train_img\"][...])\n",
        "# train_x = train_x / np.max(train_x)\n",
        "# train_x = np.clip(train_x, 0, 1)\n",
        "#train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))\n",
        "train_y = np.array(hdf5_file[\"train_labels\"][...])\n",
        "\n",
        "# Average consecutive bands\n",
        "img2 = np.zeros((train_x.shape[0], train_x.shape[1], train_x.shape[2], int(train_x.shape[3]/2)))\n",
        "for n in range(0, train_x.shape[0]):\n",
        "    # Average consecutive bands\n",
        "    for i in range(0, train_x.shape[3], 2):\n",
        "        img2[n, :, :, int(i/2)] = (train_x[n, :, :, i] + train_x[n, :, :, i + 1]) / 2.\n",
        "\n",
        "train_x = img2\n",
        "\n",
        "print(train_x.shape)\n",
        "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3], 1))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6316, 25, 25, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvx55MBgcrZJ",
        "colab_type": "text"
      },
      "source": [
        "# Trained pruned network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2xU6uWWAuz",
        "colab_type": "code",
        "outputId": "b9738d3a-18c1-4e6f-e9bf-5fe7f85fcb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data = 'WEED'\n",
        "loaded_model = tf.keras.models.load_model(\"weights-hyper3dnet\" + data + str(1) + \"-best_3layers_4filters.h5\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE2WVq7DItg",
        "colab_type": "code",
        "outputId": "4b048e2a-c7ba-4624-8fa8-a35ace4b0042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 8\n",
        "batch_size = 32;\n",
        "num_train_samples = train_x.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                   final_sparsity=0.90,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adadelta',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1584\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.debugging.assert_greater_equal is deprecated. Please use tf.compat.v1.debugging.assert_greater_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py:59: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 25, 25, 150, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv3d_7 (P (None, 25, 25, 150,  1018        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_batch_norma (None, 25, 25, 150,  33          prune_low_magnitude_conv3d_7[0][0\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_activation_ (None, 25, 25, 150,  1           prune_low_magnitude_batch_normali\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv3d_8 (P (None, 25, 25, 150,  5770        prune_low_magnitude_activation_15\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_batch_norma (None, 25, 25, 150,  33          prune_low_magnitude_conv3d_8[0][0\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_activation_ (None, 25, 25, 150,  1           prune_low_magnitude_batch_normali\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_concatenate (None, 25, 25, 150,  1           prune_low_magnitude_activation_15\n",
            "                                                                 prune_low_magnitude_activation_16\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv3d_9 (P (None, 25, 25, 150,  6922        prune_low_magnitude_concatenate_5\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_batch_norma (None, 25, 25, 150,  33          prune_low_magnitude_conv3d_9[0][0\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_activation_ (None, 25, 25, 150,  1           prune_low_magnitude_batch_normali\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_concatenate (None, 25, 25, 150,  1           prune_low_magnitude_concatenate_5\n",
            "                                                                 prune_low_magnitude_activation_17\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_reshape_3 ( (None, 25, 25, 3600) 1           prune_low_magnitude_concatenate_6\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_separable_c (None, 25, 25, 128)  954130      prune_low_magnitude_reshape_3[0][\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_batch_norma (None, 25, 25, 128)  513         prune_low_magnitude_separable_con\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_activation_ (None, 25, 25, 128)  1           prune_low_magnitude_batch_normali\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_separable_c (None, 13, 13, 128)  34050       prune_low_magnitude_activation_18\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_batch_norma (None, 13, 13, 128)  513         prune_low_magnitude_separable_con\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_activation_ (None, 13, 13, 128)  1           prune_low_magnitude_batch_normali\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_separable_c (None, 7, 7, 128)    34050       prune_low_magnitude_activation_19\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_batch_norma (None, 7, 7, 128)    513         prune_low_magnitude_separable_con\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_activation_ (None, 7, 7, 128)    1           prune_low_magnitude_batch_normali\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_separable_c (None, 4, 4, 128)    34050       prune_low_magnitude_activation_20\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_batch_norma (None, 4, 4, 128)    513         prune_low_magnitude_separable_con\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_activation_ (None, 4, 4, 128)    1           prune_low_magnitude_batch_normali\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_flatten_3 ( (None, 2048)         1           prune_low_magnitude_activation_21\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_dropout_3 ( (None, 2048)         1           prune_low_magnitude_flatten_3[0][\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_fc3 (PruneL (None, 3)            12293       prune_low_magnitude_dropout_3[0][\n",
            "==================================================================================================\n",
            "Total params: 1,084,446\n",
            "Trainable params: 560,403\n",
            "Non-trainable params: 524,043\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYBXiSTtaDt",
        "colab_type": "code",
        "outputId": "aaf99c50-b26c-48dc-f1b0-12b5794bbca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "windowSize = train_x.shape[1]\n",
        "classes = 3\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvoa = []\n",
        "cvaa = []\n",
        "cvka = []\n",
        "cvpre = []\n",
        "cvrec = []\n",
        "cvf1 = []\n",
        "cva1 = []\n",
        "cva2 = []\n",
        "cva3 = []\n",
        "\n",
        "data = 'WEED'\n",
        "ntrain = 1\n",
        "for train, test in kfold.split(train_x, train_y):\n",
        "\n",
        "    ytrain = tf.keras.utils.to_categorical(train_y[train]).astype(np.int32)\n",
        "    ytest = tf.keras.utils.to_categorical(train_y[test]).astype(np.int32)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    PRUNING\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    new_pruning_params = {\n",
        "          'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                      final_sparsity=0.30,\n",
        "                                                      begin_step=0,\n",
        "                                                      end_step=end_step,\n",
        "                                                      frequency=100)\n",
        "    }\n",
        "\n",
        "    loaded_model.load_weights(\"weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\")\n",
        "\n",
        "    new_pruned_model = sparsity.prune_low_magnitude(loaded_model, **new_pruning_params)\n",
        "    #new_pruned_model.summary()\n",
        "\n",
        "    new_pruned_model.compile(\n",
        "        loss=tf.keras.losses.categorical_crossentropy,\n",
        "        optimizer='adadelta',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # checkpoint\n",
        "    filepath = \"pruned-weights-hyper3dnet\" + data + str(ntrain) + \"-best_3layers_4filters.h5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint, sparsity.UpdatePruningStep()]\n",
        "\n",
        "    # Train model on dataset\n",
        "    print(data + \": Training\" + str(ntrain) + \"begins...\")\n",
        "    history = new_pruned_model.fit(x=train_x[train], y=ytrain, validation_data=(train_x[test], ytest),\n",
        "                        batch_size=32, epochs=epochs, callbacks=callbacks_list)\n",
        "    \n",
        "    ntrain += 1\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WEED: Training8begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9993\n",
            "Epoch 00001: val_acc improved from -inf to 0.98574, saving model to pruned-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 47s 8ms/sample - loss: 0.0038 - acc: 0.9993 - val_loss: 0.0569 - val_acc: 0.9857\n",
            "Epoch 2/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9989\n",
            "Epoch 00002: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0031 - acc: 0.9989 - val_loss: 0.1126 - val_acc: 0.9794\n",
            "Epoch 3/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9986\n",
            "Epoch 00003: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0038 - acc: 0.9986 - val_loss: 0.0830 - val_acc: 0.9810\n",
            "Epoch 4/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9982\n",
            "Epoch 00004: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0044 - acc: 0.9981 - val_loss: 0.1918 - val_acc: 0.9540\n",
            "Epoch 5/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9986\n",
            "Epoch 00005: val_acc did not improve from 0.98574\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0621 - val_acc: 0.9826\n",
            "Epoch 6/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9984\n",
            "Epoch 00006: val_acc improved from 0.98574 to 0.98732, saving model to pruned-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0458 - val_acc: 0.9873\n",
            "Epoch 7/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9988\n",
            "Epoch 00007: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0344 - val_acc: 0.9873\n",
            "Epoch 8/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9986\n",
            "Epoch 00008: val_acc improved from 0.98732 to 0.98891, saving model to pruned-weights-hyper3dnetWEED8-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0369 - val_acc: 0.9889\n",
            "WEED: Training9begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9977\n",
            "Epoch 00001: val_acc improved from -inf to 0.98257, saving model to pruned-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 41s 7ms/sample - loss: 0.0082 - acc: 0.9977 - val_loss: 0.0535 - val_acc: 0.9826\n",
            "Epoch 2/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9974\n",
            "Epoch 00002: val_acc improved from 0.98257 to 0.98732, saving model to pruned-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 35s 6ms/sample - loss: 0.0076 - acc: 0.9974 - val_loss: 0.0489 - val_acc: 0.9873\n",
            "Epoch 3/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9989\n",
            "Epoch 00003: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 35s 6ms/sample - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0788 - val_acc: 0.9794\n",
            "Epoch 4/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9979\n",
            "Epoch 00004: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 35s 6ms/sample - loss: 0.0074 - acc: 0.9979 - val_loss: 0.0627 - val_acc: 0.9826\n",
            "Epoch 5/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9970\n",
            "Epoch 00005: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 35s 6ms/sample - loss: 0.0079 - acc: 0.9970 - val_loss: 0.0456 - val_acc: 0.9873\n",
            "Epoch 6/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9974\n",
            "Epoch 00006: val_acc did not improve from 0.98732\n",
            "5685/5685 [==============================] - 35s 6ms/sample - loss: 0.0110 - acc: 0.9974 - val_loss: 0.0497 - val_acc: 0.9857\n",
            "Epoch 7/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9977\n",
            "Epoch 00007: val_acc improved from 0.98732 to 0.98891, saving model to pruned-weights-hyper3dnetWEED9-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 35s 6ms/sample - loss: 0.0090 - acc: 0.9975 - val_loss: 0.0444 - val_acc: 0.9889\n",
            "Epoch 8/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9972\n",
            "Epoch 00008: val_acc did not improve from 0.98891\n",
            "5685/5685 [==============================] - 35s 6ms/sample - loss: 0.0103 - acc: 0.9972 - val_loss: 0.0444 - val_acc: 0.9889\n",
            "WEED: Training10begins...\n",
            "Train on 5685 samples, validate on 631 samples\n",
            "Epoch 1/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9993\n",
            "Epoch 00001: val_acc improved from -inf to 0.99366, saving model to pruned-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 41s 7ms/sample - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0093 - val_acc: 0.9937\n",
            "Epoch 2/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9989\n",
            "Epoch 00002: val_acc improved from 0.99366 to 0.99842, saving model to pruned-weights-hyper3dnetWEED10-best_3layers_4filters.h5\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0026 - acc: 0.9989 - val_loss: 0.0039 - val_acc: 0.9984\n",
            "Epoch 3/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9989\n",
            "Epoch 00003: val_acc did not improve from 0.99842\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0029 - acc: 0.9989 - val_loss: 0.0044 - val_acc: 0.9984\n",
            "Epoch 4/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
            "Epoch 00004: val_acc did not improve from 0.99842\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0177 - val_acc: 0.9937\n",
            "Epoch 5/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9986\n",
            "Epoch 00005: val_acc did not improve from 0.99842\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0037 - acc: 0.9986 - val_loss: 0.0093 - val_acc: 0.9952\n",
            "Epoch 6/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9982\n",
            "Epoch 00006: val_acc did not improve from 0.99842\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0186 - val_acc: 0.9889\n",
            "Epoch 7/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9981\n",
            "Epoch 00007: val_acc did not improve from 0.99842\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0078 - val_acc: 0.9952\n",
            "Epoch 8/8\n",
            "5664/5685 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9975\n",
            "Epoch 00008: val_acc did not improve from 0.99842\n",
            "5685/5685 [==============================] - 34s 6ms/sample - loss: 0.0082 - acc: 0.9975 - val_loss: 0.0072 - val_acc: 0.9968\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-744aa700c5de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mntrain\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmz9PSRTvPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1fb7739-33dd-49a7-a7a8-26123dbf2015"
      },
      "source": [
        "new_pruned_model.load_weights(\"pruned-weights-hyper3dnet\" + data + str(2) + \"-best_3layers_4filters.h5\")\n",
        "final_model = sparsity.strip_pruning(new_pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 25, 25, 150, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_7 (Conv3D)               (None, 25, 25, 150,  512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 25, 25, 150,  32          conv3d_7[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 25, 25, 150,  0           batch_normalization_15[1][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_8 (Conv3D)               (None, 25, 25, 150,  2888        activation_15[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 25, 25, 150,  32          conv3d_8[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 25, 25, 150,  0           batch_normalization_16[1][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 25, 25, 150,  0           activation_15[1][0]              \n",
            "                                                                 activation_16[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_9 (Conv3D)               (None, 25, 25, 150,  3464        concatenate_5[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 25, 25, 150,  32          conv3d_9[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 25, 25, 150,  0           batch_normalization_17[1][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 25, 25, 150,  0           concatenate_5[1][0]              \n",
            "                                                                 activation_17[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 25, 25, 3600) 0           concatenate_6[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_9 (SeparableCo (None, 25, 25, 128)  493328      reshape_3[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 25, 25, 128)  512         separable_conv2d_9[1][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 25, 25, 128)  0           batch_normalization_18[1][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_10 (SeparableC (None, 13, 13, 128)  17664       activation_18[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 13, 13, 128)  512         separable_conv2d_10[1][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 13, 13, 128)  0           batch_normalization_19[1][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_11 (SeparableC (None, 7, 7, 128)    17664       activation_19[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 7, 7, 128)    512         separable_conv2d_11[1][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 7, 7, 128)    0           batch_normalization_20[1][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_12 (SeparableC (None, 4, 4, 128)    17664       activation_20[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 4, 4, 128)    512         separable_conv2d_12[1][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 4, 4, 128)    0           batch_normalization_21[1][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 2048)         0           activation_21[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 2048)         0           flatten_3[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "fc3 (Dense)                     (None, 3)            6147        dropout_3[1][0]                  \n",
            "==================================================================================================\n",
            "Total params: 561,475\n",
            "Trainable params: 560,403\n",
            "Non-trainable params: 1,072\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8drkv1mK1CUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "outputId": "9831291d-38b5-426c-a9d2-6756ca864abc"
      },
      "source": [
        "import numpy as np\n",
        "for i, w in enumerate(final_model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.4f}%\".format(\n",
        "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv3d_7/kernel:0 -- Total:504, Zeros: 28.9683%\n",
            "conv3d_7/bias:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_15/gamma:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_15/beta:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_15/moving_mean:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_15/moving_variance:0 -- Total:8, Zeros: 0.0000%\n",
            "conv3d_8/kernel:0 -- Total:2880, Zeros: 28.9931%\n",
            "conv3d_8/bias:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_16/gamma:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_16/beta:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_16/moving_mean:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_16/moving_variance:0 -- Total:8, Zeros: 0.0000%\n",
            "conv3d_9/kernel:0 -- Total:3456, Zeros: 28.9931%\n",
            "conv3d_9/bias:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_17/gamma:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_17/beta:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_17/moving_mean:0 -- Total:8, Zeros: 0.0000%\n",
            "batch_normalization_17/moving_variance:0 -- Total:8, Zeros: 0.0000%\n",
            "separable_conv2d_9/depthwise_kernel:0 -- Total:32400, Zeros: 0.0000%\n",
            "separable_conv2d_9/pointwise_kernel:0 -- Total:460800, Zeros: 28.9976%\n",
            "separable_conv2d_9/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_18/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_18/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_18/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_18/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "separable_conv2d_10/depthwise_kernel:0 -- Total:1152, Zeros: 0.0000%\n",
            "separable_conv2d_10/pointwise_kernel:0 -- Total:16384, Zeros: 28.9978%\n",
            "separable_conv2d_10/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_19/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_19/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_19/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_19/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "separable_conv2d_11/depthwise_kernel:0 -- Total:1152, Zeros: 0.0000%\n",
            "separable_conv2d_11/pointwise_kernel:0 -- Total:16384, Zeros: 28.9978%\n",
            "separable_conv2d_11/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_20/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_20/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_20/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_20/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "separable_conv2d_12/depthwise_kernel:0 -- Total:1152, Zeros: 0.0000%\n",
            "separable_conv2d_12/pointwise_kernel:0 -- Total:16384, Zeros: 28.9978%\n",
            "separable_conv2d_12/bias:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_21/gamma:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_21/beta:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_21/moving_mean:0 -- Total:128, Zeros: 0.0000%\n",
            "batch_normalization_21/moving_variance:0 -- Total:128, Zeros: 0.0000%\n",
            "fc3/kernel:0 -- Total:6144, Zeros: 29.0039%\n",
            "fc3/bias:0 -- Total:3, Zeros: 0.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIRfZDn02rV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c822cd24-3ec0-4e07-eb10-2fa94b20a8c4"
      },
      "source": [
        "print('Saving pruned model to: ', \"Kochia_hyper3DNet_pruned.h5\")\n",
        "tf.keras.models.save_model(final_model, \"Kochia_hyper3DNet_pruned.h5\", \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  Kochia_hyper3DNet_pruned.h5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}